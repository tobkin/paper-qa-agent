{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from IPython.display import Markdown\n",
    "from loaders import HtmlDocumentLoader\n",
    "from preprocessors import ArxivHtmlPaperPreprocessor \n",
    "\n",
    "def display_md(content):\n",
    "  display(Markdown(content))\n",
    "\n",
    "doc_uri = \"https://arxiv.org/html/2312.10997v5\"\n",
    "cache_path = \"./loader_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n\\n<html lang=\"en\">\\n<head>\\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"content-type\"/>\\n<title>Retrieval-Augmented Generation for Large Language Models: A Survey</title>\\n<!--Generated on Wed Mar 27 09:16:19 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->\\n<meta content=\"width=device-width, initial-scale=1, shrink-to-fit=no\" name=\"viewport\"/>\\n<link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css\" rel=\"stylesheet\" type=\"text/css\"/>\\n<link href=\"/static/browse/0.3.4/css/ar5iv_0.7.4.min.css\" rel=\"stylesheet\" type=\"text/css\"/>\\n<link href=\"/static/browse/0.3.4/css/latexml_styles.css\" rel=\"stylesheet\" type=\"text/css\"/>\\n<script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js\"></script>\\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js\"></script>\\n<script src=\"/static/browse/0.3.4/js/addons.js\"></script>\\n<script src=\"/static/browse/0.3.4/js/feedbackOverlay.js\"></script>\\n<meta content=\"\\nLarge language model,  retrieval-augmented generation,  natural language processing,  information retrieval\\n\" lang=\"en\" name=\"keywords\"/>\\n<base href=\"/html/2312.10997v5/\"/></head>\\n<body>\\n<nav class=\"ltx_page_navbar\">\\n<nav class=\"ltx_TOC\">\\n<ol class=\"ltx_toclist\">\\n<li class=\"ltx_tocentry ltx_tocentry_section\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S1\" title=\"I Introduction ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">I </span><span class=\"ltx_text ltx_font_smallcaps\">Introduction</span></span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_section\">\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S2\" title=\"II Overview of RAG ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">II </span><span class=\"ltx_text ltx_font_smallcaps\">Overview of RAG </span></span></a>\\n<ol class=\"ltx_toclist ltx_toclist_section\">\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S2.SS1\" title=\"II-A Naive RAG ‣ II Overview of RAG ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">II-A</span> </span><span class=\"ltx_text ltx_font_italic\">Naive RAG</span></span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S2.SS2\" title=\"II-B Advanced RAG ‣ II Overview of RAG ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">II-B</span> </span><span class=\"ltx_text ltx_font_italic\">Advanced RAG</span></span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S2.SS3\" title=\"II-C Modular RAG ‣ II Overview of RAG ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">II-C</span> </span><span class=\"ltx_text ltx_font_italic\">Modular RAG</span></span></a>\\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S2.SS3.SSS1\" title=\"II-C1 New Modules ‣ II-C Modular RAG ‣ II Overview of RAG ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">II-C</span>1 </span>New Modules</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S2.SS3.SSS2\" title=\"II-C2 New Patterns ‣ II-C Modular RAG ‣ II Overview of RAG ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">II-C</span>2 </span>New Patterns</span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S2.SS4\" title=\"II-D RAG vs Fine-tuning ‣ II Overview of RAG ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">II-D</span> </span><span class=\"ltx_text ltx_font_italic\">RAG vs Fine-tuning</span></span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_section\">\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S3\" title=\"III Retrieval ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">III </span><span class=\"ltx_text ltx_font_smallcaps\">Retrieval</span></span></a>\\n<ol class=\"ltx_toclist ltx_toclist_section\">\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S3.SS1\" title=\"III-A Retrieval Source ‣ III Retrieval ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">III-A</span> </span><span class=\"ltx_text ltx_font_italic\">Retrieval Source</span></span></a>\\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S3.SS1.SSS1\" title=\"III-A1 Data Structure ‣ III-A Retrieval Source ‣ III Retrieval ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">III-A</span>1 </span>Data Structure</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S3.SS1.SSS2\" title=\"III-A2 Retrieval Granularity ‣ III-A Retrieval Source ‣ III Retrieval ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">III-A</span>2 </span>Retrieval Granularity</span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S3.SS2\" title=\"III-B Indexing Optimization ‣ III Retrieval ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">III-B</span> </span><span class=\"ltx_text ltx_font_italic\">Indexing Optimization</span></span></a>\\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S3.SS2.SSS1\" title=\"III-B1 Chunking Strategy ‣ III-B Indexing Optimization ‣ III Retrieval ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">III-B</span>1 </span>Chunking Strategy</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S3.SS2.SSS2\" title=\"III-B2 Metadata Attachments ‣ III-B Indexing Optimization ‣ III Retrieval ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">III-B</span>2 </span>Metadata Attachments</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S3.SS2.SSS3\" title=\"III-B3 Structural Index ‣ III-B Indexing Optimization ‣ III Retrieval ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">III-B</span>3 </span>Structural Index</span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S3.SS3\" title=\"III-C Query Optimization ‣ III Retrieval ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">III-C</span> </span><span class=\"ltx_text ltx_font_italic\">Query Optimization</span></span></a>\\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S3.SS3.SSS1\" title=\"III-C1 Query Expansion ‣ III-C Query Optimization ‣ III Retrieval ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">III-C</span>1 </span>Query Expansion</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S3.SS3.SSS2\" title=\"III-C2 Query Transformation ‣ III-C Query Optimization ‣ III Retrieval ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">III-C</span>2 </span>Query Transformation</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S3.SS3.SSS3\" title=\"III-C3 Query Routing ‣ III-C Query Optimization ‣ III Retrieval ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">III-C</span>3 </span>Query Routing</span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S3.SS4\" title=\"III-D Embedding ‣ III Retrieval ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">III-D</span> </span><span class=\"ltx_text ltx_font_italic\">Embedding</span></span></a>\\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S3.SS4.SSS1\" title=\"III-D1 Mix/hybrid Retrieval ‣ III-D Embedding ‣ III Retrieval ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">III-D</span>1 </span>Mix/hybrid Retrieval</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S3.SS4.SSS2\" title=\"III-D2 Fine-tuning Embedding Model ‣ III-D Embedding ‣ III Retrieval ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">III-D</span>2 </span>Fine-tuning Embedding Model</span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S3.SS5\" title=\"III-E Adapter ‣ III Retrieval ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">III-E</span> </span><span class=\"ltx_text ltx_font_italic\">Adapter</span></span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_section\">\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S4\" title=\"IV Generation ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">IV </span><span class=\"ltx_text ltx_font_smallcaps\">Generation</span></span></a>\\n<ol class=\"ltx_toclist ltx_toclist_section\">\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S4.SS1\" title=\"IV-A Context Curation ‣ IV Generation ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">IV-A</span> </span><span class=\"ltx_text ltx_font_italic\">Context Curation</span></span></a>\\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S4.SS1.SSS1\" title=\"IV-A1 Reranking ‣ IV-A Context Curation ‣ IV Generation ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">IV-A</span>1 </span>Reranking</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S4.SS1.SSS2\" title=\"IV-A2 Context Selection/Compression ‣ IV-A Context Curation ‣ IV Generation ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">IV-A</span>2 </span>Context Selection/Compression</span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S4.SS2\" title=\"IV-B LLM Fine-tuning ‣ IV Generation ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">IV-B</span> </span><span class=\"ltx_text ltx_font_italic\">LLM Fine-tuning</span></span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_section\">\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S5\" title=\"V Augmentation process in RAG ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">V </span><span class=\"ltx_text ltx_font_smallcaps\">Augmentation process in RAG</span></span></a>\\n<ol class=\"ltx_toclist ltx_toclist_section\">\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S5.SS1\" title=\"V-A Iterative Retrieval ‣ V Augmentation process in RAG ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">V-A</span> </span><span class=\"ltx_text ltx_font_italic\">Iterative Retrieval</span></span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S5.SS2\" title=\"V-B Recursive Retrieval ‣ V Augmentation process in RAG ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">V-B</span> </span><span class=\"ltx_text ltx_font_italic\">Recursive Retrieval</span></span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S5.SS3\" title=\"V-C Adaptive Retrieval ‣ V Augmentation process in RAG ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">V-C</span> </span><span class=\"ltx_text ltx_font_italic\">Adaptive Retrieval</span></span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_section\">\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S6\" title=\"VI Task and Evaluation ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">VI </span><span class=\"ltx_text ltx_font_smallcaps\">Task and Evaluation</span></span></a>\\n<ol class=\"ltx_toclist ltx_toclist_section\">\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S6.SS1\" title=\"VI-A Downstream Task ‣ VI Task and Evaluation ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">VI-A</span> </span><span class=\"ltx_text ltx_font_italic\">Downstream Task</span></span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S6.SS2\" title=\"VI-B Evaluation Target ‣ VI Task and Evaluation ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">VI-B</span> </span><span class=\"ltx_text ltx_font_italic\">Evaluation Target</span></span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S6.SS3\" title=\"VI-C Evaluation Aspects ‣ VI Task and Evaluation ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">VI-C</span> </span><span class=\"ltx_text ltx_font_italic\">Evaluation Aspects</span></span></a>\\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S6.SS3.SSS1\" title=\"VI-C1 Quality Scores ‣ VI-C Evaluation Aspects ‣ VI Task and Evaluation ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">VI-C</span>1 </span>Quality Scores</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S6.SS3.SSS2\" title=\"VI-C2 Required Abilities ‣ VI-C Evaluation Aspects ‣ VI Task and Evaluation ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">VI-C</span>2 </span>Required Abilities</span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S6.SS4\" title=\"VI-D Evaluation Benchmarks and Tools ‣ VI Task and Evaluation ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">VI-D</span> </span><span class=\"ltx_text ltx_font_italic\">Evaluation Benchmarks and Tools</span></span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_section\">\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S7\" title=\"VII Discussion and Future Prospects ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">VII </span><span class=\"ltx_text ltx_font_smallcaps\">Discussion and Future Prospects</span></span></a>\\n<ol class=\"ltx_toclist ltx_toclist_section\">\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S7.SS1\" title=\"VII-A RAG vs Long Context ‣ VII Discussion and Future Prospects ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">VII-A</span> </span><span class=\"ltx_text ltx_font_italic\">RAG vs Long Context</span></span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S7.SS2\" title=\"VII-B RAG Robustness ‣ VII Discussion and Future Prospects ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">VII-B</span> </span><span class=\"ltx_text ltx_font_italic\">RAG Robustness</span></span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S7.SS3\" title=\"VII-C Hybrid Approaches ‣ VII Discussion and Future Prospects ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">VII-C</span> </span><span class=\"ltx_text ltx_font_italic\">Hybrid Approaches </span></span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S7.SS4\" title=\"VII-D Scaling laws of RAG ‣ VII Discussion and Future Prospects ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">VII-D</span> </span><span class=\"ltx_text ltx_font_italic\">Scaling laws of RAG </span></span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S7.SS5\" title=\"VII-E Production-Ready RAG ‣ VII Discussion and Future Prospects ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">VII-E</span> </span><span class=\"ltx_text ltx_font_italic\">Production-Ready RAG</span></span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S7.SS6\" title=\"VII-F Multi-modal RAG ‣ VII Discussion and Future Prospects ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\"><span class=\"ltx_text\">VII-F</span> </span><span class=\"ltx_text ltx_font_italic\">Multi-modal RAG</span></span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_section\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S8\" title=\"VIII Conclusion ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">VIII </span><span class=\"ltx_text ltx_font_smallcaps\">Conclusion</span></span></a></li>\\n</ol></nav>\\n</nav>\\n<div class=\"ltx_page_main\">\\n<div class=\"ltx_page_content\"><div class=\"section\" id=\"target-section\"><div id=\"license-tr\">License: arXiv.org perpetual non-exclusive license</div><div id=\"watermark-tr\">arXiv:2312.10997v5 [cs.CL] 27 Mar 2024</div></div>\\n<article class=\"ltx_document ltx_authors_1line\">\\n<h1 class=\"ltx_title ltx_title_document\">Retrieval-Augmented Generation for Large Language Models: A Survey</h1>\\n<div class=\"ltx_authors\">\\n<span class=\"ltx_creator ltx_role_author\">\\n<span class=\"ltx_personname\">Yunfan Gao\\n</span><span class=\"ltx_author_notes\">\\n<span class=\"ltx_contact ltx_role_affiliation\">Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\n</span></span></span>\\n<span class=\"ltx_creator ltx_role_author\">\\n<span class=\"ltx_personname\">Yun Xiong\\n</span><span class=\"ltx_author_notes\">\\n<span class=\"ltx_contact ltx_role_affiliation\">Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\n</span></span></span>\\n<span class=\"ltx_creator ltx_role_author\">\\n<span class=\"ltx_personname\">Xinyu Gao\\n</span><span class=\"ltx_author_notes\">\\n<span class=\"ltx_contact ltx_role_affiliation\">Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\n</span></span></span>\\n<span class=\"ltx_creator ltx_role_author\">\\n<span class=\"ltx_personname\">Kangxiang Jia\\n</span><span class=\"ltx_author_notes\">\\n<span class=\"ltx_contact ltx_role_affiliation\">Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\n</span></span></span>\\n<span class=\"ltx_creator ltx_role_author\">\\n<span class=\"ltx_personname\">Jinliu Pan\\n</span><span class=\"ltx_author_notes\">\\n<span class=\"ltx_contact ltx_role_affiliation\">Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\n</span></span></span>\\n<span class=\"ltx_creator ltx_role_author\">\\n<span class=\"ltx_personname\">Yuxi Bi\\n</span><span class=\"ltx_author_notes\">\\n<span class=\"ltx_contact ltx_role_affiliation\">College of Design and Innovation, Tongji University\\n</span></span></span>\\n<span class=\"ltx_creator ltx_role_author\">\\n<span class=\"ltx_personname\">Yi Dai\\n</span><span class=\"ltx_author_notes\">\\n<span class=\"ltx_contact ltx_role_affiliation\">Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\n</span></span></span>\\n<span class=\"ltx_creator ltx_role_author\">\\n<span class=\"ltx_personname\">Jiawei Sun\\n</span><span class=\"ltx_author_notes\">\\n<span class=\"ltx_contact ltx_role_affiliation\">Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\n</span></span></span>\\n<span class=\"ltx_creator ltx_role_author\">\\n<span class=\"ltx_personname\">Meng Wang\\n</span><span class=\"ltx_author_notes\">\\n<span class=\"ltx_contact ltx_role_affiliation\">College of Design and Innovation, Tongji University\\n</span></span></span>\\n<span class=\"ltx_creator ltx_role_author\">\\n<span class=\"ltx_personname\">Haofen Wang\\n</span><span class=\"ltx_author_notes\">Corresponding Author.Email:<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"haofen.wang@tongji.edu.cn\" title=\"\">haofen.wang@tongji.edu.cn</a>\\n<span class=\"ltx_contact ltx_role_affiliation\">Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\n</span>\\n<span class=\"ltx_contact ltx_role_affiliation\">College of Design and Innovation, Tongji University\\n</span></span></span>\\n</div>\\n<div class=\"ltx_abstract\">\\n<h6 class=\"ltx_title ltx_title_abstract\">Abstract</h6>\\n<p class=\"ltx_p\" id=\"id1.id1\">Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs’ intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development\\xa0<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Resources are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/Tongji-KGLLM/RAG-Survey\" title=\"\">https://github.com/Tongji-KGLLM/RAG-Survey</a> </span></span></span>.</p>\\n</div>\\n<div class=\"ltx_keywords\">\\n<h6 class=\"ltx_title ltx_title_keywords\">Index Terms: </h6>\\nLarge language model, retrieval-augmented generation, natural language processing, information retrieval\\n\\n</div>\\n<section class=\"ltx_section\" id=\"S1\">\\n<h2 class=\"ltx_title ltx_title_section\">\\n<span class=\"ltx_tag ltx_tag_section\">I </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S1.1.1\">Introduction</span>\\n</h2>\\n<div class=\"ltx_para\" id=\"S1.p1\">\\n<p class=\"ltx_p\" id=\"S1.p1.1\">Large language models (LLMs) have achieved remarkable success, though they still face significant limitations, especially in domain-specific or knowledge-intensive tasks\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib1\" title=\"\">1</a>]</cite>, notably producing “hallucinations”\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib2\" title=\"\">2</a>]</cite> when handling queries beyond their training data or requiring current information. To overcome challenges, Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic similarity calculation. By referencing external knowledge, RAG effectively reduces the problem of generating factually incorrect content. Its integration into LLMs has resulted in widespread adoption, establishing RAG as a key technology in advancing chatbots and enhancing the suitability of LLMs for real-world applications.</p>\\n</div>\\n<figure class=\"ltx_figure\" id=\"S1.F1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"366\" id=\"S1.F1.g1\" src=\"extracted/5498883/images/rag_tech_tree.png\" width=\"509\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 1: </span>Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs, research on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent research has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models in the pre-training stage through retrieval-augmented techniques.</figcaption>\\n</figure>\\n<div class=\"ltx_para\" id=\"S1.p2\">\\n<p class=\"ltx_p\" id=\"S1.p2.1\">RAG technology has rapidly developed in recent years, and the technology tree summarizing related research is shown in Figure\\xa0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S1.F1\" title=\"Figure 1 ‣ I Introduction ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The development trajectory of RAG in the era of large models exhibits several distinct stage characteristics. Initially, RAG’s inception coincided with the rise of the Transformer architecture, focusing on enhancing language models by incorporating additional knowledge through Pre-Training Models (PTM). This early stage was characterized by foundational work aimed at refining pre-training techniques<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib5\" title=\"\">5</a>]</cite>.The subsequent arrival of ChatGPT\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib6\" title=\"\">6</a>]</cite> marked a pivotal moment, with LLM demonstrating powerful in context learning (ICL) capabilities. RAG research shifted towards providing better information for LLMs to answer more complex and knowledge-intensive tasks during the inference stage, leading to rapid development in RAG studies. As research progressed, the enhancement of RAG was no longer limited to the inference stage but began to incorporate more with LLM fine-tuning techniques.\\n</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S1.p3\">\\n<p class=\"ltx_p\" id=\"S1.p3.1\">The burgeoning field of RAG has experienced swift growth, yet it has not been accompanied by a systematic synthesis that could clarify its broader trajectory. This survey endeavors to fill this gap by mapping out the RAG process and charting its evolution and anticipated future paths, with a focus on the integration of RAG within LLMs. This paper considers both technical paradigms and research methods, summarizing three main research paradigms from over 100 RAG studies, and analyzing key technologies in the core stages of “Retrieval,” “Generation,” and “Augmentation.” On the other hand, current research tends to focus more on methods, lacking analysis and summarization of how to evaluate RAG. This paper comprehensively reviews the downstream tasks, datasets, benchmarks, and evaluation methods applicable to RAG. Overall, this paper sets out to meticulously compile and categorize the foundational technical concepts, historical progression, and the spectrum of RAG methodologies and applications that have emerged post-LLMs. It is designed to equip readers and professionals with a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and speculate on upcoming trends and innovations.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S1.p4\">\\n<p class=\"ltx_p\" id=\"S1.p4.1\">Our contributions are as follows:\\n</p>\\n<ul class=\"ltx_itemize\" id=\"S1.I1\">\\n<li class=\"ltx_item\" id=\"S1.I1.i1\" style=\"list-style-type:none;\">\\n<span class=\"ltx_tag ltx_tag_item\">•</span>\\n<div class=\"ltx_para\" id=\"S1.I1.i1.p1\">\\n<p class=\"ltx_p\" id=\"S1.I1.i1.p1.1\">In this survey, we present a thorough and systematic review of the state-of-the-art RAG methods, delineating its evolution through paradigms including naive RAG, advanced RAG, and modular RAG. This review contextualizes the broader scope of RAG research within the landscape of LLMs.</p>\\n</div>\\n</li>\\n<li class=\"ltx_item\" id=\"S1.I1.i2\" style=\"list-style-type:none;\">\\n<span class=\"ltx_tag ltx_tag_item\">•</span>\\n<div class=\"ltx_para\" id=\"S1.I1.i2.p1\">\\n<p class=\"ltx_p\" id=\"S1.I1.i2.p1.1\">We identify and discuss the central technologies integral to the RAG process, specifically focusing on the aspects of “Retrieval”, “Generation” and “Augmentation”, and delve into their synergies, elucidating how these components intricately collaborate to form a cohesive and effective RAG framework.</p>\\n</div>\\n</li>\\n<li class=\"ltx_item\" id=\"S1.I1.i3\" style=\"list-style-type:none;\">\\n<span class=\"ltx_tag ltx_tag_item\">•</span>\\n<div class=\"ltx_para\" id=\"S1.I1.i3.p1\">\\n<p class=\"ltx_p\" id=\"S1.I1.i3.p1.1\">We have summarized the current assessment methods of RAG, covering 26 tasks, nearly 50 datasets, outlining the evaluation objectives and metrics, as well as the current evaluation benchmarks and tools. Additionally, we anticipate future directions for RAG, emphasizing potential enhancements to tackle current challenges.</p>\\n</div>\\n</li>\\n</ul>\\n</div>\\n<div class=\"ltx_para\" id=\"S1.p5\">\\n<p class=\"ltx_p\" id=\"S1.p5.1\">The paper unfolds as follows: Section\\xa0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S2\" title=\"II Overview of RAG ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> introduces the main concept and current paradigms of RAG. The following three sections explore core components—“Retrieval”, “Generation” and “Augmentation”, respectively.\\nSection\\xa0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S3\" title=\"III Retrieval ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> focuses on optimization methods in retrieval,including indexing, query and embedding optimization.\\nSection\\xa0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S4\" title=\"IV Generation ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> concentrates on post-retrieval process and LLM fine-tuning in generation.\\nSection\\xa0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S5\" title=\"V Augmentation process in RAG ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> analyzes the three augmentation processes.\\nSection\\xa0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S6\" title=\"VI Task and Evaluation ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a> focuses on RAG’s downstream tasks and evaluation system. Section\\xa0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S7\" title=\"VII Discussion and Future Prospects ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_tag\">VII</span></a> mainly discusses the challenges that RAG currently faces and its future development directions. At last, the paper concludes in Section\\xa0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S8\" title=\"VIII Conclusion ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_tag\">VIII</span></a>.</p>\\n</div>\\n</section>\\n<section class=\"ltx_section\" id=\"S2\">\\n<h2 class=\"ltx_title ltx_title_section\">\\n<span class=\"ltx_tag ltx_tag_section\">II </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S2.1.1\">Overview of RAG </span>\\n</h2>\\n<div class=\"ltx_para\" id=\"S2.p1\">\\n<p class=\"ltx_p\" id=\"S2.p1.1\">A typical application of RAG is illustrated in Figure\\xa0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S2.F2\" title=\"Figure 2 ‣ II Overview of RAG ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Here, a user poses a question to ChatGPT about a recent, widely discussed news. Given ChatGPT’s reliance on pre-training data, it initially lacks the capacity to provide updates on recent developments. RAG bridges this information gap by sourcing and incorporating knowledge from external databases. In this case, it gathers relevant news articles related to the user’s query. These articles, combined with the original question, form a comprehensive prompt that empowers LLMs to generate a well-informed answer.</p>\\n</div>\\n<figure class=\"ltx_figure\" id=\"S2.F2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"301\" id=\"S2.F2.g1\" src=\"extracted/5498883/images/RAG_case.png\" width=\"509\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 2: </span>A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks, encoded into vectors, and stored in a vector database. 2) Retrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3) Generation. Input the original question and the retrieved chunks together into LLM to generate the final answer.</figcaption>\\n</figure>\\n<div class=\"ltx_para\" id=\"S2.p2\">\\n<p class=\"ltx_p\" id=\"S2.p2.1\">The RAG research paradigm is continuously evolving, and we categorize it into three stages: Naive RAG, Advanced RAG, and Modular RAG, as showed in Figure\\xa0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S2.F3\" title=\"Figure 3 ‣ II-B Advanced RAG ‣ II Overview of RAG ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Despite RAG method are cost-effective and surpass the performance of the native LLM, they also exhibit several limitations. The development of Advanced RAG and Modular RAG is a response to these specific shortcomings in Naive RAG.</p>\\n</div>\\n<section class=\"ltx_subsection\" id=\"S2.SS1\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S2.SS1.5.1.1\">II-A</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S2.SS1.6.2\">Naive RAG</span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S2.SS1.p1\">\\n<p class=\"ltx_p\" id=\"S2.SS1.p1.1\">The Naive RAG research paradigm represents the earliest methodology, which gained prominence shortly after the widespread adoption of ChatGPT. The Naive RAG follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a “Retrieve-Read” framework\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib7\" title=\"\">7</a>]</cite>.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS1.p2\">\\n<p class=\"ltx_p\" id=\"S2.SS1.p2.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S2.SS1.p2.1.1\">Indexing</em> starts with the cleaning and extraction of raw data in diverse formats like PDF, HTML, Word, and Markdown, which is then converted into a uniform plain text format. To accommodate the context limitations of language models, text is segmented into smaller, digestible chunks. Chunks are then encoded into vector representations using an embedding model and stored in vector database. This step is crucial for enabling efficient similarity searches in the subsequent retrieval phase.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS1.p3\">\\n<p class=\"ltx_p\" id=\"S2.SS1.p3.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S2.SS1.p3.1.1\">Retrieval</em>. Upon receipt of a user query, the RAG system employs the same encoding model utilized during the indexing phase to transform the query into a vector representation. It then computes the similarity scores between the query vector and the vector of chunks within the indexed corpus. The system prioritizes and retrieves the top K chunks that demonstrate the greatest similarity to the query. These chunks are subsequently used as the expanded context in prompt.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS1.p4\">\\n<p class=\"ltx_p\" id=\"S2.SS1.p4.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S2.SS1.p4.1.1\">Generation</em>. The posed query and selected documents are synthesized into a coherent prompt to which a large language model is tasked with formulating a response. The model’s approach to answering may vary depending on task-specific criteria, allowing it to either draw upon its inherent parametric knowledge or restrict its responses to the information contained within the provided documents. In cases of ongoing dialogues, any existing conversational history can be integrated into the prompt, enabling the model to engage in multi-turn dialogue interactions effectively.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS1.p5\">\\n<p class=\"ltx_p\" id=\"S2.SS1.p5.1\">However, Naive RAG encounters notable drawbacks:</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS1.p6\">\\n<p class=\"ltx_p\" id=\"S2.SS1.p6.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S2.SS1.p6.1.1\">Retrieval Challenges</em>. The retrieval phase often struggles with precision and recall, leading to the selection of misaligned or irrelevant chunks, and the missing of crucial information.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS1.p7\">\\n<p class=\"ltx_p\" id=\"S2.SS1.p7.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S2.SS1.p7.1.1\">Generation Difficulties</em>. In generating responses, the model may face the issue of hallucination, where it produces content not supported by the retrieved context. This phase can also suffer from irrelevance, toxicity, or bias in the outputs, detracting from the quality and reliability of the responses.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS1.p8\">\\n<p class=\"ltx_p\" id=\"S2.SS1.p8.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S2.SS1.p8.1.1\">Augmentation Hurdles</em>. Integrating retrieved information with the different task can be challenging, sometimes resulting in disjointed or incoherent outputs. The process may also encounter redundancy when similar information is retrieved from multiple sources, leading to repetitive responses. Determining the significance and relevance of various passages and ensuring stylistic and tonal consistency add further complexity. Facing complex issues, a single retrieval based on the original query may not suffice to acquire adequate context information.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS1.p9\">\\n<p class=\"ltx_p\" id=\"S2.SS1.p9.1\">Moreover, there’s a concern that generation models might overly rely on augmented information, leading to outputs that simply echo retrieved content without adding insightful or synthesized information.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S2.SS2\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S2.SS2.5.1.1\">II-B</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S2.SS2.6.2\">Advanced RAG</span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S2.SS2.p1\">\\n<p class=\"ltx_p\" id=\"S2.SS2.p1.1\">Advanced RAG introduces specific improvements to overcome the limitations of Naive RAG. Focusing on enhancing retrieval quality, it employs pre-retrieval and post-retrieval strategies. To tackle the indexing issues, Advanced RAG refines its indexing techniques through the use of a sliding window approach, fine-grained segmentation, and the incorporation of metadata. Additionally, it incorporates several optimization methods to streamline the retrieval process<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib8\" title=\"\">8</a>]</cite>.\\n</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS2.p2\">\\n<p class=\"ltx_p\" id=\"S2.SS2.p2.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S2.SS2.p2.1.1\">Pre-retrieval process</em>. In this stage, the primary focus is on optimizing the indexing structure and the original query. The goal of optimizing indexing is to enhance the quality of the content being indexed. This involves strategies: enhancing data granularity, optimizing index structures, adding metadata, alignment optimization, and mixed retrieval. While the goal of query optimization is to make the user’s original question clearer and more suitable for the retrieval task. Common methods include query rewriting query transformation, query expansion and other techniques\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib11\" title=\"\">11</a>]</cite>.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS2.p3\">\\n<p class=\"ltx_p\" id=\"S2.SS2.p3.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S2.SS2.p3.1.1\">Post-Retrieval Process</em>. Once relevant context is retrieved, it’s crucial to integrate it effectively with the query. The main methods in post-retrieval process include rerank chunks and context compressing. Re-ranking the retrieved information to relocate the most relevant content to the edges of the prompt is a key strategy. This concept has been implemented in frameworks such as LlamaIndex<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.llamaindex.ai\" title=\"\">https://www.llamaindex.ai</a></span></span></span>, LangChain<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.langchain.com/\" title=\"\">https://www.langchain.com/</a></span></span></span>, and HayStack\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib12\" title=\"\">12</a>]</cite>. Feeding all relevant documents directly into LLMs can lead to information overload, diluting the focus on key details with irrelevant content.To mitigate this, post-retrieval efforts concentrate on selecting the essential information, emphasizing critical sections, and shortening the context to be processed.\\n</p>\\n</div>\\n<figure class=\"ltx_figure\" id=\"S2.F3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"292\" id=\"S2.F3.g1\" src=\"extracted/5498883/images/RAG_FrameCompre_eng.png\" width=\"480\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 3: </span>Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle) Advanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a chain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the introduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and generation; it includes methods such as iterative and adaptive retrieval.</figcaption>\\n</figure>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S2.SS3\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S2.SS3.5.1.1\">II-C</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S2.SS3.6.2\">Modular RAG</span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S2.SS3.p1\">\\n<p class=\"ltx_p\" id=\"S2.SS3.p1.1\">The modular RAG architecture advances beyond the former two RAG paradigms, offering enhanced adaptability and versatility. It incorporates diverse strategies for improving its components, such as adding a search module for similarity searches and refining the retriever through fine-tuning. Innovations like restructured RAG modules\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib13\" title=\"\">13</a>]</cite> and rearranged RAG pipelines\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib14\" title=\"\">14</a>]</cite> have been introduced to tackle specific challenges. The shift towards a modular RAG approach is becoming prevalent, supporting both sequential processing and integrated end-to-end training across its components. Despite its distinctiveness, Modular RAG builds upon the foundational principles of Advanced and Naive RAG, illustrating a progression and refinement within the RAG family.</p>\\n</div>\\n<section class=\"ltx_subsubsection\" id=\"S2.SS3.SSS1\">\\n<h4 class=\"ltx_title ltx_title_subsubsection\">\\n<span class=\"ltx_tag ltx_tag_subsubsection\"><span class=\"ltx_text\" id=\"S2.SS3.SSS1.5.1.1\">II-C</span>1 </span>New Modules</h4>\\n<div class=\"ltx_para\" id=\"S2.SS3.SSS1.p1\">\\n<p class=\"ltx_p\" id=\"S2.SS3.SSS1.p1.1\">The Modular RAG framework introduces additional specialized components to enhance retrieval and processing capabilities. The Search module adapts to specific scenarios, enabling direct searches across various data sources like search engines, databases, and knowledge graphs, using LLM-generated code and query languages\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib15\" title=\"\">15</a>]</cite>. RAG-Fusion addresses traditional search limitations by employing a multi-query strategy that expands user queries into diverse perspectives, utilizing parallel vector searches and intelligent re-ranking to uncover both explicit and transformative knowledge\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib16\" title=\"\">16</a>]</cite>. The Memory module leverages the LLM’s memory to guide retrieval, creating an unbounded memory pool that aligns the text more closely with data distribution through iterative self-enhancement\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib18\" title=\"\">18</a>]</cite>. Routing in the RAG system navigates through diverse data sources, selecting the optimal pathway for a query, whether it involves summarization, specific database searches, or merging different information streams\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib19\" title=\"\">19</a>]</cite>. The Predict module aims to reduce redundancy and noise by generating context directly through the LLM, ensuring relevance and accuracy\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib13\" title=\"\">13</a>]</cite>. Lastly, the Task Adapter module tailors RAG to various downstream tasks, automating prompt retrieval for zero-shot inputs and creating task-specific retrievers through few-shot query generation\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib21\" title=\"\">21</a>]</cite> .This comprehensive approach not only streamlines the retrieval process but also significantly improves the quality and relevance of the information retrieved, catering to a wide array of tasks and queries with enhanced precision and flexibility.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsubsection\" id=\"S2.SS3.SSS2\">\\n<h4 class=\"ltx_title ltx_title_subsubsection\">\\n<span class=\"ltx_tag ltx_tag_subsubsection\"><span class=\"ltx_text\" id=\"S2.SS3.SSS2.5.1.1\">II-C</span>2 </span>New Patterns</h4>\\n<div class=\"ltx_para\" id=\"S2.SS3.SSS2.p1\">\\n<p class=\"ltx_p\" id=\"S2.SS3.SSS2.p1.1\">Modular RAG offers remarkable adaptability by allowing module substitution or reconfiguration to address specific challenges. This goes beyond the fixed structures of Naive and Advanced RAG, characterized by a simple “Retrieve” and “Read” mechanism. Moreover, Modular RAG expands this flexibility by integrating new modules or adjusting interaction flow among existing ones, enhancing its applicability across different tasks.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS3.SSS2.p2\">\\n<p class=\"ltx_p\" id=\"S2.SS3.SSS2.p2.1\">Innovations such as the Rewrite-Retrieve-Read\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib7\" title=\"\">7</a>]</cite>model leverage the LLM’s capabilities to refine retrieval queries through a rewriting module and a LM-feedback mechanism to update rewriting model., improving task performance. Similarly, approaches like Generate-Read\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib13\" title=\"\">13</a>]</cite> replace traditional retrieval with LLM-generated content, while Recite-Read\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib22\" title=\"\">22</a>]</cite> emphasizes retrieval from model weights, enhancing the model’s ability to handle knowledge-intensive tasks. Hybrid retrieval strategies integrate keyword, semantic, and vector searches to cater to diverse queries. Additionally, employing sub-queries and hypothetical document embeddings (HyDE)\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib11\" title=\"\">11</a>]</cite> seeks to improve retrieval relevance by focusing on embedding similarities between generated answers and real documents.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS3.SSS2.p3\">\\n<p class=\"ltx_p\" id=\"S2.SS3.SSS2.p3.1\">Adjustments in module arrangement and interaction, such as the Demonstrate-Search-Predict (DSP)\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib23\" title=\"\">23</a>]</cite> framework and the iterative Retrieve-Read-Retrieve-Read flow of ITER-RETGEN\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib14\" title=\"\">14</a>]</cite>, showcase the dynamic use of module outputs to bolster another module’s functionality, illustrating a sophisticated understanding of enhancing module synergy. The flexible orchestration of Modular RAG Flow showcases the benefits of adaptive retrieval through techniques such as FLARE\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib24\" title=\"\">24</a>]</cite> and Self-RAG\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib25\" title=\"\">25</a>]</cite>. This approach transcends the fixed RAG retrieval process by evaluating the necessity of retrieval based on different scenarios. Another benefit of a flexible architecture is that the RAG system can more easily integrate with other technologies (such as fine-tuning or reinforcement learning)\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib26\" title=\"\">26</a>]</cite>. For example, this can involve fine-tuning the retriever for better retrieval results, fine-tuning the generator for more personalized outputs, or engaging in collaborative fine-tuning\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib27\" title=\"\">27</a>]</cite>.</p>\\n</div>\\n</section>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S2.SS4\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S2.SS4.5.1.1\">II-D</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S2.SS4.6.2\">RAG vs Fine-tuning</span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S2.SS4.p1\">\\n<p class=\"ltx_p\" id=\"S2.SS4.p1.1\">The augmentation of LLMs has attracted considerable attention due to their growing prevalence. Among the optimization methods for LLMs, RAG is often compared with Fine-tuning (FT) and prompt engineering. Each method has distinct characteristics as illustrated in Figure\\xa0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S2.F4\" title=\"Figure 4 ‣ II-D RAG vs Fine-tuning ‣ II Overview of RAG ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. We used a quadrant chart to illustrate the differences among three methods in two dimensions: external knowledge requirements and model adaption requirements. Prompt engineering leverages a model’s inherent capabilities with minimum necessity for external knowledge and model adaption. RAG can be likened to providing a model with a tailored textbook for information retrieval, ideal for precise information retrieval tasks. In contrast, FT is comparable to a student internalizing knowledge over time, suitable for scenarios requiring replication of specific structures, styles, or formats.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS4.p2\">\\n<p class=\"ltx_p\" id=\"S2.SS4.p2.1\">RAG excels in dynamic environments by offering real-time knowledge updates and effective utilization of external knowledge sources with high interpretability. However, it comes with higher latency and ethical considerations regarding data retrieval. On the other hand, FT is more static, requiring retraining for updates but enabling deep customization of the model’s behavior and style. It demands significant computational resources for dataset preparation and training, and while it can reduce hallucinations, it may face challenges with unfamiliar data.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS4.p3\">\\n<p class=\"ltx_p\" id=\"S2.SS4.p3.1\">In multiple evaluations of their performance on various knowledge-intensive tasks across different topics, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib28\" title=\"\">28</a>]</cite> revealed that while unsupervised fine-tuning shows some improvement, RAG consistently outperforms it, for both existing knowledge encountered during training and entirely new knowledge. Additionally, it was found that LLMs struggle to learn new factual information through unsupervised fine-tuning. The choice between RAG and FT depends on the specific needs for data dynamics, customization, and computational capabilities in the application context. RAG and FT are not mutually exclusive and can complement each other, enhancing a model’s capabilities at different levels. In some instances, their combined use may lead to optimal performance. The optimization process involving RAG and FT may require multiple iterations to achieve satisfactory results.</p>\\n</div>\\n<figure class=\"ltx_figure\" id=\"S2.F4\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"280\" id=\"S2.F4.g1\" src=\"extracted/5498883/images/rag_FT.png\" width=\"479\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 4: </span>RAG compared with other model optimization methods in the aspects of “External Knowledge Required” and “Model Adaption Required”. Prompt Engineering requires low modifications to the model and external knowledge, focusing on harnessing the capabilities of LLMs themselves. Fine-tuning, on the other hand, involves further training the model. In the early stages of RAG (Naive RAG), there is a low demand for model modifications. As research progresses, Modular RAG has become more integrated with fine-tuning techniques.</figcaption>\\n</figure>\\n<figure class=\"ltx_table\" id=\"S2.T1\">\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">TABLE I: </span>Summary of RAG methods</figcaption>\\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S2.T1.1\" style=\"width:488.0pt;height:1056.3pt;vertical-align:-0.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-61.0pt,131.9pt) scale(0.8,0.8) ;\">\\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.1.1\">\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.1\">\\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S2.T1.1.1.1.1\">Method</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S2.T1.1.1.1.2\">Retrieval Source</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S2.T1.1.1.1.3\">\\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.1.1.1.3.1\">\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.1.3.1.1\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.1.3.1.1.1\">Retrieval</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.1.3.1.2\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.1.3.1.2.1\">Data Type</td>\\n</tr>\\n</table></td>\\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S2.T1.1.1.1.4\">\\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.1.1.1.4.1\">\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.1.4.1.1\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.1.4.1.1.1\">Retrieval</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.1.4.1.2\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.1.4.1.2.1\">Granularity</td>\\n</tr>\\n</table></td>\\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S2.T1.1.1.1.5\">\\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.1.1.1.5.1\">\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.1.5.1.1\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.1.5.1.1.1\">Augmentation</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.1.5.1.2\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.1.5.1.2.1\">Stage</td>\\n</tr>\\n</table></td>\\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S2.T1.1.1.1.6\">\\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.1.1.1.6.1\">\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.1.6.1.1\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.1.6.1.1.1\">Retrieval</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.1.6.1.2\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.1.6.1.2.1\">process</td>\\n</tr>\\n</table></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.2\">\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.2.1\">CoG\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib29\" title=\"\">29</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.2.2\">Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.2.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.2.4\">Phrase</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.2.5\">Pre-training</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.2.6\">Iterative</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.3\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.3.1\">DenseX\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib30\" title=\"\">30</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.3.2\">FactoidWiki</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.3.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.3.4\">Proposition</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.3.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.3.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.4\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.4.1\">EAR\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib31\" title=\"\">31</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.4.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.4.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.4.4\">Sentence</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.4.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.4.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.5\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.5.1\">UPRISE\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib20\" title=\"\">20</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.5.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.5.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.5.4\">Sentence</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.5.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.5.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.6\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.6.1\">RAST\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib32\" title=\"\">32</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.6.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.6.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.6.4\">Sentence</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.6.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.6.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.7\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.7.1\">Self-Mem\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib17\" title=\"\">17</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.7.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.7.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.7.4\">Sentence</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.7.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.7.6\">Iterative</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.8\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.8.1\">FLARE\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib24\" title=\"\">24</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.8.2\">Search Engine,Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.8.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.8.4\">Sentence</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.8.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.8.6\">Adaptive</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.9\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.9.1\">PGRA\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib33\" title=\"\">33</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.9.2\">Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.9.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.9.4\">Sentence</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.9.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.9.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.10\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.10.1\">FILCO\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib34\" title=\"\">34</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.10.2\">Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.10.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.10.4\">Sentence</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.10.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.10.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.11\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.11.1\">RADA\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib35\" title=\"\">35</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.11.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.11.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.11.4\">Sentence</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.11.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.11.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.12\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.12.1\">Filter-rerank\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib36\" title=\"\">36</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.12.2\">Synthesized dataset</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.12.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.12.4\">Sentence</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.12.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.12.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.13\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.13.1\">R-GQA\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib37\" title=\"\">37</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.13.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.13.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.13.4\">Sentence Pair</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.13.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.13.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.14\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.14.1\">LLM-R\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib38\" title=\"\">38</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.14.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.14.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.14.4\">Sentence Pair</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.14.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.14.6\">Iterative</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.15\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.15.1\">TIGER\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib39\" title=\"\">39</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.15.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.15.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.15.4\">Item-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.15.5\">Pre-training</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.15.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.16\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.16.1\">LM-Indexer\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib40\" title=\"\">40</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.16.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.16.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.16.4\">Item-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.16.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.16.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.17\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.17.1\">BEQUE\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib9\" title=\"\">9</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.17.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.17.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.17.4\">Item-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.17.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.17.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.18\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.18.1\">CT-RAG\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib41\" title=\"\">41</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.18.2\">Synthesized dataset</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.18.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.18.4\">Item-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.18.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.18.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.19\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.19.1\">Atlas\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib42\" title=\"\">42</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.19.2\">Wikipedia, Common Crawl</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.19.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.19.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.19.5\">Pre-training</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.19.6\">Iterative</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.20\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.20.1\">RAVEN\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib43\" title=\"\">43</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.20.2\">Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.20.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.20.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.20.5\">Pre-training</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.20.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.21\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.21.1\">RETRO++\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib44\" title=\"\">44</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.21.2\">Pre-training Corpus</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.21.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.21.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.21.5\">Pre-training</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.21.6\">Iterative</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.22\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.22.1\">INSTRUCTRETRO\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib45\" title=\"\">45</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.22.2\">Pre-training corpus</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.22.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.22.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.22.5\">Pre-training</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.22.6\">Iterative</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.23\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.23.1\">RRR\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib7\" title=\"\">7</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.23.2\">Search Engine</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.23.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.23.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.23.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.23.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.24\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.24.1\">RA-e2e\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib46\" title=\"\">46</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.24.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.24.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.24.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.24.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.24.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.25\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.25.1\">PROMPTAGATOR\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib21\" title=\"\">21</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.25.2\">BEIR</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.25.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.25.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.25.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.25.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.26\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.26.1\">AAR\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib47\" title=\"\">47</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.26.2\">MSMARCO,Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.26.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.26.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.26.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.26.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.27\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.27.1\">RA-DIT\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib27\" title=\"\">27</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.27.2\">Common Crawl,Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.27.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.27.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.27.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.27.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.28\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.28.1\">RAG-Robust\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib48\" title=\"\">48</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.28.2\">Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.28.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.28.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.28.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.28.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.29\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.29.1\">RA-Long-Form\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib49\" title=\"\">49</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.29.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.29.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.29.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.29.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.29.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.30\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.30.1\">CoN\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib50\" title=\"\">50</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.30.2\">Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.30.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.30.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.30.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.30.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.31\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.31.1\">Self-RAG\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib25\" title=\"\">25</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.31.2\">Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.31.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.31.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.31.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.31.6\">Adaptive</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.32\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.32.1\">BGM\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib26\" title=\"\">26</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.32.2\">Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.32.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.32.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.32.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.32.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.33\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.33.1\">CoQ\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib51\" title=\"\">51</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.33.2\">Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.33.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.33.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.33.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.33.6\">Iterative</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.34\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.34.1\">Token-Elimination\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib52\" title=\"\">52</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.34.2\">Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.34.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.34.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.34.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.34.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.35\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.35.1\">PaperQA\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib53\" title=\"\">53</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.35.2\">Arxiv,Online Database,PubMed</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.35.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.35.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.35.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.35.6\">Iterative</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.36\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.36.1\">NoiseRAG\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib54\" title=\"\">54</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.36.2\">FactoidWiki</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.36.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.36.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.36.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.36.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.37\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.37.1\">IAG\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib55\" title=\"\">55</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.37.2\">Search Engine,Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.37.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.37.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.37.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.37.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.38\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.38.1\">NoMIRACL\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib56\" title=\"\">56</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.38.2\">Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.38.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.38.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.38.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.38.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.39\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.39.1\">ToC\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib57\" title=\"\">57</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.39.2\">Search Engine,Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.39.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.39.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.39.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.39.6\">Recursive</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.40\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.40.1\">SKR\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib58\" title=\"\">58</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.40.2\">Dataset-base,Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.40.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.40.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.40.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.40.6\">Adaptive</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.41\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.41.1\">ITRG\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib59\" title=\"\">59</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.41.2\">Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.41.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.41.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.41.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.41.6\">Iterative</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.42\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.42.1\">RAG-LongContext\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib60\" title=\"\">60</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.42.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.42.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.42.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.42.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.42.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.43\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.43.1\">ITER-RETGEN\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib14\" title=\"\">14</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.43.2\">Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.43.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.43.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.43.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.43.6\">Iterative</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.44\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.44.1\">IRCoT\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib61\" title=\"\">61</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.44.2\">Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.44.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.44.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.44.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.44.6\">Recursive</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.45\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.45.1\">LLM-Knowledge-Boundary\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib62\" title=\"\">62</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.45.2\">Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.45.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.45.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.45.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.45.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.46\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.46.1\">RAPTOR\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib63\" title=\"\">63</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.46.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.46.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.46.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.46.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.46.6\">Recursive</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.47\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.47.1\">RECITE\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib22\" title=\"\">22</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.47.2\">LLMs</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.47.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.47.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.47.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.47.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.48\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.48.1\">ICRALM\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib64\" title=\"\">64</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.48.2\">Pile,Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.48.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.48.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.48.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.48.6\">Iterative</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.49\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.49.1\">Retrieve-and-Sample\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib65\" title=\"\">65</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.49.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.49.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.49.4\">Doc</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.49.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.49.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.50\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.50.1\">Zemi\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib66\" title=\"\">66</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.50.2\">C4</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.50.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.50.4\">Doc</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.50.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.50.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.51\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.51.1\">CRAG\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib67\" title=\"\">67</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.51.2\">Arxiv</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.51.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.51.4\">Doc</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.51.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.51.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.52\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.52.1\">1-PAGER\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib68\" title=\"\">68</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.52.2\">Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.52.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.52.4\">Doc</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.52.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.52.6\">Iterative</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.53\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.53.1\">PRCA\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib69\" title=\"\">69</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.53.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.53.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.53.4\">Doc</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.53.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.53.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.54\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.54.1\">QLM-Doc-ranking\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib70\" title=\"\">70</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.54.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.54.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.54.4\">Doc</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.54.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.54.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.55\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.55.1\">Recomp\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib71\" title=\"\">71</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.55.2\">Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.55.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.55.4\">Doc</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.55.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.55.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.56\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.56.1\">DSP\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib23\" title=\"\">23</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.56.2\">Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.56.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.56.4\">Doc</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.56.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.56.6\">Iterative</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.57\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.57.1\">RePLUG\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib72\" title=\"\">72</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.57.2\">Pile</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.57.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.57.4\">Doc</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.57.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.57.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.58\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.58.1\">ARM-RAG\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib73\" title=\"\">73</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.58.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.58.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.58.4\">Doc</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.58.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.58.6\">Iterative</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.59\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.59.1\">GenRead\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib13\" title=\"\">13</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.59.2\">LLMs</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.59.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.59.4\">Doc</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.59.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.59.6\">Iterative</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.60\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.60.1\">UniMS-RAG\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib74\" title=\"\">74</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.60.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.60.3\">Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.60.4\">Multi</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.60.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.60.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.61\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.61.1\">CREA-ICL\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib19\" title=\"\">19</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.61.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.61.3\">Crosslingual,Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.61.4\">Sentence</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.61.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.61.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.62\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.62.1\">PKG\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib75\" title=\"\">75</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.62.2\">LLM</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.62.3\">Tabular,Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.62.4\">Chunk</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.62.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.62.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.63\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.63.1\">SANTA\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib76\" title=\"\">76</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.63.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.63.3\">Code,Text</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.63.4\">Item</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.63.5\">Pre-training</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.63.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.64\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.64.1\">SURGE\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib77\" title=\"\">77</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.64.2\">Freebase</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.64.3\">KG</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.64.4\">Sub-Graph</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.64.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.64.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.65\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.65.1\">MK-ToD\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib78\" title=\"\">78</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.65.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.65.3\">KG</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.65.4\">Entity</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.65.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.65.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.66\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.66.1\">Dual-Feedback-ToD\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib79\" title=\"\">79</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.66.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.66.3\">KG</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.66.4\">Entity Sequence</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.66.5\">Tuning</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.66.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.67\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.67.1\">KnowledGPT\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib15\" title=\"\">15</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.67.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.67.3\">KG</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.67.4\">Triplet</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.67.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.67.6\">Muti-time</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.68\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.68.1\">FABULA\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib80\" title=\"\">80</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.68.2\">Dataset-base,Graph</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.68.3\">KG</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.68.4\">Entity</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.68.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.68.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.69\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.69.1\">HyKGE\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib81\" title=\"\">81</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.69.2\">CMeKG</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.69.3\">KG</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.69.4\">Entity</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.69.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.69.6\">Once</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.70\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.70.1\">KALMV\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib82\" title=\"\">82</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.70.2\">Wikipedia</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.70.3\">KG</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.70.4\">Triplet</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.70.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.70.6\">Iterative</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.71\">\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.71.1\">RoG\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib83\" title=\"\">83</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.71.2\">Freebase</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.71.3\">KG</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.71.4\">Triplet</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.71.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.71.6\">Iterative</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.72\">\\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T1.1.1.72.1\">G-Retriever\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib84\" title=\"\">84</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T1.1.1.72.2\">Dataset-base</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T1.1.1.72.3\">TextGraph</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T1.1.1.72.4\">Sub-Graph</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T1.1.1.72.5\">Inference</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T1.1.1.72.6\">Once</td>\\n</tr>\\n</table>\\n</span></div>\\n</figure>\\n</section>\\n</section>\\n<section class=\"ltx_section\" id=\"S3\">\\n<h2 class=\"ltx_title ltx_title_section\">\\n<span class=\"ltx_tag ltx_tag_section\">III </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.1.1\">Retrieval</span>\\n</h2>\\n<div class=\"ltx_para\" id=\"S3.p1\">\\n<p class=\"ltx_p\" id=\"S3.p1.1\">In the context of RAG, it is crucial to efficiently retrieve relevant documents from the data source. There are several key issues involved, such as the retrieval source, retrieval granularity, pre-processing of the retrieval, and selection of the corresponding embedding model.</p>\\n</div>\\n<section class=\"ltx_subsection\" id=\"S3.SS1\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S3.SS1.5.1.1\">III-A</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S3.SS1.6.2\">Retrieval Source</span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S3.SS1.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS1.p1.1\">RAG relies on external knowledge to enhance LLMs, while the type of retrieval source and the granularity of retrieval units both affect the final generation results.</p>\\n</div>\\n<section class=\"ltx_subsubsection\" id=\"S3.SS1.SSS1\">\\n<h4 class=\"ltx_title ltx_title_subsubsection\">\\n<span class=\"ltx_tag ltx_tag_subsubsection\"><span class=\"ltx_text\" id=\"S3.SS1.SSS1.5.1.1\">III-A</span>1 </span>Data Structure</h4>\\n<div class=\"ltx_para\" id=\"S3.SS1.SSS1.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS1.SSS1.p1.1\">Initially, text is s the mainstream source of retrieval. Subsequently, the retrieval source expanded to include semi-structured data (PDF) and structured data (Knowledge Graph, KG) for enhancement. In addition to retrieving from original external sources, there is also a growing trend in recent researches towards utilizing content generated by LLMs themselves for retrieval and enhancement purposes.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S3.SS1.SSS1.p2\">\\n<p class=\"ltx_p\" id=\"S3.SS1.SSS1.p2.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS1.SSS1.p2.1.1\">Unstructured Data</em>, such as text, is the most widely used retrieval source, which are mainly gathered from corpus. For open-domain question-answering (ODQA) tasks, the primary retrieval sources are Wikipedia Dump with the current major versions including HotpotQA\\xa0<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hotpotqa.github.io/wiki-readme.html\" title=\"\">https://hotpotqa.github.io/wiki-readme.html</a></span></span></span> (1st October , 2017), DPR<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/facebookresearch/DPR\" title=\"\">https://github.com/facebookresearch/DPR</a></span></span></span> (20 December, 2018). In addition to encyclopedic data, common unstructured data includes cross-lingual text\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib19\" title=\"\">19</a>]</cite> and domain-specific data (such as medical\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib67\" title=\"\">67</a>]</cite>and legal domains\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib29\" title=\"\">29</a>]</cite>).</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S3.SS1.SSS1.p3\">\\n<p class=\"ltx_p\" id=\"S3.SS1.SSS1.p3.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS1.SSS1.p3.1.1\">Semi-structured data</em>. typically refers to data that contains a combination of text and table information, such as PDF. Handling semi-structured data poses challenges for conventional RAG systems due to two main reasons. Firstly, text splitting processes may inadvertently separate tables, leading to data corruption during retrieval. Secondly, incorporating tables into the data can complicate semantic similarity searches. When dealing with semi-structured data, one approach involves leveraging the code capabilities of LLMs to execute Text-2-SQL queries on tables within databases, such as TableGPT\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib85\" title=\"\">85</a>]</cite>. Alternatively, tables can be transformed into text format for further analysis using text-based methods\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib75\" title=\"\">75</a>]</cite>. However, both of these methods are not optimal solutions, indicating substantial research opportunities in this area.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S3.SS1.SSS1.p4\">\\n<p class=\"ltx_p\" id=\"S3.SS1.SSS1.p4.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS1.SSS1.p4.1.1\">Structured data</em>, such as knowledge graphs (KGs)\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib86\" title=\"\">86</a>]</cite> , which are typically verified and can provide more precise information. KnowledGPT\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib15\" title=\"\">15</a>]</cite> generates KB search queries and stores knowledge in a personalized base, enhancing the RAG model’s knowledge richness. In response to the limitations of LLMs in understanding and answering questions about textual graphs, G-Retriever\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib84\" title=\"\">84</a>]</cite> integrates Graph Neural Networks (GNNs), LLMs and RAG, enhancing graph comprehension and question-answering capabilities through soft prompting of the LLM, and employs the Prize-Collecting Steiner Tree (PCST) optimization problem for targeted graph retrieval. On the contrary, it requires additional effort to build, validate, and maintain structured databases. On the contrary, it requires additional effort to build, validate, and maintain structured databases.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S3.SS1.SSS1.p5\">\\n<p class=\"ltx_p\" id=\"S3.SS1.SSS1.p5.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS1.SSS1.p5.1.1\">LLMs-Generated Content.</em> Addressing the limitations of external auxiliary information in RAG, some research has focused on exploiting LLMs’ internal knowledge. SKR\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib58\" title=\"\">58</a>]</cite> classifies questions as known or unknown, applying retrieval enhancement selectively. GenRead\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib13\" title=\"\">13</a>]</cite> replaces the retriever with an LLM generator, finding that LLM-generated contexts often contain more accurate answers due to better alignment with the pre-training objectives of causal language modeling. Selfmem\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib17\" title=\"\">17</a>]</cite> iteratively creates an unbounded memory pool with a retrieval-enhanced generator, using a memory selector to choose outputs that serve as dual problems to the original question, thus self-enhancing the generative model. These methodologies underscore the breadth of innovative data source utilization in RAG, striving to improve model performance and task effectiveness.\\n</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsubsection\" id=\"S3.SS1.SSS2\">\\n<h4 class=\"ltx_title ltx_title_subsubsection\">\\n<span class=\"ltx_tag ltx_tag_subsubsection\"><span class=\"ltx_text\" id=\"S3.SS1.SSS2.5.1.1\">III-A</span>2 </span>Retrieval Granularity</h4>\\n<div class=\"ltx_para\" id=\"S3.SS1.SSS2.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS1.SSS2.p1.1\">Another important factor besides the data format of the retrieval source is the granularity of the retrieved data. Coarse-grained retrieval units theoretically can provide more relevant information for the problem, but they may also contain redundant content, which could distract the retriever and language models in downstream tasks\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib87\" title=\"\">87</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib50\" title=\"\">50</a>]</cite>. On the other hand, fine-grained retrieval unit granularity increases the burden of retrieval and does not guarantee semantic integrity and meeting the required knowledge. Choosing the appropriate retrieval granularity during inference can be a simple and effective strategy to improve the retrieval and downstream task performance of dense retrievers.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S3.SS1.SSS2.p2\">\\n<p class=\"ltx_p\" id=\"S3.SS1.SSS2.p2.1\">In text, retrieval granularity ranges from fine to coarse, including Token, Phrase, Sentence, Proposition, Chunks, Document. Among them, DenseX\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib30\" title=\"\">30</a>]</cite>proposed the concept of using propositions as retrieval units. Propositions are defined as atomic expressions in the text, each encapsulating a unique factual segment and presented in a concise, self-contained natural language format. This approach aims to enhance retrieval precision and relevance. On the Knowledge Graph (KG), retrieval granularity includes Entity, Triplet, and sub-Graph. The granularity of retrieval can also be adapted to downstream tasks, such as retrieving Item IDs\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib40\" title=\"\">40</a>]</cite>in recommendation tasks and Sentence pairs\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib38\" title=\"\">38</a>]</cite>. Detailed information is illustrated in Table\\xa0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S2.T1\" title=\"TABLE I ‣ II-D RAG vs Fine-tuning ‣ II Overview of RAG ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\\n</div>\\n</section>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S3.SS2\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S3.SS2.5.1.1\">III-B</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S3.SS2.6.2\">Indexing Optimization</span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S3.SS2.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS2.p1.1\">In the Indexing phase, documents will be processed, segmented, and transformed into Embeddings to be stored in a vector database. The quality of index construction determines whether the correct context can be obtained in the retrieval phase.</p>\\n</div>\\n<section class=\"ltx_subsubsection\" id=\"S3.SS2.SSS1\">\\n<h4 class=\"ltx_title ltx_title_subsubsection\">\\n<span class=\"ltx_tag ltx_tag_subsubsection\"><span class=\"ltx_text\" id=\"S3.SS2.SSS1.5.1.1\">III-B</span>1 </span>Chunking Strategy</h4>\\n<div class=\"ltx_para\" id=\"S3.SS2.SSS1.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS2.SSS1.p1.1\">The most common method is to split the document into chunks on a fixed number of tokens (e.g., 100, 256, 512)\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib88\" title=\"\">88</a>]</cite>. Larger chunks can capture more context, but they also generate more noise, requiring longer processing time and higher costs. While smaller chunks may not fully convey the necessary context, they do have less noise. However, chunks leads to truncation within sentences, prompting the optimization of a recursive splits and sliding window methods, enabling layered retrieval by merging globally related information across multiple retrieval processes\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib89\" title=\"\">89</a>]</cite>. Nevertheless, these approaches still cannot strike a balance between semantic completeness and context length. Therefore, methods like Small2Big have been proposed, where sentences (small) are used as the retrieval unit, and the preceding and following sentences are provided as (big) context to LLMs\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib90\" title=\"\">90</a>]</cite>.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsubsection\" id=\"S3.SS2.SSS2\">\\n<h4 class=\"ltx_title ltx_title_subsubsection\">\\n<span class=\"ltx_tag ltx_tag_subsubsection\"><span class=\"ltx_text\" id=\"S3.SS2.SSS2.5.1.1\">III-B</span>2 </span>Metadata Attachments</h4>\\n<div class=\"ltx_para\" id=\"S3.SS2.SSS2.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS2.SSS2.p1.1\">Chunks can be enriched with metadata information such as page number, file name, author,category timestamp. Subsequently, retrieval can be filtered based on this metadata, limiting the scope of the retrieval. Assigning different weights to document timestamps during retrieval can achieve time-aware RAG, ensuring the freshness of knowledge and avoiding outdated information.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S3.SS2.SSS2.p2\">\\n<p class=\"ltx_p\" id=\"S3.SS2.SSS2.p2.1\">In addition to extracting metadata from the original documents, metadata can also be artificially constructed. For example, adding summaries of paragraph, as well as introducing hypothetical questions. This method is also known as Reverse HyDE. Specifically, using LLM to generate questions that can be answered by the document, then calculating the similarity between the original question and the hypothetical question during retrieval to reduce the semantic gap between the question and the answer.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsubsection\" id=\"S3.SS2.SSS3\">\\n<h4 class=\"ltx_title ltx_title_subsubsection\">\\n<span class=\"ltx_tag ltx_tag_subsubsection\"><span class=\"ltx_text\" id=\"S3.SS2.SSS3.5.1.1\">III-B</span>3 </span>Structural Index</h4>\\n<div class=\"ltx_para\" id=\"S3.SS2.SSS3.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS2.SSS3.p1.1\">One effective method for enhancing information retrieval is to establish a hierarchical structure for the documents. By constructing In structure, RAG system can expedite the retrieval and processing of pertinent data.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S3.SS2.SSS3.p2\">\\n<p class=\"ltx_p\" id=\"S3.SS2.SSS3.p2.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS2.SSS3.p2.1.1\">Hierarchical index structure</em>. File are arranged in parent-child relationships, with chunks linked to them. Data summaries are stored at each node, aiding in the swift traversal of data and assisting the RAG system in determining which chunks to extract. This approach can also mitigate the illusion caused by block extraction issues.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S3.SS2.SSS3.p3\">\\n<p class=\"ltx_p\" id=\"S3.SS2.SSS3.p3.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS2.SSS3.p3.1.1\">Knowledge Graph index</em>. Utilize KG in constructing the hierarchical structure of documents contributes to maintaining consistency. It delineates the connections between different concepts and entities, markedly reducing the potential for illusions. Another advantage is the transformation of the information retrieval process into instructions that LLM can comprehend, thereby enhancing the accuracy of knowledge retrieval and enabling LLM to generate contextually coherent responses, thus improving the overall efficiency of the RAG system. To capture the logical relationship between document content and structure, KGP\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib91\" title=\"\">91</a>]</cite> proposed a method of building an index between multiple documents using KG. This KG consists of nodes (representing paragraphs or structures in the documents, such as pages and tables) and edges (indicating semantic/lexical similarity between paragraphs or relationships within the document structure), effectively addressing knowledge retrieval and reasoning problems in a multi-document environment.\\n</p>\\n</div>\\n</section>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S3.SS3\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S3.SS3.5.1.1\">III-C</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S3.SS3.6.2\">Query Optimization</span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S3.SS3.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS3.p1.1\">One of the primary challenges with Naive RAG is its direct reliance on the user’s original query as the basis for retrieval. Formulating a precise and clear question is difficult, and imprudent queries result in subpar retrieval effectiveness. Sometimes, the question itself is complex, and the language is not well-organized. Another difficulty lies in language complexity ambiguity. Language models often struggle when dealing with specialized vocabulary or ambiguous abbreviations with multiple meanings. For instance, they may not discern whether “LLM” refers to <span class=\"ltx_text ltx_font_italic\" id=\"S3.SS3.p1.1.1\">large language model</span> or a <span class=\"ltx_text ltx_font_italic\" id=\"S3.SS3.p1.1.2\">Master of Laws</span> in a legal context.</p>\\n</div>\\n<section class=\"ltx_subsubsection\" id=\"S3.SS3.SSS1\">\\n<h4 class=\"ltx_title ltx_title_subsubsection\">\\n<span class=\"ltx_tag ltx_tag_subsubsection\"><span class=\"ltx_text\" id=\"S3.SS3.SSS1.5.1.1\">III-C</span>1 </span>Query Expansion</h4>\\n<div class=\"ltx_para\" id=\"S3.SS3.SSS1.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS3.SSS1.p1.1\">Expanding a single query into multiple queries enriches the content of the query, providing further context to address any lack of specific nuances, thereby ensuring the optimal relevance of the generated answers.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S3.SS3.SSS1.p2\">\\n<p class=\"ltx_p\" id=\"S3.SS3.SSS1.p2.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS3.SSS1.p2.1.1\">Multi-Query</em>. By employing prompt engineering to expand queries via LLMs, these queries can then be executed in parallel. The expansion of queries is not random, but rather meticulously designed.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S3.SS3.SSS1.p3\">\\n<p class=\"ltx_p\" id=\"S3.SS3.SSS1.p3.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS3.SSS1.p3.1.1\">Sub-Query</em>. The process of sub-question planning represents the generation of the necessary sub-questions to contextualize and fully answer the original question when combined. This process of adding relevant context is, in principle, similar to query expansion. Specifically, a complex question can be decomposed into a series of simpler sub-questions using the least-to-most prompting method\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib92\" title=\"\">92</a>]</cite>.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S3.SS3.SSS1.p4\">\\n<p class=\"ltx_p\" id=\"S3.SS3.SSS1.p4.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS3.SSS1.p4.1.1\">Chain-of-Verification(CoVe)</em>. The expanded queries undergo validation by LLM to achieve the effect of reducing hallucinations. Validated expanded queries typically exhibit higher reliability\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib93\" title=\"\">93</a>]</cite>.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsubsection\" id=\"S3.SS3.SSS2\">\\n<h4 class=\"ltx_title ltx_title_subsubsection\">\\n<span class=\"ltx_tag ltx_tag_subsubsection\"><span class=\"ltx_text\" id=\"S3.SS3.SSS2.5.1.1\">III-C</span>2 </span>Query Transformation</h4>\\n<div class=\"ltx_para\" id=\"S3.SS3.SSS2.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS3.SSS2.p1.1\">The core concept is to retrieve chunks based on a transformed query instead of the user’s original query.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S3.SS3.SSS2.p2\">\\n<p class=\"ltx_p\" id=\"S3.SS3.SSS2.p2.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS3.SSS2.p2.1.1\">Query Rewrite</em>.The original queries are not always optimal for LLM retrieval, especially in real-world scenarios. Therefore, we can prompt LLM to rewrite the queries. In addition to using LLM for query rewriting, specialized smaller language models, such as RRR (Rewrite-retrieve-read)\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib7\" title=\"\">7</a>]</cite>. The implementation of the query rewrite method in the Taobao, known as BEQUE\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib9\" title=\"\">9</a>]</cite> has notably enhanced recall effectiveness for long-tail queries, resulting in a rise in GMV.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S3.SS3.SSS2.p3\">\\n<p class=\"ltx_p\" id=\"S3.SS3.SSS2.p3.1\">Another query transformation method is to use prompt engineering to let LLM generate a query based on the original query for subsequent retrieval. HyDE\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib11\" title=\"\">11</a>]</cite> construct hypothetical documents (assumed answers to the original query). It focuses on embedding similarity from answer to answer rather than seeking embedding similarity for the problem or query. Using the Step-back Prompting method\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib10\" title=\"\">10</a>]</cite>, the original query is abstracted to generate a high-level concept question (step-back question). In the RAG system, both the step-back question and the original query are used for retrieval, and both the results are utilized as the basis for language model answer generation.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsubsection\" id=\"S3.SS3.SSS3\">\\n<h4 class=\"ltx_title ltx_title_subsubsection\">\\n<span class=\"ltx_tag ltx_tag_subsubsection\"><span class=\"ltx_text\" id=\"S3.SS3.SSS3.5.1.1\">III-C</span>3 </span>Query Routing</h4>\\n<div class=\"ltx_para\" id=\"S3.SS3.SSS3.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS3.SSS3.p1.1\">Based on varying queries, routing to distinct RAG pipeline,which is suitable for a versatile RAG system designed to accommodate diverse scenarios.\\n</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S3.SS3.SSS3.p2\">\\n<p class=\"ltx_p\" id=\"S3.SS3.SSS3.p2.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS3.SSS3.p2.1.1\">Metadata Router/ Filter</em>. The first step involves extracting keywords (entity) from the query, followed by filtering based on the keywords and metadata within the chunks to narrow down the search scope.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S3.SS3.SSS3.p3\">\\n<p class=\"ltx_p\" id=\"S3.SS3.SSS3.p3.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS3.SSS3.p3.1.1\">Semantic Router</em> is another method of routing involves leveraging the semantic information of the query. Specific apprach see Semantic Router\\xa0<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/aurelio-labs/semantic-router\" title=\"\">https://github.com/aurelio-labs/semantic-router</a></span></span></span>. Certainly, a hybrid routing approach can also be employed, combining both semantic and metadata-based methods for enhanced query routing.</p>\\n</div>\\n</section>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S3.SS4\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S3.SS4.5.1.1\">III-D</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S3.SS4.6.2\">Embedding</span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S3.SS4.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS4.p1.1\">In RAG, retrieval is achieved by calculating the similarity (e.g. cosine similarity) between the embeddings of the question and document chunks, where the semantic representation capability of embedding models plays a key role. This mainly includes a sparse encoder (BM25) and a dense retriever (BERT architecture Pre-training language models). Recent research has introduced prominent embedding models such as AngIE, Voyage, BGE,etc\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib94\" title=\"\">94</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib95\" title=\"\">95</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib96\" title=\"\">96</a>]</cite>, which are benefit from multi-task instruct tuning. Hugging Face’s MTEB leaderboard\\xa0<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/spaces/mteb/leaderboard\" title=\"\">https://huggingface.co/spaces/mteb/leaderboard</a></span></span></span> evaluates embedding models across 8 tasks, covering 58 datasests. Additionally, C-MTEB focuses on Chinese capability, covering 6 tasks and 35 datasets. There is no one-size-fits-all answer to “which embedding model to use.” However, some specific models are better suited for particular use cases.</p>\\n</div>\\n<section class=\"ltx_subsubsection\" id=\"S3.SS4.SSS1\">\\n<h4 class=\"ltx_title ltx_title_subsubsection\">\\n<span class=\"ltx_tag ltx_tag_subsubsection\"><span class=\"ltx_text\" id=\"S3.SS4.SSS1.5.1.1\">III-D</span>1 </span>Mix/hybrid Retrieval </h4>\\n<div class=\"ltx_para\" id=\"S3.SS4.SSS1.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS4.SSS1.p1.1\">Sparse and dense embedding approaches capture different relevance features and can benefit from each other by leveraging complementary relevance information. For instance, sparse retrieval models can be used to provide initial search results for training dense retrieval models. Additionally, pre-training language models (PLMs) can be utilized to learn term weights to enhance sparse retrieval. Specifically, it also demonstrates that sparse retrieval models can enhance the zero-shot retrieval capability of dense retrieval models and assist dense retrievers in handling queries containing rare entities, thereby improving robustness.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsubsection\" id=\"S3.SS4.SSS2\">\\n<h4 class=\"ltx_title ltx_title_subsubsection\">\\n<span class=\"ltx_tag ltx_tag_subsubsection\"><span class=\"ltx_text\" id=\"S3.SS4.SSS2.5.1.1\">III-D</span>2 </span>Fine-tuning Embedding Model</h4>\\n<div class=\"ltx_para\" id=\"S3.SS4.SSS2.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS4.SSS2.p1.1\">In instances where the context significantly deviates from pre-training corpus, particularly within highly specialized disciplines such as healthcare, legal practice, and other sectors replete with proprietary jargon, fine-tuning the embedding model on your own domain dataset becomes essential to mitigate such discrepancies.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S3.SS4.SSS2.p2\">\\n<p class=\"ltx_p\" id=\"S3.SS4.SSS2.p2.1\">In addition to supplementing domain knowledge, another purpose of fine-tuning is to align the retriever and generator, for example, using the results of LLM as the supervision signal for fine-tuning, known as LSR (LM-supervised Retriever). PROMPTAGATOR\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib21\" title=\"\">21</a>]</cite> utilizes the LLM as a few-shot query generator to create task-specific retrievers, addressing challenges in supervised fine-tuning, particularly in data-scarce domains. Another approach, LLM-Embedder\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib97\" title=\"\">97</a>]</cite>, exploits LLMs to generate reward signals across multiple downstream tasks. The retriever is fine-tuned with two types of supervised signals: hard labels for the dataset and soft rewards from the LLMs. This dual-signal approach fosters a more effective fine-tuning process, tailoring the embedding model to diverse downstream applications. REPLUG\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib72\" title=\"\">72</a>]</cite> utilizes a retriever and an LLM to calculate the probability distributions of the retrieved documents and then performs supervised training by computing the KL divergence. This straightforward and effective training method enhances the performance of the retrieval model by using an LM as the supervisory signal, eliminating the need for specific cross-attention mechanisms. Moreover, inspired by RLHF (Reinforcement Learning from Human Feedback), utilizing LM-based feedback to reinforce the retriever through reinforcement learning.</p>\\n</div>\\n</section>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S3.SS5\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S3.SS5.5.1.1\">III-E</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S3.SS5.6.2\">Adapter</span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S3.SS5.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS5.p1.1\">Fine-tuning models may present challenges, such as integrating functionality through an API or addressing constraints arising from limited local computational resources. Consequently, some approaches opt to incorporate an external adapter to aid in alignment.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S3.SS5.p2\">\\n<p class=\"ltx_p\" id=\"S3.SS5.p2.1\">To optimize the multi-task capabilities of LLM, UPRISE\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib20\" title=\"\">20</a>]</cite> trained a lightweight prompt retriever that can automatically retrieve prompts from a pre-built prompt pool that are suitable for a given zero-shot task input. AAR (Augmentation-Adapted Retriver)\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib47\" title=\"\">47</a>]</cite> introduces a universal adapter designed to accommodate multiple downstream tasks. While PRCA\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib69\" title=\"\">69</a>]</cite> add a pluggable reward-driven contextual adapter to enhance performance on specific tasks. BGM\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib26\" title=\"\">26</a>]</cite> keeps the retriever and LLM fixed,and trains a bridge Seq2Seq model in between. The bridge model aims to transform the retrieved information into a format that LLMs can work with effectively, allowing it to not only rerank but also dynamically select passages for each query, and potentially employ more advanced strategies like repetition. Furthermore, PKG introduces an innovative method for integrating knowledge into white-box models via directive fine-tuning\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib75\" title=\"\">75</a>]</cite>. In this approach, the retriever module is directly substituted to generate relevant documents according to a query. This method assists in addressing the difficulties encountered during the fine-tuning process and enhances model performance.</p>\\n</div>\\n</section>\\n</section>\\n<section class=\"ltx_section\" id=\"S4\">\\n<h2 class=\"ltx_title ltx_title_section\">\\n<span class=\"ltx_tag ltx_tag_section\">IV </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.1.1\">Generation</span>\\n</h2>\\n<div class=\"ltx_para\" id=\"S4.p1\">\\n<p class=\"ltx_p\" id=\"S4.p1.1\">After retrieval, it is not a good practice to directly input all the retrieved information to the LLM for answering questions. Following will introduce adjustments from two perspectives: adjusting the retrieved content and adjusting the LLM.</p>\\n</div>\\n<section class=\"ltx_subsection\" id=\"S4.SS1\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S4.SS1.5.1.1\">IV-A</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S4.SS1.6.2\">Context Curation</span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S4.SS1.p1\">\\n<p class=\"ltx_p\" id=\"S4.SS1.p1.1\">Redundant information can interfere with the final generation of LLM, and overly long contexts can also lead LLM to the “Lost in the middle” problem\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib98\" title=\"\">98</a>]</cite>. Like humans, LLM tends to only focus on the beginning and end of long texts, while forgetting the middle portion. Therefore, in the RAG system, we typically need to further process the retrieved content.</p>\\n</div>\\n<section class=\"ltx_subsubsection\" id=\"S4.SS1.SSS1\">\\n<h4 class=\"ltx_title ltx_title_subsubsection\">\\n<span class=\"ltx_tag ltx_tag_subsubsection\"><span class=\"ltx_text\" id=\"S4.SS1.SSS1.5.1.1\">IV-A</span>1 </span>Reranking</h4>\\n<div class=\"ltx_para\" id=\"S4.SS1.SSS1.p1\">\\n<p class=\"ltx_p\" id=\"S4.SS1.SSS1.p1.1\">Reranking fundamentally reorders document chunks to highlight the most pertinent results first, effectively reducing the overall document pool, severing a dual purpose in information retrieval, acting as both an enhancer and a filter, delivering refined inputs for more precise language model processing\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib70\" title=\"\">70</a>]</cite>. Reranking can be performed using rule-based methods that depend on predefined metrics like Diversity, Relevance, and MRR, or model-based approaches like Encoder-Decoder models from the BERT series (e.g., SpanBERT), specialized reranking models such as Cohere rerank or bge-raranker-large, and general large language models like GPT\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib99\" title=\"\">99</a>]</cite>.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsubsection\" id=\"S4.SS1.SSS2\">\\n<h4 class=\"ltx_title ltx_title_subsubsection\">\\n<span class=\"ltx_tag ltx_tag_subsubsection\"><span class=\"ltx_text\" id=\"S4.SS1.SSS2.5.1.1\">IV-A</span>2 </span>Context Selection/Compression</h4>\\n<div class=\"ltx_para\" id=\"S4.SS1.SSS2.p1\">\\n<p class=\"ltx_p\" id=\"S4.SS1.SSS2.p1.1\">A common misconception in the RAG process is the belief that retrieving as many relevant documents as possible and concatenating them to form a lengthy retrieval prompt is beneficial. However, excessive context can introduce more noise, diminishing the LLM’s perception of key information .</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S4.SS1.SSS2.p2\">\\n<p class=\"ltx_p\" id=\"S4.SS1.SSS2.p2.1\">(Long) LLMLingua\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib100\" title=\"\">100</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib101\" title=\"\">101</a>]</cite> utilize small language models (SLMs) such as GPT-2 Small or LLaMA-7B, to detect and remove unimportant tokens, transforming it into a form that is challenging for humans to comprehend but well understood by LLMs. This approach presents a direct and practical method for prompt compression, eliminating the need for additional training of LLMs while balancing language integrity and compression ratio. PRCA tackled this issue by training an information extractor\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib69\" title=\"\">69</a>]</cite>. Similarly, RECOMP adopts a comparable approach by training an information condenser using contrastive learning\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib71\" title=\"\">71</a>]</cite>. Each training data point consists of one positive sample and five negative samples, and the encoder undergoes training using contrastive loss throughout this process\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib102\" title=\"\">102</a>]</cite> .</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S4.SS1.SSS2.p3\">\\n<p class=\"ltx_p\" id=\"S4.SS1.SSS2.p3.1\">In addition to compressing the context, reducing the number of documents aslo helps improve the accuracy of the model’s answers. Ma et al.\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib103\" title=\"\">103</a>]</cite> propose the “Filter-Reranker” paradigm, which combines the strengths of LLMs and SLMs. In this paradigm, SLMs serve as filters, while LLMs function as reordering agents. The research shows that instructing LLMs to rearrange challenging samples identified by SLMs leads to significant improvements in various Information Extraction (IE) tasks. Another straightforward and effective approach involves having the LLM evaluate the retrieved content before generating the final answer. This allows the LLM to filter out documents with poor relevance through LLM critique. For instance, in Chatlaw\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib104\" title=\"\">104</a>]</cite>, the LLM is prompted to self-suggestion on the referenced legal provisions to assess their relevance.</p>\\n</div>\\n</section>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S4.SS2\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S4.SS2.5.1.1\">IV-B</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S4.SS2.6.2\">LLM Fine-tuning</span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S4.SS2.p1\">\\n<p class=\"ltx_p\" id=\"S4.SS2.p1.1\">Targeted fine-tuning based on the scenario and data characteristics on LLMs can yield better results. This is also one of the greatest advantages of using on-premise LLMs. When LLMs lack data in a specific domain, additional knowledge can be provided to the LLM through fine-tuning. Huggingface’s fine-tuning data can also be used as an initial step.\\n</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S4.SS2.p2\">\\n<p class=\"ltx_p\" id=\"S4.SS2.p2.1\">Another benefit of fine-tuning is the ability to adjust the model’s input and output. For example, it can enable LLM to adapt to specific data formats and generate responses in a particular style as instructed\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib37\" title=\"\">37</a>]</cite>. For retrieval tasks that engage with structured data, the SANTA framework\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib76\" title=\"\">76</a>]</cite> implements a tripartite training regimen to effectively encapsulate both structural and semantic nuances. The initial phase focuses on the retriever, where contrastive learning is harnessed to refine the query and document embeddings.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S4.SS2.p3\">\\n<p class=\"ltx_p\" id=\"S4.SS2.p3.1\">Aligning LLM outputs with human or retriever preferences through reinforcement learning is a potential approach. For instance, manually annotating the final generated answers and then providing feedback through reinforcement learning. In addition to aligning with human preferences, it is also possible to align with the preferences of fine-tuned models and retrievers\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib79\" title=\"\">79</a>]</cite>. When circumstances prevent access to powerful proprietary models or larger parameter open-source models, a simple and effective method is to distill the more powerful models(e.g. GPT-4). Fine-tuning of LLM can also be coordinated with fine-tuning of the retriever to align preferences. A typical approach, such as RA-DIT\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib27\" title=\"\">27</a>]</cite>, aligns the scoring functions between Retriever and Generator using KL divergence.</p>\\n</div>\\n</section>\\n</section>\\n<section class=\"ltx_section\" id=\"S5\">\\n<h2 class=\"ltx_title ltx_title_section\">\\n<span class=\"ltx_tag ltx_tag_section\">V </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.1.1\">Augmentation process in RAG</span>\\n</h2>\\n<div class=\"ltx_para\" id=\"S5.p1\">\\n<p class=\"ltx_p\" id=\"S5.p1.1\">In the domain of RAG, the standard practice often involves a singular (once) retrieval step followed by generation, which can lead to inefficiencies and sometimes is typically insufficient for complex problems demanding multi-step reasoning, as it provides a limited scope of information\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib105\" title=\"\">105</a>]</cite>. Many studies have optimized the retrieval process in response to this issue, and we have summarised them in Figure\\xa0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S5.F5\" title=\"Figure 5 ‣ V Augmentation process in RAG ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\\n</div>\\n<figure class=\"ltx_figure\" id=\"S5.F5\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"314\" id=\"S5.F5.g1\" src=\"extracted/5498883/images/aug_process.png\" width=\"598\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 5: </span>In addition to the most common once retrieval, RAG also includes three types of retrieval augmentation processes. (left) Iterative retrieval involves alternating between retrieval and generation, allowing for richer and more targeted context from the knowledge base at each step. (Middle) Recursive retrieval involves gradually refining the user query and breaking down the problem into sub-problems, then continuously solving complex problems through retrieval and generation. (Right) Adaptive retrieval focuses on enabling the RAG system to autonomously determine whether external knowledge retrieval is necessary and when to stop retrieval and generation, often utilizing LLM-generated special tokens for control.</figcaption>\\n</figure>\\n<section class=\"ltx_subsection\" id=\"S5.SS1\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S5.SS1.5.1.1\">V-A</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S5.SS1.6.2\">Iterative Retrieval</span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S5.SS1.p1\">\\n<p class=\"ltx_p\" id=\"S5.SS1.p1.1\">Iterative retrieval is a process where the knowledge base is repeatedly searched based on the initial query and the text generated so far, providing a more comprehensive knowledge base for LLMs. This approach has been shown to enhance the robustness of subsequent answer generation by offering additional contextual references through multiple retrieval iterations. However, it may be affected by semantic discontinuity and the accumulation of irrelevant information. ITER-RETGEN\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib14\" title=\"\">14</a>]</cite> employs a synergistic approach that leverages “retrieval-enhanced generation” alongside “generation-enhanced retrieval” for tasks that necessitate the reproduction of specific information. The model harnesses the content required to address the input task as a contextual basis for retrieving pertinent knowledge, which in turn facilitates the generation of improved responses in subsequent iterations.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S5.SS2\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S5.SS2.5.1.1\">V-B</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S5.SS2.6.2\">Recursive Retrieval</span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S5.SS2.p1\">\\n<p class=\"ltx_p\" id=\"S5.SS2.p1.1\">Recursive retrieval is often used in information retrieval and NLP to improve the depth and relevance of search results. The process involves iteratively refining search queries based on the results obtained from previous searches. Recursive Retrieval aims to enhance the search experience by gradually converging on the most pertinent information through a feedback loop. IRCoT\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib61\" title=\"\">61</a>]</cite> uses chain-of-thought to guide the retrieval process and refines the CoT with the obtained retrieval results. ToC\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib57\" title=\"\">57</a>]</cite> creates a clarification tree that systematically optimizes the ambiguous parts in the Query. It can be particularly useful in complex search scenarios where the user’s needs are not entirely clear from the outset or where the information sought is highly specialized or nuanced. The recursive nature of the process allows for continuous learning and adaptation to the user’s requirements, often resulting in improved satisfaction with the search outcomes.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S5.SS2.p2\">\\n<p class=\"ltx_p\" id=\"S5.SS2.p2.1\">To address specific data scenarios, recursive retrieval and multi-hop retrieval techniques are utilized together. Recursive retrieval involves a structured index to process and retrieve data in a hierarchical manner, which may include summarizing sections of a document or lengthy PDF before performing a retrieval based on this summary. Subsequently, a secondary retrieval within the document refines the search, embodying the recursive nature of the process. In contrast, multi-hop retrieval is designed to delve deeper into graph-structured data sources, extracting interconnected information\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib106\" title=\"\">106</a>]</cite>.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S5.SS3\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S5.SS3.5.1.1\">V-C</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S5.SS3.6.2\">Adaptive Retrieval</span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S5.SS3.p1\">\\n<p class=\"ltx_p\" id=\"S5.SS3.p1.1\">Adaptive retrieval methods, exemplified by Flare\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib24\" title=\"\">24</a>]</cite> and Self-RAG\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib25\" title=\"\">25</a>]</cite>, refine the RAG framework by enabling LLMs to actively determine the optimal moments and content for retrieval, thus enhancing the efficiency and relevance of the information sourced.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S5.SS3.p2\">\\n<p class=\"ltx_p\" id=\"S5.SS3.p2.1\">These methods are part of a broader trend wherein LLMs employ active judgment in their operations, as seen in model agents like AutoGPT, Toolformer, and Graph-Toolformer\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib107\" title=\"\">107</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib108\" title=\"\">108</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib109\" title=\"\">109</a>]</cite>. Graph-Toolformer, for instance, divides its retrieval process into distinct steps where LLMs proactively use retrievers, apply Self-Ask techniques, and employ few-shot prompts to initiate search queries. This proactive stance allows LLMs to decide when to search for necessary information, akin to how an agent utilizes tools.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S5.SS3.p3\">\\n<p class=\"ltx_p\" id=\"S5.SS3.p3.1\">WebGPT\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib110\" title=\"\">110</a>]</cite> integrates a reinforcement learning framework to train the GPT-3 model in autonomously using a search engine during text generation. It navigates this process using special tokens that facilitate actions such as search engine queries, browsing results, and citing references, thereby expanding GPT-3’s capabilities through the use of external search engines. Flare automates timing retrieval by monitoring the confidence of the generation process, as indicated by the probability of generated terms\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib24\" title=\"\">24</a>]</cite>. When the probability falls below a certain threshold would activates the retrieval system to collect relevant information, thus optimizing the retrieval cycle. Self-RAG\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib25\" title=\"\">25</a>]</cite> introduces “reflection tokens” that allow the model to introspect its outputs. These tokens come in two varieties: “retrieve” and “critic”. The model autonomously decides when to activate retrieval, or alternatively, a predefined threshold may trigger the process. During retrieval, the generator conducts a fragment-level beam search across multiple paragraphs to derive the most coherent sequence. Critic scores are used to update the subdivision scores, with the flexibility to adjust these weights during inference, tailoring the model’s behavior. Self-RAG’s design obviates the need for additional classifiers or reliance on Natural Language Inference (NLI) models, thus streamlining the decision-making process for when to engage retrieval mechanisms and improving the model’s autonomous judgment capabilities in generating accurate responses.</p>\\n</div>\\n</section>\\n</section>\\n<section class=\"ltx_section\" id=\"S6\">\\n<h2 class=\"ltx_title ltx_title_section\">\\n<span class=\"ltx_tag ltx_tag_section\">VI </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S6.1.1\">Task and Evaluation</span>\\n</h2>\\n<div class=\"ltx_para\" id=\"S6.p1\">\\n<p class=\"ltx_p\" id=\"S6.p1.1\">The rapid advancement and growing adoption of RAG in the field of NLP have propelled the evaluation of RAG models to the forefront of research in the LLMs community. The primary objective of this evaluation is to comprehend and optimize the performance of RAG models across diverse application scenarios.This chapter will mainly introduce the main downstream tasks of RAG, datasets, and how to evaluate RAG systems.</p>\\n</div>\\n<section class=\"ltx_subsection\" id=\"S6.SS1\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S6.SS1.5.1.1\">VI-A</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S6.SS1.6.2\">Downstream Task</span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S6.SS1.p1\">\\n<p class=\"ltx_p\" id=\"S6.SS1.p1.1\">The core task of RAG remains Question Answering (QA), including traditional single-hop/multi-hop QA, multiple-choice, domain-specific QA as well as long-form scenarios suitable for RAG. In addition to QA, RAG is continuously being expanded into multiple downstream tasks, such as Information Extraction (IE), dialogue generation, code search, etc. The main downstream tasks of RAG and their corresponding datasets are summarized in Table \\xa0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S6.T2\" title=\"TABLE II ‣ VI-A Downstream Task ‣ VI Task and Evaluation ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_tag\">II</span></a>.</p>\\n</div>\\n<figure class=\"ltx_table\" id=\"S6.T2\">\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">TABLE II: </span>Downstream tasks and datasets of RAG</figcaption>\\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S6.T2.1\">\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.1\">\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S6.T2.1.1.1\">Task</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S6.T2.1.1.2\">Sub Task</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S6.T2.1.1.3\">Dataset</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S6.T2.1.1.4\">Method</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.2\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.2.1\">QA</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.2.2\">Single-hop</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.2.3\">Natural Qustion(NQ)\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib111\" title=\"\">111</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.2.4\">\\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T2.1.2.4.1\">\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.2.4.1.1\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.2.4.1.1.1\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib82\" title=\"\">82</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib52\" title=\"\">52</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib50\" title=\"\">50</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib34\" title=\"\">34</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib42\" title=\"\">42</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib64\" title=\"\">64</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib59\" title=\"\">59</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.2.4.1.2\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.2.4.1.2.1\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib62\" title=\"\">62</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib54\" title=\"\">54</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib112\" title=\"\">112</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib43\" title=\"\">43</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib71\" title=\"\">71</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.2.4.1.3\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.2.4.1.3.1\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib72\" title=\"\">72</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib44\" title=\"\">44</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib20\" title=\"\">20</a>]</cite></td>\\n</tr>\\n</table>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.3\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.3.1\"></td>\\n<td class=\"ltx_td\" id=\"S6.T2.1.3.2\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.3.3\">TriviaQA(TQA)\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib113\" title=\"\">113</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.3.4\">\\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T2.1.3.4.1\">\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.3.4.1.1\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.3.4.1.1.1\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib50\" title=\"\">50</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib34\" title=\"\">34</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib64\" title=\"\">64</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib45\" title=\"\">45</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.3.4.1.2\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.3.4.1.2.1\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib59\" title=\"\">59</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib62\" title=\"\">62</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib112\" title=\"\">112</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.3.4.1.3\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.3.4.1.3.1\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib43\" title=\"\">43</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib71\" title=\"\">71</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib72\" title=\"\">72</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib44\" title=\"\">44</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib25\" title=\"\">25</a>]</cite></td>\\n</tr>\\n</table>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.4\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.4.1\"></td>\\n<td class=\"ltx_td\" id=\"S6.T2.1.4.2\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.4.3\">SQuAD\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib114\" title=\"\">114</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.4.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib69\" title=\"\">69</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib112\" title=\"\">112</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib32\" title=\"\">32</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib20\" title=\"\">20</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.5\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.5.1\"></td>\\n<td class=\"ltx_td\" id=\"S6.T2.1.5.2\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.5.3\">Web Questions(WebQ)\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib115\" title=\"\">115</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.5.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib68\" title=\"\">68</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib50\" title=\"\">50</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib3\" title=\"\">3</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.6\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.6.1\"></td>\\n<td class=\"ltx_td\" id=\"S6.T2.1.6.2\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.6.3\">PopQA\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib116\" title=\"\">116</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.6.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib67\" title=\"\">67</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib25\" title=\"\">25</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.7\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.7.1\"></td>\\n<td class=\"ltx_td\" id=\"S6.T2.1.7.2\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.7.3\">MS MARCO\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib117\" title=\"\">117</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.7.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib52\" title=\"\">52</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib4\" title=\"\">4</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.8\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.8.1\"></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.8.2\">Multi-hop</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.8.3\">HotpotQA\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib118\" title=\"\">118</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.8.4\">\\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T2.1.8.4.1\">\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.8.4.1.1\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.8.4.1.1.1\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib82\" title=\"\">82</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib51\" title=\"\">51</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib34\" title=\"\">34</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib47\" title=\"\">47</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib61\" title=\"\">61</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.8.4.1.2\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.8.4.1.2.1\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib59\" title=\"\">59</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib91\" title=\"\">91</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib62\" title=\"\">62</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib69\" title=\"\">69</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib71\" title=\"\">71</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib7\" title=\"\">7</a>]</cite></td>\\n</tr>\\n</table>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.9\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.9.1\"></td>\\n<td class=\"ltx_td\" id=\"S6.T2.1.9.2\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.9.3\">2WikiMultiHopQA\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib119\" title=\"\">119</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.9.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib61\" title=\"\">61</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib59\" title=\"\">59</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib91\" title=\"\">91</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib48\" title=\"\">48</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.10\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.10.1\"></td>\\n<td class=\"ltx_td\" id=\"S6.T2.1.10.2\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.10.3\">MuSiQue\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib120\" title=\"\">120</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.10.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib51\" title=\"\">51</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib61\" title=\"\">61</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib91\" title=\"\">91</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.11\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.11.1\"></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.11.2\">Long-form QA</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.11.3\">ELI5\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib121\" title=\"\">121</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.11.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib51\" title=\"\">51</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib34\" title=\"\">34</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib49\" title=\"\">49</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib43\" title=\"\">43</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.12\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.12.1\"></td>\\n<td class=\"ltx_td\" id=\"S6.T2.1.12.2\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.12.3\">NarrativeQA(NQA)\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib122\" title=\"\">122</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.12.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib63\" title=\"\">63</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib123\" title=\"\">123</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib60\" title=\"\">60</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.13\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.13.1\"></td>\\n<td class=\"ltx_td\" id=\"S6.T2.1.13.2\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.13.3\">ASQA\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib124\" title=\"\">124</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.13.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib57\" title=\"\">57</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.14\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.14.1\"></td>\\n<td class=\"ltx_td\" id=\"S6.T2.1.14.2\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.14.3\">QMSum(QM)\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib125\" title=\"\">125</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.14.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib123\" title=\"\">123</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib60\" title=\"\">60</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.15\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.15.1\"></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.15.2\">Domain QA</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.15.3\">Qasper\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib126\" title=\"\">126</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.15.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib63\" title=\"\">63</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib60\" title=\"\">60</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.16\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.16.1\"></td>\\n<td class=\"ltx_td\" id=\"S6.T2.1.16.2\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.16.3\">COVID-QA\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib127\" title=\"\">127</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.16.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib35\" title=\"\">35</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib46\" title=\"\">46</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.17\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.17.1\"></td>\\n<td class=\"ltx_td\" id=\"S6.T2.1.17.2\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.17.3\">CMB\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib128\" title=\"\">128</a>]</cite>,MMCU_Medical\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib129\" title=\"\">129</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.17.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib81\" title=\"\">81</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.18\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.18.1\"></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.18.2\">Multi-Choice QA</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.18.3\">QuALITY\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib130\" title=\"\">130</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.18.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib63\" title=\"\">63</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib60\" title=\"\">60</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.19\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.19.1\"></td>\\n<td class=\"ltx_td\" id=\"S6.T2.1.19.2\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.19.3\">ARC\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib131\" title=\"\">131</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.19.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib67\" title=\"\">67</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib25\" title=\"\">25</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.20\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.20.1\"></td>\\n<td class=\"ltx_td\" id=\"S6.T2.1.20.2\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.20.3\">CommonsenseQA\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib132\" title=\"\">132</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.20.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib58\" title=\"\">58</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib66\" title=\"\">66</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.21\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.21.1\"></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.21.2\">Graph QA</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.21.3\">GraphQA\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib84\" title=\"\">84</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.21.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib84\" title=\"\">84</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.22\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.22.1\">Dialog</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.22.2\">Dialog Generation</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.22.3\">Wizard of Wikipedia (WoW)\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib133\" title=\"\">133</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.22.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib34\" title=\"\">34</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib42\" title=\"\">42</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib27\" title=\"\">27</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.23\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.23.1\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.23.2\">Personal Dialog</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.23.3\">KBP\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib134\" title=\"\">134</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.23.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib135\" title=\"\">135</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib74\" title=\"\">74</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.24\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.24.1\"></td>\\n<td class=\"ltx_td\" id=\"S6.T2.1.24.2\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.24.3\">DuleMon\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib136\" title=\"\">136</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.24.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib74\" title=\"\">74</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.25\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.25.1\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.25.2\">Task-oriented Dialog</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.25.3\">CamRest\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib137\" title=\"\">137</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.25.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib79\" title=\"\">79</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib78\" title=\"\">78</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.26\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.26.1\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.26.2\">Recommendation</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.26.3\">Amazon(Toys,Sport,Beauty)\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib138\" title=\"\">138</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.26.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib39\" title=\"\">39</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.27\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.27.1\">IE</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.27.2\">Event Argument Extraction</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.27.3\">WikiEvent<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib139\" title=\"\">139</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.27.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib42\" title=\"\">42</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib27\" title=\"\">27</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.28\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.28.1\"></td>\\n<td class=\"ltx_td\" id=\"S6.T2.1.28.2\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.28.3\">RAMS\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib140\" title=\"\">140</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.28.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib36\" title=\"\">36</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib37\" title=\"\">37</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.29\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.29.1\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.29.2\">Relation Extraction</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.29.3\">T-REx\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib141\" title=\"\">141</a>]</cite>,ZsRE\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib142\" title=\"\">142</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.29.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib51\" title=\"\">51</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib27\" title=\"\">27</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.30\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.30.1\">Reasoning</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.30.2\">Commonsense Reasoning</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.30.3\">HellaSwag\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib143\" title=\"\">143</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.30.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib66\" title=\"\">66</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.31\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.31.1\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.31.2\">CoT Reasoning</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.31.3\">CoT Reasoning\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib144\" title=\"\">144</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.31.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib27\" title=\"\">27</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.32\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.32.1\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.32.2\">Complex Reasoning</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.32.3\">CSQA\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib145\" title=\"\">145</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.32.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib55\" title=\"\">55</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.33\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.33.1\">Others</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.33.2\">Language Understanding</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.33.3\">MMLU\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib146\" title=\"\">146</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.33.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib47\" title=\"\">47</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib42\" title=\"\">42</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib43\" title=\"\">43</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib72\" title=\"\">72</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib7\" title=\"\">7</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.34\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.34.1\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.34.2\">Language Modeling</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.34.3\">WikiText-103\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib147\" title=\"\">147</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.34.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib29\" title=\"\">29</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib64\" title=\"\">64</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib71\" title=\"\">71</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib5\" title=\"\">5</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.35\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.35.1\"></td>\\n<td class=\"ltx_td\" id=\"S6.T2.1.35.2\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.35.3\">StrategyQA\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib148\" title=\"\">148</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.35.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib51\" title=\"\">51</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib55\" title=\"\">55</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib48\" title=\"\">48</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib58\" title=\"\">58</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.36\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.36.1\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.36.2\">Fact Checking/Verification</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.36.3\">FEVER\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib149\" title=\"\">149</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.36.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib50\" title=\"\">50</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib34\" title=\"\">34</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib42\" title=\"\">42</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib4\" title=\"\">4</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.37\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.37.1\"></td>\\n<td class=\"ltx_td\" id=\"S6.T2.1.37.2\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.37.3\">PubHealth\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib150\" title=\"\">150</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.37.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib67\" title=\"\">67</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.38\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.38.1\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.38.2\">Text Generation</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.38.3\">Biography\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib151\" title=\"\">151</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.38.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib67\" title=\"\">67</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.39\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.39.1\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.39.2\">Text Summarization</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.39.3\">WikiASP\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib152\" title=\"\">152</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.39.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib24\" title=\"\">24</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.40\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.40.1\"></td>\\n<td class=\"ltx_td\" id=\"S6.T2.1.40.2\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.40.3\">XSum\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib153\" title=\"\">153</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.40.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib17\" title=\"\">17</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.41\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.41.1\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.41.2\">Text Classification</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.41.3\">VioLens\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib154\" title=\"\">154</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.41.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib19\" title=\"\">19</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.42\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.42.1\"></td>\\n<td class=\"ltx_td\" id=\"S6.T2.1.42.2\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.42.3\">TREC\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib155\" title=\"\">155</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.42.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib33\" title=\"\">33</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.43\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.43.1\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.43.2\">Sentiment</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.43.3\">SST-2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib156\" title=\"\">156</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.43.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib38\" title=\"\">38</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.44\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.44.1\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.44.2\">Code Search</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.44.3\">CodeSearchNet\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib157\" title=\"\">157</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.44.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib76\" title=\"\">76</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.45\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.45.1\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.45.2\">Robustness Evaluation</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.45.3\">NoMIRACL\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib56\" title=\"\">56</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.45.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib56\" title=\"\">56</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.46\">\\n<td class=\"ltx_td\" id=\"S6.T2.1.46.1\"></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.46.2\">Math</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.46.3\">GSM8K\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib158\" title=\"\">158</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.46.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib73\" title=\"\">73</a>]</cite></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T2.1.47\">\\n<td class=\"ltx_td ltx_border_b\" id=\"S6.T2.1.47.1\"></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S6.T2.1.47.2\">Machine Translation</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S6.T2.1.47.3\">JRC-Acquis\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib159\" title=\"\">159</a>]</cite>\\n</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S6.T2.1.47.4\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib17\" title=\"\">17</a>]</cite></td>\\n</tr>\\n</table>\\n</figure>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S6.SS2\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S6.SS2.5.1.1\">VI-B</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S6.SS2.6.2\">Evaluation Target</span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S6.SS2.p1\">\\n<p class=\"ltx_p\" id=\"S6.SS2.p1.1\">Historically, RAG models assessments have centered on their execution in specific downstream tasks. These evaluations employ established metrics suitable to the tasks at hand. For instance, question answering evaluations might rely on EM and F1 scores\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib72\" title=\"\">72</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib59\" title=\"\">59</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib7\" title=\"\">7</a>]</cite>, whereas fact-checking tasks often hinge on Accuracy as the primary metric\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib42\" title=\"\">42</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib14\" title=\"\">14</a>]</cite>. BLEU and ROUGE metrics are also commonly used to evaluate answer quality\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib78\" title=\"\">78</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib52\" title=\"\">52</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib32\" title=\"\">32</a>]</cite>. Tools like RALLE, designed for the automatic evaluation of RAG applications, similarly base their assessments on these task-specific metrics\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib160\" title=\"\">160</a>]</cite>. Despite this, there is a notable paucity of research dedicated to evaluating the distinct characteristics of RAG models.The main evaluation objectives include:</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S6.SS2.p2\">\\n<p class=\"ltx_p\" id=\"S6.SS2.p2.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S6.SS2.p2.1.1\">Retrieval Quality</em>. Evaluating the retrieval quality is crucial for determining the effectiveness of the context sourced by the retriever component. Standard metrics from the domains of search engines, recommendation systems, and information retrieval systems are employed to measure the performance of the RAG retrieval module. Metrics such as Hit Rate, MRR, and NDCG are commonly utilized for this purpose\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib161\" title=\"\">161</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib162\" title=\"\">162</a>]</cite>.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S6.SS2.p3\">\\n<p class=\"ltx_p\" id=\"S6.SS2.p3.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S6.SS2.p3.1.1\">Generation Quality</em>. The assessment of generation quality centers on the generator’s capacity to synthesize coherent and relevant answers from the retrieved context. This evaluation can be categorized based on the content’s objectives: unlabeled and labeled content. For unlabeled content, the evaluation encompasses the faithfulness, relevance, and non-harmfulness of the generated answers. In contrast, for labeled content, the focus is on the accuracy of the information produced by the model\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib161\" title=\"\">161</a>]</cite>. Additionally, both retrieval and generation quality assessments can be conducted through manual or automatic evaluation methods\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib161\" title=\"\">161</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib29\" title=\"\">29</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib163\" title=\"\">163</a>]</cite>.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S6.SS3\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S6.SS3.5.1.1\">VI-C</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S6.SS3.6.2\">Evaluation Aspects</span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S6.SS3.p1\">\\n<p class=\"ltx_p\" id=\"S6.SS3.p1.1\">Contemporary evaluation practices of RAG models emphasize three primary quality scores and four essential abilities, which collectively inform the evaluation of the two principal targets of the RAG model: retrieval and generation.</p>\\n</div>\\n<section class=\"ltx_subsubsection\" id=\"S6.SS3.SSS1\">\\n<h4 class=\"ltx_title ltx_title_subsubsection\">\\n<span class=\"ltx_tag ltx_tag_subsubsection\"><span class=\"ltx_text\" id=\"S6.SS3.SSS1.5.1.1\">VI-C</span>1 </span>Quality Scores</h4>\\n<div class=\"ltx_para\" id=\"S6.SS3.SSS1.p1\">\\n<p class=\"ltx_p\" id=\"S6.SS3.SSS1.p1.1\">Quality scores include context relevance, answer faithfulness, and answer relevance. These quality scores evaluate the efficiency of the RAG model from different perspectives in the process of information retrieval and generation\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib164\" title=\"\">164</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib165\" title=\"\">165</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib166\" title=\"\">166</a>]</cite>.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S6.SS3.SSS1.p2\">\\n<p class=\"ltx_p\" id=\"S6.SS3.SSS1.p2.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S6.SS3.SSS1.p2.1.1\">Context Relevance</em> evaluates the precision and specificity of the retrieved context, ensuring relevance and minimizing processing costs associated with extraneous content.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S6.SS3.SSS1.p3\">\\n<p class=\"ltx_p\" id=\"S6.SS3.SSS1.p3.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S6.SS3.SSS1.p3.1.1\">Answer Faithfulness</em> ensures that the generated answers remain true to the retrieved context, maintaining consistency and avoiding contradictions.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S6.SS3.SSS1.p4\">\\n<p class=\"ltx_p\" id=\"S6.SS3.SSS1.p4.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S6.SS3.SSS1.p4.1.1\">Answer Relevance</em> requires that the generated answers are directly pertinent to the posed questions, effectively addressing the core inquiry.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsubsection\" id=\"S6.SS3.SSS2\">\\n<h4 class=\"ltx_title ltx_title_subsubsection\">\\n<span class=\"ltx_tag ltx_tag_subsubsection\"><span class=\"ltx_text\" id=\"S6.SS3.SSS2.5.1.1\">VI-C</span>2 </span>Required Abilities</h4>\\n<div class=\"ltx_para\" id=\"S6.SS3.SSS2.p1\">\\n<p class=\"ltx_p\" id=\"S6.SS3.SSS2.p1.1\">RAG evaluation also encompasses four abilities indicative of its adaptability and efficiency: noise robustness, negative rejection, information integration, and counterfactual robustness\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib167\" title=\"\">167</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib168\" title=\"\">168</a>]</cite>. These abilities are critical for the model’s performance under various challenges and complex scenarios, impacting the quality scores.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S6.SS3.SSS2.p2\">\\n<p class=\"ltx_p\" id=\"S6.SS3.SSS2.p2.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S6.SS3.SSS2.p2.1.1\">Noise Robustness</em> appraises the model’s capability to manage noise documents that are question-related but lack substantive information.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S6.SS3.SSS2.p3\">\\n<p class=\"ltx_p\" id=\"S6.SS3.SSS2.p3.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S6.SS3.SSS2.p3.1.1\">Negative Rejection</em> assesses the model’s discernment in refraining from responding when the retrieved documents do not contain the necessary knowledge to answer a question.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S6.SS3.SSS2.p4\">\\n<p class=\"ltx_p\" id=\"S6.SS3.SSS2.p4.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S6.SS3.SSS2.p4.1.1\">Information Integration</em> evaluates the model’s proficiency in synthesizing information from multiple documents to address complex questions.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S6.SS3.SSS2.p5\">\\n<p class=\"ltx_p\" id=\"S6.SS3.SSS2.p5.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S6.SS3.SSS2.p5.1.1\">Counterfactual Robustness</em> tests the model’s ability to recognize and disregard known inaccuracies within documents, even when instructed about potential misinformation.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S6.SS3.SSS2.p6\">\\n<p class=\"ltx_p\" id=\"S6.SS3.SSS2.p6.1\">Context relevance and noise robustness are important for evaluating the quality of retrieval, while answer faithfulness, answer relevance, negative rejection, information integration, and counterfactual robustness are important for evaluating the quality of generation.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S6.SS3.SSS2.p7\">\\n<p class=\"ltx_p\" id=\"S6.SS3.SSS2.p7.1\">The specific metrics for each evaluation aspect are summarized in Table\\xa0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S6.T3\" title=\"TABLE III ‣ VI-C2 Required Abilities ‣ VI-C Evaluation Aspects ‣ VI Task and Evaluation ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>. It is essential to recognize that these metrics, derived from related work, are traditional measures and do not yet represent a mature or standardized approach for quantifying RAG evaluation aspects. Custom metrics tailored to the nuances of RAG models, though not included here, have also been developed in some evaluation studies.</p>\\n</div>\\n<figure class=\"ltx_table\" id=\"S6.T3\">\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">TABLE III: </span>Summary of metrics applicable for evaluation aspects of RAG</figcaption>\\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S6.T3.1\">\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"S6.T3.1.1.1\">\\n<td class=\"ltx_td ltx_border_tt\" id=\"S6.T3.1.1.1.1\"></td>\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S6.T3.1.1.1.2\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T3.1.1.1.2.1\">\\n<p class=\"ltx_p\" id=\"S6.T3.1.1.1.2.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T3.1.1.1.2.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.2.1.2.1\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T3.1.1.1.2.1.2.1.1\">\\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.2.1.2.1.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.2.1.2.1.1.1.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.2.1.2.1.1.1.1.1\" style=\"font-size:80%;\">Context</span></span></span>\\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.2.1.2.1.1.2\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.2.1.2.1.1.2.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.2.1.2.1.1.2.1.1\" style=\"font-size:80%;\">Relevance</span></span></span>\\n</span></span></p>\\n<p class=\"ltx_p\" id=\"S6.T3.1.1.1.2.1.3\"></p>\\n</div>\\n</th>\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S6.T3.1.1.1.3\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T3.1.1.1.3.1\">\\n<p class=\"ltx_p\" id=\"S6.T3.1.1.1.3.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T3.1.1.1.3.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.3.1.2.1\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T3.1.1.1.3.1.2.1.1\">\\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.3.1.2.1.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.3.1.2.1.1.1.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.3.1.2.1.1.1.1.1\" style=\"font-size:80%;\">Faithfulness</span></span></span>\\n</span></span></p>\\n<p class=\"ltx_p\" id=\"S6.T3.1.1.1.3.1.3\"></p>\\n</div>\\n</th>\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S6.T3.1.1.1.4\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T3.1.1.1.4.1\">\\n<p class=\"ltx_p\" id=\"S6.T3.1.1.1.4.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T3.1.1.1.4.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.4.1.2.1\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T3.1.1.1.4.1.2.1.1\">\\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.4.1.2.1.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.4.1.2.1.1.1.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.4.1.2.1.1.1.1.1\" style=\"font-size:80%;\">Answer</span></span></span>\\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.4.1.2.1.1.2\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.4.1.2.1.1.2.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.4.1.2.1.1.2.1.1\" style=\"font-size:80%;\">Relevance</span></span></span>\\n</span></span></p>\\n<p class=\"ltx_p\" id=\"S6.T3.1.1.1.4.1.3\"></p>\\n</div>\\n</th>\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S6.T3.1.1.1.5\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T3.1.1.1.5.1\">\\n<p class=\"ltx_p\" id=\"S6.T3.1.1.1.5.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T3.1.1.1.5.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.5.1.2.1\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T3.1.1.1.5.1.2.1.1\">\\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.5.1.2.1.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.5.1.2.1.1.1.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.5.1.2.1.1.1.1.1\" style=\"font-size:80%;\">Noise</span></span></span>\\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.5.1.2.1.1.2\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.5.1.2.1.1.2.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.5.1.2.1.1.2.1.1\" style=\"font-size:80%;\">Robustness</span></span></span>\\n</span></span></p>\\n<p class=\"ltx_p\" id=\"S6.T3.1.1.1.5.1.3\"></p>\\n</div>\\n</th>\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S6.T3.1.1.1.6\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T3.1.1.1.6.1\">\\n<p class=\"ltx_p\" id=\"S6.T3.1.1.1.6.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T3.1.1.1.6.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.6.1.2.1\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T3.1.1.1.6.1.2.1.1\">\\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.6.1.2.1.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.6.1.2.1.1.1.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.6.1.2.1.1.1.1.1\" style=\"font-size:80%;\">Negative</span></span></span>\\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.6.1.2.1.1.2\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.6.1.2.1.1.2.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.6.1.2.1.1.2.1.1\" style=\"font-size:80%;\">Rejection</span></span></span>\\n</span></span></p>\\n<p class=\"ltx_p\" id=\"S6.T3.1.1.1.6.1.3\"></p>\\n</div>\\n</th>\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S6.T3.1.1.1.7\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T3.1.1.1.7.1\">\\n<p class=\"ltx_p\" id=\"S6.T3.1.1.1.7.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T3.1.1.1.7.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.7.1.2.1\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T3.1.1.1.7.1.2.1.1\">\\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.7.1.2.1.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.7.1.2.1.1.1.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.7.1.2.1.1.1.1.1\" style=\"font-size:80%;\">Information</span></span></span>\\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.7.1.2.1.1.2\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.7.1.2.1.1.2.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.7.1.2.1.1.2.1.1\" style=\"font-size:80%;\">Integration</span></span></span>\\n</span></span></p>\\n<p class=\"ltx_p\" id=\"S6.T3.1.1.1.7.1.3\"></p>\\n</div>\\n</th>\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S6.T3.1.1.1.8\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T3.1.1.1.8.1\">\\n<p class=\"ltx_p\" id=\"S6.T3.1.1.1.8.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T3.1.1.1.8.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.8.1.2.1\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T3.1.1.1.8.1.2.1.1\">\\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.8.1.2.1.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.8.1.2.1.1.1.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.8.1.2.1.1.1.1.1\" style=\"font-size:80%;\">Counterfactual</span></span></span>\\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.8.1.2.1.1.2\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.8.1.2.1.1.2.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.8.1.2.1.1.2.1.1\" style=\"font-size:80%;\">Robustness</span></span></span>\\n</span></span></p>\\n<p class=\"ltx_p\" id=\"S6.T3.1.1.1.8.1.3\"></p>\\n</div>\\n</th>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T3.1.2.2\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T3.1.2.2.1\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.2.2.1.1\">Accuracy</p>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.2.2.2\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.2.2.2.1\">✓</p>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.2.2.3\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.2.2.3.1\">✓</p>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.2.2.4\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.2.2.4.1\">✓</p>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.2.2.5\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.2.2.5.1\">✓</p>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.2.2.6\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.2.2.6.1\">✓</p>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.2.2.7\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.2.2.7.1\">✓</p>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.2.2.8\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.2.2.8.1\">✓</p>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T3.1.3.3\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T3.1.3.3.1\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.3.3.1.1\">EM</p>\\n</td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.3.3.2\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.3.3.3\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.3.3.4\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.3.3.5\"></td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.3.3.6\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.3.3.6.1\">✓</p>\\n</td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.3.3.7\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.3.3.8\"></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T3.1.4.4\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T3.1.4.4.1\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.4.4.1.1\">Recall</p>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.4.4.2\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.4.4.2.1\">✓</p>\\n</td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.4.4.3\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.4.4.4\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.4.4.5\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.4.4.6\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.4.4.7\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.4.4.8\"></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T3.1.5.5\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T3.1.5.5.1\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.5.5.1.1\">Precision</p>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.5.5.2\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.5.5.2.1\">✓</p>\\n</td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.5.5.3\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.5.5.4\"></td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.5.5.5\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.5.5.5.1\">✓</p>\\n</td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.5.5.6\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.5.5.7\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.5.5.8\"></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T3.1.6.6\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T3.1.6.6.1\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.6.6.1.1\">R-Rate</p>\\n</td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.6.6.2\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.6.6.3\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.6.6.4\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.6.6.5\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.6.6.6\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.6.6.7\"></td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.6.6.8\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.6.6.8.1\">✓</p>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T3.1.7.7\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T3.1.7.7.1\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.7.7.1.1\">Cosine Similarity</p>\\n</td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.7.7.2\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.7.7.3\"></td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.7.7.4\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.7.7.4.1\">✓</p>\\n</td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.7.7.5\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.7.7.6\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.7.7.7\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.7.7.8\"></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T3.1.8.8\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T3.1.8.8.1\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.8.8.1.1\">Hit Rate</p>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.8.8.2\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.8.8.2.1\">✓</p>\\n</td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.8.8.3\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.8.8.4\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.8.8.5\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.8.8.6\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.8.8.7\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.8.8.8\"></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T3.1.9.9\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T3.1.9.9.1\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.9.9.1.1\">MRR</p>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.9.9.2\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.9.9.2.1\">✓</p>\\n</td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.9.9.3\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.9.9.4\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.9.9.5\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.9.9.6\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.9.9.7\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.9.9.8\"></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T3.1.10.10\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T3.1.10.10.1\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.10.10.1.1\">NDCG</p>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.10.10.2\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.10.10.2.1\">✓</p>\\n</td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.10.10.3\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.10.10.4\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.10.10.5\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.10.10.6\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.10.10.7\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.10.10.8\"></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T3.1.11.11\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T3.1.11.11.1\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.11.11.1.1\">BLEU</p>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.11.11.2\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.11.11.2.1\">✓</p>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.11.11.3\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.11.11.3.1\">✓</p>\\n</td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.11.11.4\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.11.11.4.1\">✓</p>\\n</td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.11.11.5\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.11.11.6\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.11.11.7\"></td>\\n<td class=\"ltx_td\" id=\"S6.T3.1.11.11.8\"></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T3.1.12.12\">\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S6.T3.1.12.12.1\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.12.12.1.1\">ROUGE/ROUGE-L</p>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T3.1.12.12.2\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.12.12.2.1\">✓</p>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T3.1.12.12.3\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.12.12.3.1\">✓</p>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T3.1.12.12.4\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T3.1.12.12.4.1\">✓</p>\\n</td>\\n<td class=\"ltx_td ltx_border_bb\" id=\"S6.T3.1.12.12.5\"></td>\\n<td class=\"ltx_td ltx_border_bb\" id=\"S6.T3.1.12.12.6\"></td>\\n<td class=\"ltx_td ltx_border_bb\" id=\"S6.T3.1.12.12.7\"></td>\\n<td class=\"ltx_td ltx_border_bb\" id=\"S6.T3.1.12.12.8\"></td>\\n</tr>\\n</tbody>\\n</table>\\n</figure>\\n</section>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S6.SS4\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S6.SS4.5.1.1\">VI-D</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S6.SS4.6.2\">Evaluation Benchmarks and Tools</span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S6.SS4.p1\">\\n<p class=\"ltx_p\" id=\"S6.SS4.p1.1\">A series of benchmark tests and tools have been proposed to facilitate the evaluation of RAG.These instruments furnish quantitative metrics that not only gauge RAG model performance but also enhance comprehension of the model’s capabilities across various evaluation aspects. Prominent benchmarks such as RGB, RECALL and CRUD \\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib167\" title=\"\">167</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib168\" title=\"\">168</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib169\" title=\"\">169</a>]</cite> focus on appraising the essential abilities of RAG models. Concurrently, state-of-the-art automated tools like RAGAS\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib164\" title=\"\">164</a>]</cite>, ARES\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib165\" title=\"\">165</a>]</cite>, and TruLens<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.trulens.org/trulens_eval/core_concepts_rag_triad/\" title=\"\">https://www.trulens.org/trulens_eval/core_concepts_rag_triad/</a></span></span></span> employ LLMs to adjudicate the quality scores. These tools and benchmarks collectively form a robust framework for the systematic evaluation of RAG models, as summarized in Table\\xa0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S6.T4\" title=\"TABLE IV ‣ VI-D Evaluation Benchmarks and Tools ‣ VI Task and Evaluation ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>.\\n</p>\\n</div>\\n<figure class=\"ltx_table\" id=\"S6.T4\">\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">TABLE IV: </span>Summary of evaluation frameworks</figcaption><div class=\"ltx_flex_figure ltx_flex_table\">\\n<div class=\"ltx_flex_cell\">\\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S6.T4.6\">\\n<thead class=\"ltx_thead\">\\n<tr class=\"ltx_tr\" id=\"S6.T4.6.7.1\">\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S6.T4.6.7.1.1\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.6.7.1.1.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.6.7.1.1.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.6.7.1.1.1.2\"><span class=\"ltx_text\" id=\"S6.T4.6.7.1.1.1.2.1\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.6.7.1.1.1.2.1.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.6.7.1.1.1.2.1.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.6.7.1.1.1.2.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T4.6.7.1.1.1.2.1.1.1.1.1\" style=\"font-size:80%;\">Evaluation Framework</span></span></span>\\n</span></span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.6.7.1.1.1.3\"></p>\\n</div>\\n</th>\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S6.T4.6.7.1.2\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.6.7.1.2.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.6.7.1.2.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.6.7.1.2.1.2\"><span class=\"ltx_text\" id=\"S6.T4.6.7.1.2.1.2.1\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.6.7.1.2.1.2.1.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.6.7.1.2.1.2.1.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.6.7.1.2.1.2.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T4.6.7.1.2.1.2.1.1.1.1.1\" style=\"font-size:80%;\">Evaluation Targets</span></span></span>\\n</span></span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.6.7.1.2.1.3\"></p>\\n</div>\\n</th>\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S6.T4.6.7.1.3\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.6.7.1.3.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.6.7.1.3.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.6.7.1.3.1.2\"><span class=\"ltx_text\" id=\"S6.T4.6.7.1.3.1.2.1\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.6.7.1.3.1.2.1.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.6.7.1.3.1.2.1.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.6.7.1.3.1.2.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T4.6.7.1.3.1.2.1.1.1.1.1\" style=\"font-size:80%;\">Evaluation Aspects</span></span></span>\\n</span></span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.6.7.1.3.1.3\"></p>\\n</div>\\n</th>\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S6.T4.6.7.1.4\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.6.7.1.4.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.6.7.1.4.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.6.7.1.4.1.2\"><span class=\"ltx_text\" id=\"S6.T4.6.7.1.4.1.2.1\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.6.7.1.4.1.2.1.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.6.7.1.4.1.2.1.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.6.7.1.4.1.2.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T4.6.7.1.4.1.2.1.1.1.1.1\" style=\"font-size:80%;\">Quantitative Metrics</span></span></span>\\n</span></span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.6.7.1.4.1.3\"></p>\\n</div>\\n</th>\\n</tr>\\n</thead>\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"S6.T4.1.1\">\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.1\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.1.1.1.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.1.1.1.1.2\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.1.1.1.1.1\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.1.1.1.1.1.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.1.1.1.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.1.1.1.1\">RGB<math alttext=\"{}^{\\\\dagger}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T4.1.1.1.1.1.1.1.1.m1.1\"><semantics id=\"S6.T4.1.1.1.1.1.1.1.1.m1.1a\"><msup id=\"S6.T4.1.1.1.1.1.1.1.1.m1.1.1\" xref=\"S6.T4.1.1.1.1.1.1.1.1.m1.1.1.cmml\"><mi id=\"S6.T4.1.1.1.1.1.1.1.1.m1.1.1a\" xref=\"S6.T4.1.1.1.1.1.1.1.1.m1.1.1.cmml\"></mi><mo id=\"S6.T4.1.1.1.1.1.1.1.1.m1.1.1.1\" xref=\"S6.T4.1.1.1.1.1.1.1.1.m1.1.1.1.cmml\">†</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.1.1.1.1.1.1.1.1.m1.1b\"><apply id=\"S6.T4.1.1.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S6.T4.1.1.1.1.1.1.1.1.m1.1.1\"><ci id=\"S6.T4.1.1.1.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S6.T4.1.1.1.1.1.1.1.1.m1.1.1.1\">†</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.1.1.1.1.1.1.1.1.m1.1c\">{}^{\\\\dagger}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S6.T4.1.1.1.1.1.1.1.1.m1.1d\">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span>\\n</span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.1.1.1.1.3\"></p>\\n</div>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.2\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.1.1.2.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.1.1.2.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.1.1.2.1.2\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.1.1.2.1.2.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.2.1.2.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.2.1.2.1.1.1\">Retrieval Quality</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.2.1.2.1.2\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.2.1.2.1.2.1\">Generation Quality</span></span>\\n</span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.1.1.2.1.3\"></p>\\n</div>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.3\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.1.1.3.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.1.1.3.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.1.1.3.1.2\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.1.1.3.1.2.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.3.1.2.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.3.1.2.1.1.1\">Noise Robustness</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.3.1.2.1.2\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.3.1.2.1.2.1\">Negative Rejection</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.3.1.2.1.3\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.3.1.2.1.3.1\">Information Integration</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.3.1.2.1.4\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.3.1.2.1.4.1\">Counterfactual Robustness</span></span>\\n</span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.1.1.3.1.3\"></p>\\n</div>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.4\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.1.1.4.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.1.1.4.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.1.1.4.1.2\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.1.1.4.1.2.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.4.1.2.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.4.1.2.1.1.1\">Accuracy</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.4.1.2.1.2\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.4.1.2.1.2.1\">EM</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.4.1.2.1.3\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.4.1.2.1.3.1\">Accuracy</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.4.1.2.1.4\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.4.1.2.1.4.1\">Accuracy</span></span>\\n</span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.1.1.4.1.3\"></p>\\n</div>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T4.2.2\">\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.2.2.1\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.2.2.1.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.2.2.1.1.2\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.2.2.1.1.1\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.2.2.1.1.1.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.2.2.1.1.1.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.2.2.1.1.1.1.1.1\">RECALL<math alttext=\"{}^{\\\\dagger}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T4.2.2.1.1.1.1.1.1.m1.1\"><semantics id=\"S6.T4.2.2.1.1.1.1.1.1.m1.1a\"><msup id=\"S6.T4.2.2.1.1.1.1.1.1.m1.1.1\" xref=\"S6.T4.2.2.1.1.1.1.1.1.m1.1.1.cmml\"><mi id=\"S6.T4.2.2.1.1.1.1.1.1.m1.1.1a\" xref=\"S6.T4.2.2.1.1.1.1.1.1.m1.1.1.cmml\"></mi><mo id=\"S6.T4.2.2.1.1.1.1.1.1.m1.1.1.1\" xref=\"S6.T4.2.2.1.1.1.1.1.1.m1.1.1.1.cmml\">†</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.2.2.1.1.1.1.1.1.m1.1b\"><apply id=\"S6.T4.2.2.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S6.T4.2.2.1.1.1.1.1.1.m1.1.1\"><ci id=\"S6.T4.2.2.1.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S6.T4.2.2.1.1.1.1.1.1.m1.1.1.1\">†</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.2.2.1.1.1.1.1.1.m1.1c\">{}^{\\\\dagger}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S6.T4.2.2.1.1.1.1.1.1.m1.1d\">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span>\\n</span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.2.2.1.1.3\"></p>\\n</div>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.2.2.2\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T4.2.2.2.1\">Generation Quality</p>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.2.2.3\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T4.2.2.3.1\">Counterfactual Robustness</p>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.2.2.4\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S6.T4.2.2.4.1\">R-Rate (Reappearance Rate)</p>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T4.3.3\">\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.3.3.1\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.3.3.1.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.3.3.1.1.2\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.3.3.1.1.1\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.3.3.1.1.1.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.3.3.1.1.1.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.3.3.1.1.1.1.1.1\">RAGAS<math alttext=\"{}^{\\\\ddagger}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T4.3.3.1.1.1.1.1.1.m1.1\"><semantics id=\"S6.T4.3.3.1.1.1.1.1.1.m1.1a\"><msup id=\"S6.T4.3.3.1.1.1.1.1.1.m1.1.1\" xref=\"S6.T4.3.3.1.1.1.1.1.1.m1.1.1.cmml\"><mi id=\"S6.T4.3.3.1.1.1.1.1.1.m1.1.1a\" xref=\"S6.T4.3.3.1.1.1.1.1.1.m1.1.1.cmml\"></mi><mo id=\"S6.T4.3.3.1.1.1.1.1.1.m1.1.1.1\" xref=\"S6.T4.3.3.1.1.1.1.1.1.m1.1.1.1.cmml\">‡</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.3.3.1.1.1.1.1.1.m1.1b\"><apply id=\"S6.T4.3.3.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S6.T4.3.3.1.1.1.1.1.1.m1.1.1\"><ci id=\"S6.T4.3.3.1.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S6.T4.3.3.1.1.1.1.1.1.m1.1.1.1\">‡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.3.3.1.1.1.1.1.1.m1.1c\">{}^{\\\\ddagger}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S6.T4.3.3.1.1.1.1.1.1.m1.1d\">start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span>\\n</span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.3.3.1.1.3\"></p>\\n</div>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.3.3.2\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.3.3.2.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.3.3.2.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.3.3.2.1.2\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.3.3.2.1.2.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.3.3.2.1.2.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.3.3.2.1.2.1.1.1\">Retrieval Quality</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.3.3.2.1.2.1.2\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.3.3.2.1.2.1.2.1\">Generation Quality</span></span>\\n</span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.3.3.2.1.3\"></p>\\n</div>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.3.3.3\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.3.3.3.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.3.3.3.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.3.3.3.1.2\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.3.3.3.1.2.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.3.3.3.1.2.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.3.3.3.1.2.1.1.1\">Context Relevance</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.3.3.3.1.2.1.2\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.3.3.3.1.2.1.2.1\">Faithfulness</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.3.3.3.1.2.1.3\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.3.3.3.1.2.1.3.1\">Answer Relevance</span></span>\\n</span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.3.3.3.1.3\"></p>\\n</div>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.3.3.4\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.3.3.4.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.3.3.4.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.3.3.4.1.2\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.3.3.4.1.2.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.3.3.4.1.2.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.3.3.4.1.2.1.1.1\">*</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.3.3.4.1.2.1.2\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.3.3.4.1.2.1.2.1\">Cosine Similarity</span></span>\\n</span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.3.3.4.1.3\"></p>\\n</div>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T4.4.4\">\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.4.4.1\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.4.4.1.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.4.4.1.1.2\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.4.4.1.1.1\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.4.4.1.1.1.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.4.4.1.1.1.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.4.4.1.1.1.1.1.1\">ARES<math alttext=\"{}^{\\\\ddagger}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T4.4.4.1.1.1.1.1.1.m1.1\"><semantics id=\"S6.T4.4.4.1.1.1.1.1.1.m1.1a\"><msup id=\"S6.T4.4.4.1.1.1.1.1.1.m1.1.1\" xref=\"S6.T4.4.4.1.1.1.1.1.1.m1.1.1.cmml\"><mi id=\"S6.T4.4.4.1.1.1.1.1.1.m1.1.1a\" xref=\"S6.T4.4.4.1.1.1.1.1.1.m1.1.1.cmml\"></mi><mo id=\"S6.T4.4.4.1.1.1.1.1.1.m1.1.1.1\" xref=\"S6.T4.4.4.1.1.1.1.1.1.m1.1.1.1.cmml\">‡</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.4.4.1.1.1.1.1.1.m1.1b\"><apply id=\"S6.T4.4.4.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S6.T4.4.4.1.1.1.1.1.1.m1.1.1\"><ci id=\"S6.T4.4.4.1.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S6.T4.4.4.1.1.1.1.1.1.m1.1.1.1\">‡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.4.4.1.1.1.1.1.1.m1.1c\">{}^{\\\\ddagger}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S6.T4.4.4.1.1.1.1.1.1.m1.1d\">start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span>\\n</span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.4.4.1.1.3\"></p>\\n</div>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.4.4.2\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.4.4.2.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.4.4.2.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.4.4.2.1.2\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.4.4.2.1.2.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.4.4.2.1.2.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.4.4.2.1.2.1.1.1\">Retrieval Quality</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.4.4.2.1.2.1.2\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.4.4.2.1.2.1.2.1\">Generation Quality</span></span>\\n</span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.4.4.2.1.3\"></p>\\n</div>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.4.4.3\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.4.4.3.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.4.4.3.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.4.4.3.1.2\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.4.4.3.1.2.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.4.4.3.1.2.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.4.4.3.1.2.1.1.1\">Context Relevance</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.4.4.3.1.2.1.2\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.4.4.3.1.2.1.2.1\">Faithfulness</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.4.4.3.1.2.1.3\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.4.4.3.1.2.1.3.1\">Answer Relevance</span></span>\\n</span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.4.4.3.1.3\"></p>\\n</div>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.4.4.4\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.4.4.4.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.4.4.4.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.4.4.4.1.2\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.4.4.4.1.2.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.4.4.4.1.2.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.4.4.4.1.2.1.1.1\">Accuracy</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.4.4.4.1.2.1.2\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.4.4.4.1.2.1.2.1\">Accuracy</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.4.4.4.1.2.1.3\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.4.4.4.1.2.1.3.1\">Accuracy</span></span>\\n</span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.4.4.4.1.3\"></p>\\n</div>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T4.5.5\">\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.5.5.1\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.5.5.1.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.5.5.1.1.2\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.5.5.1.1.1\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.5.5.1.1.1.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.5.5.1.1.1.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.5.5.1.1.1.1.1.1\">TruLens<math alttext=\"{}^{\\\\ddagger}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T4.5.5.1.1.1.1.1.1.m1.1\"><semantics id=\"S6.T4.5.5.1.1.1.1.1.1.m1.1a\"><msup id=\"S6.T4.5.5.1.1.1.1.1.1.m1.1.1\" xref=\"S6.T4.5.5.1.1.1.1.1.1.m1.1.1.cmml\"><mi id=\"S6.T4.5.5.1.1.1.1.1.1.m1.1.1a\" xref=\"S6.T4.5.5.1.1.1.1.1.1.m1.1.1.cmml\"></mi><mo id=\"S6.T4.5.5.1.1.1.1.1.1.m1.1.1.1\" xref=\"S6.T4.5.5.1.1.1.1.1.1.m1.1.1.1.cmml\">‡</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.5.5.1.1.1.1.1.1.m1.1b\"><apply id=\"S6.T4.5.5.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S6.T4.5.5.1.1.1.1.1.1.m1.1.1\"><ci id=\"S6.T4.5.5.1.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S6.T4.5.5.1.1.1.1.1.1.m1.1.1.1\">‡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.5.5.1.1.1.1.1.1.m1.1c\">{}^{\\\\ddagger}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S6.T4.5.5.1.1.1.1.1.1.m1.1d\">start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span>\\n</span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.5.5.1.1.3\"></p>\\n</div>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.5.5.2\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.5.5.2.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.5.5.2.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.5.5.2.1.2\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.5.5.2.1.2.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.5.5.2.1.2.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.5.5.2.1.2.1.1.1\">Retrieval Quality</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.5.5.2.1.2.1.2\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.5.5.2.1.2.1.2.1\">Generation Quality</span></span>\\n</span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.5.5.2.1.3\"></p>\\n</div>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.5.5.3\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.5.5.3.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.5.5.3.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.5.5.3.1.2\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.5.5.3.1.2.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.5.5.3.1.2.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.5.5.3.1.2.1.1.1\">Context Relevance</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.5.5.3.1.2.1.2\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.5.5.3.1.2.1.2.1\">Faithfulness</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.5.5.3.1.2.1.3\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.5.5.3.1.2.1.3.1\">Answer Relevance</span></span>\\n</span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.5.5.3.1.3\"></p>\\n</div>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.5.5.4\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.5.5.4.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.5.5.4.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.5.5.4.1.2\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.5.5.4.1.2.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.5.5.4.1.2.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.5.5.4.1.2.1.1.1\">*</span></span>\\n</span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.5.5.4.1.3\"></p>\\n</div>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S6.T4.6.6\">\\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T4.6.6.1\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.6.6.1.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.6.6.1.1.2\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.6.6.1.1.1\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.6.6.1.1.1.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.6.6.1.1.1.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.6.6.1.1.1.1.1.1\">CRUD<math alttext=\"{}^{\\\\dagger}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T4.6.6.1.1.1.1.1.1.m1.1\"><semantics id=\"S6.T4.6.6.1.1.1.1.1.1.m1.1a\"><msup id=\"S6.T4.6.6.1.1.1.1.1.1.m1.1.1\" xref=\"S6.T4.6.6.1.1.1.1.1.1.m1.1.1.cmml\"><mi id=\"S6.T4.6.6.1.1.1.1.1.1.m1.1.1a\" xref=\"S6.T4.6.6.1.1.1.1.1.1.m1.1.1.cmml\"></mi><mo id=\"S6.T4.6.6.1.1.1.1.1.1.m1.1.1.1\" xref=\"S6.T4.6.6.1.1.1.1.1.1.m1.1.1.1.cmml\">†</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.6.6.1.1.1.1.1.1.m1.1b\"><apply id=\"S6.T4.6.6.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S6.T4.6.6.1.1.1.1.1.1.m1.1.1\"><ci id=\"S6.T4.6.6.1.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S6.T4.6.6.1.1.1.1.1.1.m1.1.1.1\">†</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.6.6.1.1.1.1.1.1.m1.1c\">{}^{\\\\dagger}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S6.T4.6.6.1.1.1.1.1.1.m1.1d\">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span>\\n</span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.6.6.1.1.3\"></p>\\n</div>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T4.6.6.2\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.6.6.2.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.6.6.2.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.6.6.2.1.2\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.6.6.2.1.2.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.6.6.2.1.2.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.6.6.2.1.2.1.1.1\">Retrieval Quality</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.6.6.2.1.2.1.2\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.6.6.2.1.2.1.2.1\">Generation Quality</span></span>\\n</span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.6.6.2.1.3\"></p>\\n</div>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T4.6.6.3\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.6.6.3.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.6.6.3.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.6.6.3.1.2\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.6.6.3.1.2.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.6.6.3.1.2.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.6.6.3.1.2.1.1.1\">Creative Generation</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.6.6.3.1.2.1.2\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.6.6.3.1.2.1.2.1\">Knowledge-intensive QA</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.6.6.3.1.2.1.3\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.6.6.3.1.2.1.3.1\">Error Correction</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.6.6.3.1.2.1.4\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.6.6.3.1.2.1.4.1\">Summarization</span></span>\\n</span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.6.6.3.1.3\"></p>\\n</div>\\n</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T4.6.6.4\">\\n<div class=\"ltx_block ltx_align_top\" id=\"S6.T4.6.6.4.1\">\\n<p class=\"ltx_p\" id=\"S6.T4.6.6.4.1.1\"></p>\\n<p class=\"ltx_p\" id=\"S6.T4.6.6.4.1.2\">\\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.6.6.4.1.2.1\">\\n<span class=\"ltx_tr\" id=\"S6.T4.6.6.4.1.2.1.1\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.6.6.4.1.2.1.1.1\">BLEU</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.6.6.4.1.2.1.2\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.6.6.4.1.2.1.2.1\">ROUGE-L</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.6.6.4.1.2.1.3\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.6.6.4.1.2.1.3.1\">BertScore</span></span>\\n<span class=\"ltx_tr\" id=\"S6.T4.6.6.4.1.2.1.4\">\\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.6.6.4.1.2.1.4.1\">RAGQuestEval</span></span>\\n</span></p>\\n<p class=\"ltx_p\" id=\"S6.T4.6.6.4.1.3\"></p>\\n</div>\\n</td>\\n</tr>\\n</tbody>\\n</table>\\n</div>\\n<div class=\"ltx_flex_break\"></div>\\n<div class=\"ltx_flex_cell\">\\n<p class=\"ltx_p ltx_align_center\" id=\"S6.T4.7\"><span class=\"ltx_text ltx_font_italic\" id=\"S6.T4.7.1\">† represents a benchmark, and ‡ represents a tool. * denotes customized quantitative metrics, which deviate from traditional metrics. Readers are encouraged to consult pertinent literature for the specific quantification formulas associated with these metrics, as required.</span></p>\\n</div>\\n</div>\\n</figure>\\n</section>\\n</section>\\n<section class=\"ltx_section\" id=\"S7\">\\n<h2 class=\"ltx_title ltx_title_section\">\\n<span class=\"ltx_tag ltx_tag_section\">VII </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S7.1.1\">Discussion and Future Prospects</span>\\n</h2>\\n<div class=\"ltx_para\" id=\"S7.p1\">\\n<p class=\"ltx_p\" id=\"S7.p1.1\">Despite the considerable progress in RAG technology, several challenges persist that warrant in-depth research.This chapter will mainly introduce the current challenges and future research directions faced by RAG.</p>\\n</div>\\n<section class=\"ltx_subsection\" id=\"S7.SS1\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S7.SS1.5.1.1\">VII-A</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S7.SS1.6.2\">RAG vs Long Context</span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S7.SS1.p1\">\\n<p class=\"ltx_p\" id=\"S7.SS1.p1.1\">With the deepening of related research, the context of LLMs is continuously expanding\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib170\" title=\"\">170</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib171\" title=\"\">171</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib172\" title=\"\">172</a>]</cite>. Presently, LLMs can effortlessly manage contexts exceeding 200,000 tokens\\xa0<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://kimi.moonshot.cn\" title=\"\">https://kimi.moonshot.cn</a></span></span></span>. This capability signifies that long-document question answering, previously reliant on RAG, can now incorporate the entire document directly into the prompt. This has also sparked discussions on whether RAG is still necessary when LLMs are not constrained by context. In fact, RAG still plays an irreplaceable role. On one hand, providing LLMs with a large amount of context at once will significantly impact its inference speed, while chunked retrieval and on-demand input can significantly improve operational efficiency. On the other hand, RAG-based generation can quickly locate the original references for LLMs to help users verify the generated answers. The entire retrieval and reasoning process is observable, while generation solely relying on long context remains a black box. Conversely, the expansion of context provides new opportunities for the development of RAG, enabling it to address more complex problems and integrative or summary questions that require reading a large amount of material to answer\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib49\" title=\"\">49</a>]</cite>. Developing new RAG methods in the context of super-long contexts is one of the future research trends.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S7.SS2\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S7.SS2.5.1.1\">VII-B</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S7.SS2.6.2\">RAG Robustness</span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S7.SS2.p1\">\\n<p class=\"ltx_p\" id=\"S7.SS2.p1.1\">The presence of noise or contradictory information during retrieval can detrimentally affect RAG’s output quality. This situation is figuratively referred to as “Misinformation can be worse than no information at all”. Improving RAG’s resistance to such adversarial or counterfactual inputs is gaining research momentum and has become a key performance metric\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib50\" title=\"\">50</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib48\" title=\"\">48</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib82\" title=\"\">82</a>]</cite>. Cuconasu et al.\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib54\" title=\"\">54</a>]</cite> analyze which type of documents should be retrieved, evaluate the relevance of the documents to the prompt, their position, and the number included in the context. The research findings reveal that including irrelevant documents can unexpectedly increase accuracy by over 30%, contradicting the initial assumption of reduced quality. These results underscore the importance of developing specialized strategies to integrate retrieval with language generation models, highlighting the need for further research and exploration into the robustness of RAG.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S7.SS3\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S7.SS3.5.1.1\">VII-C</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S7.SS3.6.2\">Hybrid Approaches </span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S7.SS3.p1\">\\n<p class=\"ltx_p\" id=\"S7.SS3.p1.1\">Combining RAG with fine-tuning is emerging as a leading strategy. Determining the optimal integration of RAG and fine-tuning whether sequential, alternating, or through end-to-end joint training—and how to harness both parameterized and non-parameterized advantages are areas ripe for exploration\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib27\" title=\"\">27</a>]</cite>. Another trend is to introduce SLMs with specific functionalities into RAG and fine-tuned by the results of RAG system. For example, CRAG\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib67\" title=\"\">67</a>]</cite> trains a lightweight retrieval evaluator to assess the overall quality of the retrieved documents for a query and triggers different knowledge retrieval actions based on confidence levels.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S7.SS4\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S7.SS4.5.1.1\">VII-D</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S7.SS4.6.2\">Scaling laws of RAG </span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S7.SS4.p1\">\\n<p class=\"ltx_p\" id=\"S7.SS4.p1.1\">End-to-end RAG models and pre-trained models based on RAG are still one of the focuses of current researchers\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib173\" title=\"\">173</a>]</cite>.The parameters of these models are one of the key factors.While scaling laws\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib174\" title=\"\">174</a>]</cite> are established for LLMs, their applicability to RAG remains uncertain. Initial studies like RETRO++\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib44\" title=\"\">44</a>]</cite> have begun to address this, yet the parameter count in RAG models still lags behind that of LLMs. The possibility of an Inverse Scaling Law\\xa0<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/inverse-scaling/prize\" title=\"\">https://github.com/inverse-scaling/prize</a></span></span></span>, where smaller models outperform larger ones, is particularly intriguing and merits further investigation.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S7.SS5\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S7.SS5.5.1.1\">VII-E</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S7.SS5.6.2\">Production-Ready RAG</span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S7.SS5.p1\">\\n<p class=\"ltx_p\" id=\"S7.SS5.p1.1\">RAG’s practicality and alignment with engineering requirements have facilitated its adoption. However, enhancing retrieval efficiency, improving document recall in large knowledge bases, and ensuring data security—such as preventing inadvertent disclosure of document sources or metadata by LLMs—are critical engineering challenges that remain to be addressed\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib175\" title=\"\">175</a>]</cite>.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S7.SS5.p2\">\\n<p class=\"ltx_p\" id=\"S7.SS5.p2.1\">The development of the RAG ecosystem is greatly impacted by the progression of its technical stack. Key tools like LangChain and LLamaIndex have quickly gained popularity with the emergence of ChatGPT, providing extensive RAG-related APIs and becoming essential in the realm of LLMs.The emerging technology stack, while not as rich in features as LangChain and LLamaIndex, stands out through its specialized products. For example, Flowise AI prioritizes a low-code approach, allowing users to deploy AI applications, including RAG, through a user-friendly drag-and-drop interface. Other technologies like HayStack, Meltano, and Cohere Coral are also gaining attention for their unique contributions to the field.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S7.SS5.p3\">\\n<p class=\"ltx_p\" id=\"S7.SS5.p3.1\">In addition to AI-focused vendors, traditional software and cloud service providers are expanding their offerings to include RAG-centric services. Weaviate’s Verba\\xa0<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/weaviate/Verba\" title=\"\">https://github.com/weaviate/Verba</a></span></span></span> is designed for personal assistant applications, while Amazon’s Kendra\\xa0<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://aws.amazon.com/cn/kendra/\" title=\"\">https://aws.amazon.com/cn/kendra/</a></span></span></span> offers intelligent enterprise search services, enabling users to browse various content repositories using built-in connectors. In the development of RAG technology, there is a clear trend towards different specialization directions, such as: 1) Customization - tailoring RAG to meet specific requirements. 2) Simplification - making RAG easier to use to reduce the initial learning curve. 3) Specialization - optimizing RAG to better serve production environments.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S7.SS5.p4\">\\n<p class=\"ltx_p\" id=\"S7.SS5.p4.1\">The mutual growth of RAG models and their technology stacks is evident; technological advancements continuously establish new standards for existing infrastructure. In turn, enhancements to the technology stack drive the development of RAG capabilities. RAG toolkits are converging into a foundational technology stack, laying the groundwork for advanced enterprise applications. However, a fully integrated, comprehensive platform concept is still in the future, requiring further innovation and development.</p>\\n</div>\\n<figure class=\"ltx_figure\" id=\"S7.F6\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"254\" id=\"S7.F6.g1\" src=\"extracted/5498883/images/rag_summary.png\" width=\"413\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 6: </span>Summary of RAG ecosystem</figcaption>\\n</figure>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S7.SS6\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\"><span class=\"ltx_text\" id=\"S7.SS6.5.1.1\">VII-F</span> </span><span class=\"ltx_text ltx_font_italic\" id=\"S7.SS6.6.2\">Multi-modal RAG</span>\\n</h3>\\n<div class=\"ltx_para\" id=\"S7.SS6.p1\">\\n<p class=\"ltx_p\" id=\"S7.SS6.p1.1\">RAG has transcended its initial text-based question-answering confines, embracing a diverse array of modal data. This expansion has spawned innovative multimodal models that integrate RAG concepts across various domains:</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S7.SS6.p2\">\\n<p class=\"ltx_p\" id=\"S7.SS6.p2.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S7.SS6.p2.1.1\">Image</em>. RA-CM3\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib176\" title=\"\">176</a>]</cite> stands as a pioneering multimodal model of both retrieving and generating text and images. BLIP-2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib177\" title=\"\">177</a>]</cite> leverages frozen image encoders alongside LLMs for efficient visual language pre-training, enabling zero-shot image-to-text conversions. The “Visualize Before You Write” method\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib178\" title=\"\">178</a>]</cite> employs image generation to steer the LM’s text generation, showing promise in open-ended text generation tasks.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S7.SS6.p3\">\\n<p class=\"ltx_p\" id=\"S7.SS6.p3.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S7.SS6.p3.1.1\">Audio and Video</em>. The GSS method retrieves and stitches together audio clips to convert machine-translated data into speech-translated data\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib179\" title=\"\">179</a>]</cite>. UEOP marks a significant advancement in end-to-end automatic speech recognition by incorporating external, offline strategies for voice-to-text conversion\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib180\" title=\"\">180</a>]</cite>. Additionally, KNN-based attention fusion leverages audio embeddings and semantically related text embeddings to refine ASR, thereby accelerating domain adaptation. Vid2Seq augments language models with specialized temporal markers, facilitating the prediction of event boundaries and textual descriptions within a unified output sequence\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib181\" title=\"\">181</a>]</cite>.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S7.SS6.p4\">\\n<p class=\"ltx_p\" id=\"S7.SS6.p4.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S7.SS6.p4.1.1\">Code</em>. RBPS\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib182\" title=\"\">182</a>]</cite> excels in small-scale learning tasks by retrieving code examples that align with developers’ objectives through encoding and frequency analysis. This approach has demonstrated efficacy in tasks such as test assertion generation and program repair. For structured knowledge, the CoK method\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#bib.bib106\" title=\"\">106</a>]</cite> first extracts facts pertinent to the input query from a knowledge graph, then integrates these facts as hints within the input, enhancing performance in knowledge graph question-answering tasks.\\n</p>\\n</div>\\n</section>\\n</section>\\n<section class=\"ltx_section\" id=\"S8\">\\n<h2 class=\"ltx_title ltx_title_section\">\\n<span class=\"ltx_tag ltx_tag_section\">VIII </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S8.1.1\">Conclusion</span>\\n</h2>\\n<div class=\"ltx_para\" id=\"S8.p1\">\\n<p class=\"ltx_p\" id=\"S8.p1.1\">The summary of this paper, as depicted in Figure\\xa0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2312.10997v5#S7.F6\" title=\"Figure 6 ‣ VII-E Production-Ready RAG ‣ VII Discussion and Future Prospects ‣ Retrieval-Augmented Generation for Large Language Models: A Survey\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, emphasizes RAG’s significant advancement in enhancing the capabilities of LLMs by integrating parameterized knowledge from language models with extensive non-parameterized data from external knowledge bases. The survey showcases the evolution of RAG technologies and their application on many different tasks. The analysis outlines three developmental paradigms within the RAG framework: Naive, Advanced, and Modular RAG, each representing a progressive enhancement over its predecessors. RAG’s technical integration with other AI methodologies, such as fine-tuning and reinforcement learning, has further expanded its capabilities. Despite the progress in RAG technology, there are research opportunities to improve its robustness and its ability to handle extended contexts. RAG’s application scope is expanding into multimodal domains, adapting its principles to interpret and process diverse data forms like images, videos, and code. This expansion highlights RAG’s significant practical implications for AI deployment, attracting interest from academic and industrial sectors. The growing ecosystem of RAG is evidenced by the rise in RAG-centric AI applications and the continuous development of supportive tools. As RAG’s application landscape broadens, there is a need to refine evaluation methodologies to keep pace with its evolution. Ensuring accurate and representative performance assessments is crucial for fully capturing RAG’s contributions to the AI research and development community.</p>\\n</div>\\n</section>\\n<section class=\"ltx_bibliography\" id=\"bib\">\\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\\n<ul class=\"ltx_biblist\">\\n<li class=\"ltx_bibitem\" id=\"bib.bib1\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[1]</span>\\n<span class=\"ltx_bibblock\">\\nN.\\xa0Kandpal, H.\\xa0Deng, A.\\xa0Roberts, E.\\xa0Wallace, and C.\\xa0Raffel, “Large language models struggle to learn long-tail knowledge,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib1.1.1\">International Conference on Machine Learning</em>.\\xa0\\xa0\\xa0PMLR, 2023, pp. 15\\u2009696–15\\u2009707.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib2\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[2]</span>\\n<span class=\"ltx_bibblock\">\\nY.\\xa0Zhang, Y.\\xa0Li, L.\\xa0Cui, D.\\xa0Cai, L.\\xa0Liu, T.\\xa0Fu, X.\\xa0Huang, E.\\xa0Zhao, Y.\\xa0Zhang, Y.\\xa0Chen <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib2.1.1\">et\\xa0al.</em>, “Siren’s song in the ai ocean: A survey on hallucination in large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib2.2.2\">arXiv preprint arXiv:2309.01219</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib3\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[3]</span>\\n<span class=\"ltx_bibblock\">\\nD.\\xa0Arora, A.\\xa0Kini, S.\\xa0R. Chowdhury, N.\\xa0Natarajan, G.\\xa0Sinha, and A.\\xa0Sharma, “Gar-meets-rag paradigm for zero-shot information retrieval,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib3.1.1\">arXiv preprint arXiv:2310.20158</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib4\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[4]</span>\\n<span class=\"ltx_bibblock\">\\nP.\\xa0Lewis, E.\\xa0Perez, A.\\xa0Piktus, F.\\xa0Petroni, V.\\xa0Karpukhin, N.\\xa0Goyal, H.\\xa0Küttler, M.\\xa0Lewis, W.-t. Yih, T.\\xa0Rocktäschel <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib4.1.1\">et\\xa0al.</em>, “Retrieval-augmented generation for knowledge-intensive nlp tasks,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib4.2.2\">Advances in Neural Information Processing Systems</em>, vol.\\xa033, pp. 9459–9474, 2020.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib5\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[5]</span>\\n<span class=\"ltx_bibblock\">\\nS.\\xa0Borgeaud, A.\\xa0Mensch, J.\\xa0Hoffmann, T.\\xa0Cai, E.\\xa0Rutherford, K.\\xa0Millican, G.\\xa0B. Van Den\\xa0Driessche, J.-B. Lespiau, B.\\xa0Damoc, A.\\xa0Clark <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib5.1.1\">et\\xa0al.</em>, “Improving language models by retrieving from trillions of tokens,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib5.2.2\">International conference on machine learning</em>.\\xa0\\xa0\\xa0PMLR, 2022, pp. 2206–2240.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib6\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[6]</span>\\n<span class=\"ltx_bibblock\">\\nL.\\xa0Ouyang, J.\\xa0Wu, X.\\xa0Jiang, D.\\xa0Almeida, C.\\xa0Wainwright, P.\\xa0Mishkin, C.\\xa0Zhang, S.\\xa0Agarwal, K.\\xa0Slama, A.\\xa0Ray <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib6.1.1\">et\\xa0al.</em>, “Training language models to follow instructions with human feedback,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib6.2.2\">Advances in neural information processing systems</em>, vol.\\xa035, pp. 27\\u2009730–27\\u2009744, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib7\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[7]</span>\\n<span class=\"ltx_bibblock\">\\nX.\\xa0Ma, Y.\\xa0Gong, P.\\xa0He, H.\\xa0Zhao, and N.\\xa0Duan, “Query rewriting for retrieval-augmented large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib7.1.1\">arXiv preprint arXiv:2305.14283</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib8\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[8]</span>\\n<span class=\"ltx_bibblock\">\\nI.\\xa0ILIN, “Advanced rag techniques: an illustrated overview,” <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6\" title=\"\">https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6</a>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib9\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[9]</span>\\n<span class=\"ltx_bibblock\">\\nW.\\xa0Peng, G.\\xa0Li, Y.\\xa0Jiang, Z.\\xa0Wang, D.\\xa0Ou, X.\\xa0Zeng, E.\\xa0Chen <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib9.1.1\">et\\xa0al.</em>, “Large language model based long-tail query rewriting in taobao search,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib9.2.2\">arXiv preprint arXiv:2311.03758</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib10\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[10]</span>\\n<span class=\"ltx_bibblock\">\\nH.\\xa0S. Zheng, S.\\xa0Mishra, X.\\xa0Chen, H.-T. Cheng, E.\\xa0H. Chi, Q.\\xa0V. Le, and D.\\xa0Zhou, “Take a step back: Evoking reasoning via abstraction in large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib10.1.1\">arXiv preprint arXiv:2310.06117</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib11\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[11]</span>\\n<span class=\"ltx_bibblock\">\\nL.\\xa0Gao, X.\\xa0Ma, J.\\xa0Lin, and J.\\xa0Callan, “Precise zero-shot dense retrieval without relevance labels,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib11.1.1\">arXiv preprint arXiv:2212.10496</em>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib12\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[12]</span>\\n<span class=\"ltx_bibblock\">\\nV.\\xa0Blagojevi, “Enhancing rag pipelines in haystack: Introducing diversityranker and lostinthemiddleranker,” <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://towardsdatascience.com/enhancing-rag-pipelines-in-haystack-45f14e2bc9f5\" title=\"\">https://towardsdatascience.com/enhancing-rag-pipelines-in-haystack-45f14e2bc9f5</a>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib13\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[13]</span>\\n<span class=\"ltx_bibblock\">\\nW.\\xa0Yu, D.\\xa0Iter, S.\\xa0Wang, Y.\\xa0Xu, M.\\xa0Ju, S.\\xa0Sanyal, C.\\xa0Zhu, M.\\xa0Zeng, and M.\\xa0Jiang, “Generate rather than retrieve: Large language models are strong context generators,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib13.1.1\">arXiv preprint arXiv:2209.10063</em>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib14\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[14]</span>\\n<span class=\"ltx_bibblock\">\\nZ.\\xa0Shao, Y.\\xa0Gong, Y.\\xa0Shen, M.\\xa0Huang, N.\\xa0Duan, and W.\\xa0Chen, “Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib14.1.1\">arXiv preprint arXiv:2305.15294</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib15\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[15]</span>\\n<span class=\"ltx_bibblock\">\\nX.\\xa0Wang, Q.\\xa0Yang, Y.\\xa0Qiu, J.\\xa0Liang, Q.\\xa0He, Z.\\xa0Gu, Y.\\xa0Xiao, and W.\\xa0Wang, “Knowledgpt: Enhancing large language models with retrieval and storage access on knowledge bases,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib15.1.1\">arXiv preprint arXiv:2308.11761</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib16\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[16]</span>\\n<span class=\"ltx_bibblock\">\\nA.\\xa0H. Raudaschl, “Forget rag, the future is rag-fusion,” <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1\" title=\"\">https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1</a>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib17\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[17]</span>\\n<span class=\"ltx_bibblock\">\\nX.\\xa0Cheng, D.\\xa0Luo, X.\\xa0Chen, L.\\xa0Liu, D.\\xa0Zhao, and R.\\xa0Yan, “Lift yourself up: Retrieval-augmented text generation with self memory,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib17.1.1\">arXiv preprint arXiv:2305.02437</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib18\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[18]</span>\\n<span class=\"ltx_bibblock\">\\nS.\\xa0Wang, Y.\\xa0Xu, Y.\\xa0Fang, Y.\\xa0Liu, S.\\xa0Sun, R.\\xa0Xu, C.\\xa0Zhu, and M.\\xa0Zeng, “Training data is more valuable than you think: A simple and effective method by retrieving from training data,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib18.1.1\">arXiv preprint arXiv:2203.08773</em>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib19\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[19]</span>\\n<span class=\"ltx_bibblock\">\\nX.\\xa0Li, E.\\xa0Nie, and S.\\xa0Liang, “From classification to generation: Insights into crosslingual retrieval augmented icl,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib19.1.1\">arXiv preprint arXiv:2311.06595</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib20\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[20]</span>\\n<span class=\"ltx_bibblock\">\\nD.\\xa0Cheng, S.\\xa0Huang, J.\\xa0Bi, Y.\\xa0Zhan, J.\\xa0Liu, Y.\\xa0Wang, H.\\xa0Sun, F.\\xa0Wei, D.\\xa0Deng, and Q.\\xa0Zhang, “Uprise: Universal prompt retrieval for improving zero-shot evaluation,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib20.1.1\">arXiv preprint arXiv:2303.08518</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib21\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[21]</span>\\n<span class=\"ltx_bibblock\">\\nZ.\\xa0Dai, V.\\xa0Y. Zhao, J.\\xa0Ma, Y.\\xa0Luan, J.\\xa0Ni, J.\\xa0Lu, A.\\xa0Bakalov, K.\\xa0Guu, K.\\xa0B. Hall, and M.-W. Chang, “Promptagator: Few-shot dense retrieval from 8 examples,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib21.1.1\">arXiv preprint arXiv:2209.11755</em>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib22\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[22]</span>\\n<span class=\"ltx_bibblock\">\\nZ.\\xa0Sun, X.\\xa0Wang, Y.\\xa0Tay, Y.\\xa0Yang, and D.\\xa0Zhou, “Recitation-augmented language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib22.1.1\">arXiv preprint arXiv:2210.01296</em>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib23\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[23]</span>\\n<span class=\"ltx_bibblock\">\\nO.\\xa0Khattab, K.\\xa0Santhanam, X.\\xa0L. Li, D.\\xa0Hall, P.\\xa0Liang, C.\\xa0Potts, and M.\\xa0Zaharia, “Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib23.1.1\">arXiv preprint arXiv:2212.14024</em>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib24\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[24]</span>\\n<span class=\"ltx_bibblock\">\\nZ.\\xa0Jiang, F.\\xa0F. Xu, L.\\xa0Gao, Z.\\xa0Sun, Q.\\xa0Liu, J.\\xa0Dwivedi-Yu, Y.\\xa0Yang, J.\\xa0Callan, and G.\\xa0Neubig, “Active retrieval augmented generation,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib24.1.1\">arXiv preprint arXiv:2305.06983</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib25\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[25]</span>\\n<span class=\"ltx_bibblock\">\\nA.\\xa0Asai, Z.\\xa0Wu, Y.\\xa0Wang, A.\\xa0Sil, and H.\\xa0Hajishirzi, “Self-rag: Learning to retrieve, generate, and critique through self-reflection,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib25.1.1\">arXiv preprint arXiv:2310.11511</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib26\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[26]</span>\\n<span class=\"ltx_bibblock\">\\nZ.\\xa0Ke, W.\\xa0Kong, C.\\xa0Li, M.\\xa0Zhang, Q.\\xa0Mei, and M.\\xa0Bendersky, “Bridging the preference gap between retrievers and llms,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib26.1.1\">arXiv preprint arXiv:2401.06954</em>, 2024.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib27\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[27]</span>\\n<span class=\"ltx_bibblock\">\\nX.\\xa0V. Lin, X.\\xa0Chen, M.\\xa0Chen, W.\\xa0Shi, M.\\xa0Lomeli, R.\\xa0James, P.\\xa0Rodriguez, J.\\xa0Kahn, G.\\xa0Szilvasy, M.\\xa0Lewis <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib27.1.1\">et\\xa0al.</em>, “Ra-dit: Retrieval-augmented dual instruction tuning,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib27.2.2\">arXiv preprint arXiv:2310.01352</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib28\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[28]</span>\\n<span class=\"ltx_bibblock\">\\nO.\\xa0Ovadia, M.\\xa0Brief, M.\\xa0Mishaeli, and O.\\xa0Elisha, “Fine-tuning or retrieval? comparing knowledge injection in llms,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib28.1.1\">arXiv preprint arXiv:2312.05934</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib29\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[29]</span>\\n<span class=\"ltx_bibblock\">\\nT.\\xa0Lan, D.\\xa0Cai, Y.\\xa0Wang, H.\\xa0Huang, and X.-L. Mao, “Copy is all you need,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib29.1.1\">The Eleventh International Conference on Learning Representations</em>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib30\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[30]</span>\\n<span class=\"ltx_bibblock\">\\nT.\\xa0Chen, H.\\xa0Wang, S.\\xa0Chen, W.\\xa0Yu, K.\\xa0Ma, X.\\xa0Zhao, D.\\xa0Yu, and H.\\xa0Zhang, “Dense x retrieval: What retrieval granularity should we use?” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib30.1.1\">arXiv preprint arXiv:2312.06648</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib31\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[31]</span>\\n<span class=\"ltx_bibblock\">\\nF.\\xa0Luo and M.\\xa0Surdeanu, “Divide &amp; conquer for entailment-aware multi-hop evidence retrieval,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib31.1.1\">arXiv preprint arXiv:2311.02616</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib32\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[32]</span>\\n<span class=\"ltx_bibblock\">\\nQ.\\xa0Gou, Z.\\xa0Xia, B.\\xa0Yu, H.\\xa0Yu, F.\\xa0Huang, Y.\\xa0Li, and N.\\xa0Cam-Tu, “Diversify question generation with retrieval-augmented style transfer,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib32.1.1\">arXiv preprint arXiv:2310.14503</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib33\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[33]</span>\\n<span class=\"ltx_bibblock\">\\nZ.\\xa0Guo, S.\\xa0Cheng, Y.\\xa0Wang, P.\\xa0Li, and Y.\\xa0Liu, “Prompt-guided retrieval augmentation for non-knowledge-intensive tasks,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib33.1.1\">arXiv preprint arXiv:2305.17653</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib34\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[34]</span>\\n<span class=\"ltx_bibblock\">\\nZ.\\xa0Wang, J.\\xa0Araki, Z.\\xa0Jiang, M.\\xa0R. Parvez, and G.\\xa0Neubig, “Learning to filter context for retrieval-augmented generation,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib34.1.1\">arXiv preprint arXiv:2311.08377</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib35\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[35]</span>\\n<span class=\"ltx_bibblock\">\\nM.\\xa0Seo, J.\\xa0Baek, J.\\xa0Thorne, and S.\\xa0J. Hwang, “Retrieval-augmented data augmentation for low-resource domain tasks,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib35.1.1\">arXiv preprint arXiv:2402.13482</em>, 2024.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib36\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[36]</span>\\n<span class=\"ltx_bibblock\">\\nY.\\xa0Ma, Y.\\xa0Cao, Y.\\xa0Hong, and A.\\xa0Sun, “Large language model is not a good few-shot information extractor, but a good reranker for hard samples!” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib36.1.1\">arXiv preprint arXiv:2303.08559</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib37\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[37]</span>\\n<span class=\"ltx_bibblock\">\\nX.\\xa0Du and H.\\xa0Ji, “Retrieval-augmented generative question answering for event argument extraction,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib37.1.1\">arXiv preprint arXiv:2211.07067</em>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib38\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[38]</span>\\n<span class=\"ltx_bibblock\">\\nL.\\xa0Wang, N.\\xa0Yang, and F.\\xa0Wei, “Learning to retrieve in-context examples for large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib38.1.1\">arXiv preprint arXiv:2307.07164</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib39\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[39]</span>\\n<span class=\"ltx_bibblock\">\\nS.\\xa0Rajput, N.\\xa0Mehta, A.\\xa0Singh, R.\\xa0H. Keshavan, T.\\xa0Vu, L.\\xa0Heldt, L.\\xa0Hong, Y.\\xa0Tay, V.\\xa0Q. Tran, J.\\xa0Samost <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib39.1.1\">et\\xa0al.</em>, “Recommender systems with generative retrieval,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib39.2.2\">arXiv preprint arXiv:2305.05065</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib40\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[40]</span>\\n<span class=\"ltx_bibblock\">\\nB.\\xa0Jin, H.\\xa0Zeng, G.\\xa0Wang, X.\\xa0Chen, T.\\xa0Wei, R.\\xa0Li, Z.\\xa0Wang, Z.\\xa0Li, Y.\\xa0Li, H.\\xa0Lu <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib40.1.1\">et\\xa0al.</em>, “Language models as semantic indexers,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib40.2.2\">arXiv preprint arXiv:2310.07815</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib41\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[41]</span>\\n<span class=\"ltx_bibblock\">\\nR.\\xa0Anantha, T.\\xa0Bethi, D.\\xa0Vodianik, and S.\\xa0Chappidi, “Context tuning for retrieval augmented generation,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib41.1.1\">arXiv preprint arXiv:2312.05708</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib42\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[42]</span>\\n<span class=\"ltx_bibblock\">\\nG.\\xa0Izacard, P.\\xa0Lewis, M.\\xa0Lomeli, L.\\xa0Hosseini, F.\\xa0Petroni, T.\\xa0Schick, J.\\xa0Dwivedi-Yu, A.\\xa0Joulin, S.\\xa0Riedel, and E.\\xa0Grave, “Few-shot learning with retrieval augmented language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib42.1.1\">arXiv preprint arXiv:2208.03299</em>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib43\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[43]</span>\\n<span class=\"ltx_bibblock\">\\nJ.\\xa0Huang, W.\\xa0Ping, P.\\xa0Xu, M.\\xa0Shoeybi, K.\\xa0C.-C. Chang, and B.\\xa0Catanzaro, “Raven: In-context learning with retrieval augmented encoder-decoder language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib43.1.1\">arXiv preprint arXiv:2308.07922</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib44\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[44]</span>\\n<span class=\"ltx_bibblock\">\\nB.\\xa0Wang, W.\\xa0Ping, P.\\xa0Xu, L.\\xa0McAfee, Z.\\xa0Liu, M.\\xa0Shoeybi, Y.\\xa0Dong, O.\\xa0Kuchaiev, B.\\xa0Li, C.\\xa0Xiao <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib44.1.1\">et\\xa0al.</em>, “Shall we pretrain autoregressive language models with retrieval? a comprehensive study,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib44.2.2\">arXiv preprint arXiv:2304.06762</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib45\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[45]</span>\\n<span class=\"ltx_bibblock\">\\nB.\\xa0Wang, W.\\xa0Ping, L.\\xa0McAfee, P.\\xa0Xu, B.\\xa0Li, M.\\xa0Shoeybi, and B.\\xa0Catanzaro, “Instructretro: Instruction tuning post retrieval-augmented pretraining,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib45.1.1\">arXiv preprint arXiv:2310.07713</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib46\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[46]</span>\\n<span class=\"ltx_bibblock\">\\nS.\\xa0Siriwardhana, R.\\xa0Weerasekera, E.\\xa0Wen, T.\\xa0Kaluarachchi, R.\\xa0Rana, and S.\\xa0Nanayakkara, “Improving the domain adaptation of retrieval augmented generation (rag) models for open domain question answering,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib46.1.1\">Transactions of the Association for Computational Linguistics</em>, vol.\\xa011, pp. 1–17, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib47\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[47]</span>\\n<span class=\"ltx_bibblock\">\\nZ.\\xa0Yu, C.\\xa0Xiong, S.\\xa0Yu, and Z.\\xa0Liu, “Augmentation-adapted retriever improves generalization of language models as generic plug-in,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib47.1.1\">arXiv preprint arXiv:2305.17331</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib48\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[48]</span>\\n<span class=\"ltx_bibblock\">\\nO.\\xa0Yoran, T.\\xa0Wolfson, O.\\xa0Ram, and J.\\xa0Berant, “Making retrieval-augmented language models robust to irrelevant context,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib48.1.1\">arXiv preprint arXiv:2310.01558</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib49\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[49]</span>\\n<span class=\"ltx_bibblock\">\\nH.-T. Chen, F.\\xa0Xu, S.\\xa0A. Arora, and E.\\xa0Choi, “Understanding retrieval augmentation for long-form question answering,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib49.1.1\">arXiv preprint arXiv:2310.12150</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib50\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[50]</span>\\n<span class=\"ltx_bibblock\">\\nW.\\xa0Yu, H.\\xa0Zhang, X.\\xa0Pan, K.\\xa0Ma, H.\\xa0Wang, and D.\\xa0Yu, “Chain-of-note: Enhancing robustness in retrieval-augmented language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib50.1.1\">arXiv preprint arXiv:2311.09210</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib51\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[51]</span>\\n<span class=\"ltx_bibblock\">\\nS.\\xa0Xu, L.\\xa0Pang, H.\\xa0Shen, X.\\xa0Cheng, and T.-S. Chua, “Search-in-the-chain: Towards accurate, credible and traceable large language models for knowledgeintensive tasks,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib51.1.1\">CoRR, vol. abs/2304.14732</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib52\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[52]</span>\\n<span class=\"ltx_bibblock\">\\nM.\\xa0Berchansky, P.\\xa0Izsak, A.\\xa0Caciularu, I.\\xa0Dagan, and M.\\xa0Wasserblat, “Optimizing retrieval-augmented reader models via token elimination,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib52.1.1\">arXiv preprint arXiv:2310.13682</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib53\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[53]</span>\\n<span class=\"ltx_bibblock\">\\nJ.\\xa0Lála, O.\\xa0O’Donoghue, A.\\xa0Shtedritski, S.\\xa0Cox, S.\\xa0G. Rodriques, and A.\\xa0D. White, “Paperqa: Retrieval-augmented generative agent for scientific research,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib53.1.1\">arXiv preprint arXiv:2312.07559</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib54\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[54]</span>\\n<span class=\"ltx_bibblock\">\\nF.\\xa0Cuconasu, G.\\xa0Trappolini, F.\\xa0Siciliano, S.\\xa0Filice, C.\\xa0Campagnano, Y.\\xa0Maarek, N.\\xa0Tonellotto, and F.\\xa0Silvestri, “The power of noise: Redefining retrieval for rag systems,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib54.1.1\">arXiv preprint arXiv:2401.14887</em>, 2024.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib55\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[55]</span>\\n<span class=\"ltx_bibblock\">\\nZ.\\xa0Zhang, X.\\xa0Zhang, Y.\\xa0Ren, S.\\xa0Shi, M.\\xa0Han, Y.\\xa0Wu, R.\\xa0Lai, and Z.\\xa0Cao, “Iag: Induction-augmented generation framework for answering reasoning questions,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib55.1.1\">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, 2023, pp. 1–14.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib56\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[56]</span>\\n<span class=\"ltx_bibblock\">\\nN.\\xa0Thakur, L.\\xa0Bonifacio, X.\\xa0Zhang, O.\\xa0Ogundepo, E.\\xa0Kamalloo, D.\\xa0Alfonso-Hermelo, X.\\xa0Li, Q.\\xa0Liu, B.\\xa0Chen, M.\\xa0Rezagholizadeh <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib56.1.1\">et\\xa0al.</em>, “Nomiracl: Knowing when you don’t know for robust multilingual retrieval-augmented generation,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib56.2.2\">arXiv preprint arXiv:2312.11361</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib57\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[57]</span>\\n<span class=\"ltx_bibblock\">\\nG.\\xa0Kim, S.\\xa0Kim, B.\\xa0Jeon, J.\\xa0Park, and J.\\xa0Kang, “Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib57.1.1\">arXiv preprint arXiv:2310.14696</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib58\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[58]</span>\\n<span class=\"ltx_bibblock\">\\nY.\\xa0Wang, P.\\xa0Li, M.\\xa0Sun, and Y.\\xa0Liu, “Self-knowledge guided retrieval augmentation for large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib58.1.1\">arXiv preprint arXiv:2310.05002</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib59\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[59]</span>\\n<span class=\"ltx_bibblock\">\\nZ.\\xa0Feng, X.\\xa0Feng, D.\\xa0Zhao, M.\\xa0Yang, and B.\\xa0Qin, “Retrieval-generation synergy augmented large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib59.1.1\">arXiv preprint arXiv:2310.05149</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib60\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[60]</span>\\n<span class=\"ltx_bibblock\">\\nP.\\xa0Xu, W.\\xa0Ping, X.\\xa0Wu, L.\\xa0McAfee, C.\\xa0Zhu, Z.\\xa0Liu, S.\\xa0Subramanian, E.\\xa0Bakhturina, M.\\xa0Shoeybi, and B.\\xa0Catanzaro, “Retrieval meets long context large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib60.1.1\">arXiv preprint arXiv:2310.03025</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib61\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[61]</span>\\n<span class=\"ltx_bibblock\">\\nH.\\xa0Trivedi, N.\\xa0Balasubramanian, T.\\xa0Khot, and A.\\xa0Sabharwal, “Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib61.1.1\">arXiv preprint arXiv:2212.10509</em>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib62\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[62]</span>\\n<span class=\"ltx_bibblock\">\\nR.\\xa0Ren, Y.\\xa0Wang, Y.\\xa0Qu, W.\\xa0X. Zhao, J.\\xa0Liu, H.\\xa0Tian, H.\\xa0Wu, J.-R. Wen, and H.\\xa0Wang, “Investigating the factual knowledge boundary of large language models with retrieval augmentation,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib62.1.1\">arXiv preprint arXiv:2307.11019</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib63\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[63]</span>\\n<span class=\"ltx_bibblock\">\\nP.\\xa0Sarthi, S.\\xa0Abdullah, A.\\xa0Tuli, S.\\xa0Khanna, A.\\xa0Goldie, and C.\\xa0D. Manning, “Raptor: Recursive abstractive processing for tree-organized retrieval,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib63.1.1\">arXiv preprint arXiv:2401.18059</em>, 2024.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib64\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[64]</span>\\n<span class=\"ltx_bibblock\">\\nO.\\xa0Ram, Y.\\xa0Levine, I.\\xa0Dalmedigos, D.\\xa0Muhlgay, A.\\xa0Shashua, K.\\xa0Leyton-Brown, and Y.\\xa0Shoham, “In-context retrieval-augmented language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib64.1.1\">arXiv preprint arXiv:2302.00083</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib65\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[65]</span>\\n<span class=\"ltx_bibblock\">\\nY.\\xa0Ren, Y.\\xa0Cao, P.\\xa0Guo, F.\\xa0Fang, W.\\xa0Ma, and Z.\\xa0Lin, “Retrieve-and-sample: Document-level event argument extraction via hybrid retrieval augmentation,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib65.1.1\">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 2023, pp. 293–306.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib66\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[66]</span>\\n<span class=\"ltx_bibblock\">\\nZ.\\xa0Wang, X.\\xa0Pan, D.\\xa0Yu, D.\\xa0Yu, J.\\xa0Chen, and H.\\xa0Ji, “Zemi: Learning zero-shot semi-parametric language models from multiple tasks,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib66.1.1\">arXiv preprint arXiv:2210.00185</em>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib67\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[67]</span>\\n<span class=\"ltx_bibblock\">\\nS.-Q. Yan, J.-C. Gu, Y.\\xa0Zhu, and Z.-H. Ling, “Corrective retrieval augmented generation,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib67.1.1\">arXiv preprint arXiv:2401.15884</em>, 2024.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib68\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[68]</span>\\n<span class=\"ltx_bibblock\">\\nP.\\xa0Jain, L.\\xa0B. Soares, and T.\\xa0Kwiatkowski, “1-pager: One pass answer generation and evidence retrieval,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib68.1.1\">arXiv preprint arXiv:2310.16568</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib69\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[69]</span>\\n<span class=\"ltx_bibblock\">\\nH.\\xa0Yang, Z.\\xa0Li, Y.\\xa0Zhang, J.\\xa0Wang, N.\\xa0Cheng, M.\\xa0Li, and J.\\xa0Xiao, “Prca: Fitting black-box large language models for retrieval question answering via pluggable reward-driven contextual adapter,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib69.1.1\">arXiv preprint arXiv:2310.18347</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib70\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[70]</span>\\n<span class=\"ltx_bibblock\">\\nS.\\xa0Zhuang, B.\\xa0Liu, B.\\xa0Koopman, and G.\\xa0Zuccon, “Open-source large language models are strong zero-shot query likelihood models for document ranking,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib70.1.1\">arXiv preprint arXiv:2310.13243</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib71\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[71]</span>\\n<span class=\"ltx_bibblock\">\\nF.\\xa0Xu, W.\\xa0Shi, and E.\\xa0Choi, “Recomp: Improving retrieval-augmented lms with compression and selective augmentation,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib71.1.1\">arXiv preprint arXiv:2310.04408</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib72\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[72]</span>\\n<span class=\"ltx_bibblock\">\\nW.\\xa0Shi, S.\\xa0Min, M.\\xa0Yasunaga, M.\\xa0Seo, R.\\xa0James, M.\\xa0Lewis, L.\\xa0Zettlemoyer, and W.-t. Yih, “Replug: Retrieval-augmented black-box language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib72.1.1\">arXiv preprint arXiv:2301.12652</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib73\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[73]</span>\\n<span class=\"ltx_bibblock\">\\nE.\\xa0Melz, “Enhancing llm intelligence with arm-rag: Auxiliary rationale memory for retrieval augmented generation,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib73.1.1\">arXiv preprint arXiv:2311.04177</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib74\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[74]</span>\\n<span class=\"ltx_bibblock\">\\nH.\\xa0Wang, W.\\xa0Huang, Y.\\xa0Deng, R.\\xa0Wang, Z.\\xa0Wang, Y.\\xa0Wang, F.\\xa0Mi, J.\\xa0Z. Pan, and K.-F. Wong, “Unims-rag: A unified multi-source retrieval-augmented generation for personalized dialogue systems,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib74.1.1\">arXiv preprint arXiv:2401.13256</em>, 2024.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib75\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[75]</span>\\n<span class=\"ltx_bibblock\">\\nZ.\\xa0Luo, C.\\xa0Xu, P.\\xa0Zhao, X.\\xa0Geng, C.\\xa0Tao, J.\\xa0Ma, Q.\\xa0Lin, and D.\\xa0Jiang, “Augmented large language models with parametric knowledge guiding,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib75.1.1\">arXiv preprint arXiv:2305.04757</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib76\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[76]</span>\\n<span class=\"ltx_bibblock\">\\nX.\\xa0Li, Z.\\xa0Liu, C.\\xa0Xiong, S.\\xa0Yu, Y.\\xa0Gu, Z.\\xa0Liu, and G.\\xa0Yu, “Structure-aware language model pretraining improves dense retrieval on structured data,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib76.1.1\">arXiv preprint arXiv:2305.19912</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib77\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[77]</span>\\n<span class=\"ltx_bibblock\">\\nM.\\xa0Kang, J.\\xa0M. Kwak, J.\\xa0Baek, and S.\\xa0J. Hwang, “Knowledge graph-augmented language models for knowledge-grounded dialogue generation,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib77.1.1\">arXiv preprint arXiv:2305.18846</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib78\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[78]</span>\\n<span class=\"ltx_bibblock\">\\nW.\\xa0Shen, Y.\\xa0Gao, C.\\xa0Huang, F.\\xa0Wan, X.\\xa0Quan, and W.\\xa0Bi, “Retrieval-generation alignment for end-to-end task-oriented dialogue system,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib78.1.1\">arXiv preprint arXiv:2310.08877</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib79\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[79]</span>\\n<span class=\"ltx_bibblock\">\\nT.\\xa0Shi, L.\\xa0Li, Z.\\xa0Lin, T.\\xa0Yang, X.\\xa0Quan, and Q.\\xa0Wang, “Dual-feedback knowledge retrieval for task-oriented dialogue systems,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib79.1.1\">arXiv preprint arXiv:2310.14528</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib80\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[80]</span>\\n<span class=\"ltx_bibblock\">\\nP.\\xa0Ranade and A.\\xa0Joshi, “Fabula: Intelligence report generation using retrieval-augmented narrative construction,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib80.1.1\">arXiv preprint arXiv:2310.13848</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib81\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[81]</span>\\n<span class=\"ltx_bibblock\">\\nX.\\xa0Jiang, R.\\xa0Zhang, Y.\\xa0Xu, R.\\xa0Qiu, Y.\\xa0Fang, Z.\\xa0Wang, J.\\xa0Tang, H.\\xa0Ding, X.\\xa0Chu, J.\\xa0Zhao <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib81.1.1\">et\\xa0al.</em>, “Think and retrieval: A hypothesis knowledge graph enhanced medical large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib81.2.2\">arXiv preprint arXiv:2312.15883</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib82\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[82]</span>\\n<span class=\"ltx_bibblock\">\\nJ.\\xa0Baek, S.\\xa0Jeong, M.\\xa0Kang, J.\\xa0C. Park, and S.\\xa0J. Hwang, “Knowledge-augmented language model verification,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib82.1.1\">arXiv preprint arXiv:2310.12836</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib83\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[83]</span>\\n<span class=\"ltx_bibblock\">\\nL.\\xa0Luo, Y.-F. Li, G.\\xa0Haffari, and S.\\xa0Pan, “Reasoning on graphs: Faithful and interpretable large language model reasoning,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib83.1.1\">arXiv preprint arXiv:2310.01061</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib84\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[84]</span>\\n<span class=\"ltx_bibblock\">\\nX.\\xa0He, Y.\\xa0Tian, Y.\\xa0Sun, N.\\xa0V. Chawla, T.\\xa0Laurent, Y.\\xa0LeCun, X.\\xa0Bresson, and B.\\xa0Hooi, “G-retriever: Retrieval-augmented generation for textual graph understanding and question answering,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib84.1.1\">arXiv preprint arXiv:2402.07630</em>, 2024.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib85\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[85]</span>\\n<span class=\"ltx_bibblock\">\\nL.\\xa0Zha, J.\\xa0Zhou, L.\\xa0Li, R.\\xa0Wang, Q.\\xa0Huang, S.\\xa0Yang, J.\\xa0Yuan, C.\\xa0Su, X.\\xa0Li, A.\\xa0Su <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib85.1.1\">et\\xa0al.</em>, “Tablegpt: Towards unifying tables, nature language and commands into one gpt,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib85.2.2\">arXiv preprint arXiv:2307.08674</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib86\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[86]</span>\\n<span class=\"ltx_bibblock\">\\nM.\\xa0Gaur, K.\\xa0Gunaratna, V.\\xa0Srinivasan, and H.\\xa0Jin, “Iseeq: Information seeking question generation using dynamic meta-information retrieval and knowledge graphs,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib86.1.1\">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol.\\xa036, no.\\xa010, 2022, pp. 10\\u2009672–10\\u2009680.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib87\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[87]</span>\\n<span class=\"ltx_bibblock\">\\nF.\\xa0Shi, X.\\xa0Chen, K.\\xa0Misra, N.\\xa0Scales, D.\\xa0Dohan, E.\\xa0H. Chi, N.\\xa0Schärli, and D.\\xa0Zhou, “Large language models can be easily distracted by irrelevant context,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib87.1.1\">International Conference on Machine Learning</em>.\\xa0\\xa0\\xa0PMLR, 2023, pp. 31\\u2009210–31\\u2009227.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib88\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[88]</span>\\n<span class=\"ltx_bibblock\">\\nR.\\xa0Teja, “Evaluating the ideal chunk size for a rag system using llamaindex,” <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5\" title=\"\">https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5</a>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib89\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[89]</span>\\n<span class=\"ltx_bibblock\">\\nLangchain, “Recursively split by character,” <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter\" title=\"\">https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter</a>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib90\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[90]</span>\\n<span class=\"ltx_bibblock\">\\nS.\\xa0Yang, “Advanced rag 01: Small-to-big retrieval,” <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://towardsdatascience.com/advanced-rag-01-small-to-big-retrieval-172181b396d4\" title=\"\">https://towardsdatascience.com/advanced-rag-01-small-to-big-retrieval-172181b396d4</a>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib91\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[91]</span>\\n<span class=\"ltx_bibblock\">\\nY.\\xa0Wang, N.\\xa0Lipka, R.\\xa0A. Rossi, A.\\xa0Siu, R.\\xa0Zhang, and T.\\xa0Derr, “Knowledge graph prompting for multi-document question answering,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib91.1.1\">arXiv preprint arXiv:2308.11730</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib92\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[92]</span>\\n<span class=\"ltx_bibblock\">\\nD.\\xa0Zhou, N.\\xa0Schärli, L.\\xa0Hou, J.\\xa0Wei, N.\\xa0Scales, X.\\xa0Wang, D.\\xa0Schuurmans, C.\\xa0Cui, O.\\xa0Bousquet, Q.\\xa0Le <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib92.1.1\">et\\xa0al.</em>, “Least-to-most prompting enables complex reasoning in large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib92.2.2\">arXiv preprint arXiv:2205.10625</em>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib93\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[93]</span>\\n<span class=\"ltx_bibblock\">\\nS.\\xa0Dhuliawala, M.\\xa0Komeili, J.\\xa0Xu, R.\\xa0Raileanu, X.\\xa0Li, A.\\xa0Celikyilmaz, and J.\\xa0Weston, “Chain-of-verification reduces hallucination in large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib93.1.1\">arXiv preprint arXiv:2309.11495</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib94\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[94]</span>\\n<span class=\"ltx_bibblock\">\\nX.\\xa0Li and J.\\xa0Li, “Angle-optimized text embeddings,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib94.1.1\">arXiv preprint arXiv:2309.12871</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib95\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[95]</span>\\n<span class=\"ltx_bibblock\">\\nVoyageAI, “Voyage’s embedding models,” <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://docs.voyageai.com/embeddings/\" title=\"\">https://docs.voyageai.com/embeddings/</a>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib96\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[96]</span>\\n<span class=\"ltx_bibblock\">\\nBAAI, “Flagembedding,” <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/FlagOpen/FlagEmbedding\" title=\"\">https://github.com/FlagOpen/FlagEmbedding</a>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib97\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[97]</span>\\n<span class=\"ltx_bibblock\">\\nP.\\xa0Zhang, S.\\xa0Xiao, Z.\\xa0Liu, Z.\\xa0Dou, and J.-Y. Nie, “Retrieve anything to augment large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib97.1.1\">arXiv preprint arXiv:2310.07554</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib98\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[98]</span>\\n<span class=\"ltx_bibblock\">\\nN.\\xa0F. Liu, K.\\xa0Lin, J.\\xa0Hewitt, A.\\xa0Paranjape, M.\\xa0Bevilacqua, F.\\xa0Petroni, and P.\\xa0Liang, “Lost in the middle: How language models use long contexts,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib98.1.1\">arXiv preprint arXiv:2307.03172</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib99\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[99]</span>\\n<span class=\"ltx_bibblock\">\\nY.\\xa0Gao, T.\\xa0Sheng, Y.\\xa0Xiang, Y.\\xa0Xiong, H.\\xa0Wang, and J.\\xa0Zhang, “Chat-rec: Towards interactive and explainable llms-augmented recommender system,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib99.1.1\">arXiv preprint arXiv:2303.14524</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib100\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[100]</span>\\n<span class=\"ltx_bibblock\">\\nN.\\xa0Anderson, C.\\xa0Wilson, and S.\\xa0D. Richardson, “Lingua: Addressing scenarios for live interpretation and automatic dubbing,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib100.1.1\">Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas (Volume 2: Users and Providers Track and Government Track)</em>, J.\\xa0Campbell, S.\\xa0Larocca, J.\\xa0Marciano, K.\\xa0Savenkov, and A.\\xa0Yanishevsky, Eds.\\xa0\\xa0\\xa0Orlando, USA: Association for Machine Translation in the Americas, Sep. 2022, pp. 202–209. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://aclanthology.org/2022.amta-upg.14\" title=\"\">https://aclanthology.org/2022.amta-upg.14</a>\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib101\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[101]</span>\\n<span class=\"ltx_bibblock\">\\nH.\\xa0Jiang, Q.\\xa0Wu, X.\\xa0Luo, D.\\xa0Li, C.-Y. Lin, Y.\\xa0Yang, and L.\\xa0Qiu, “Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib101.1.1\">arXiv preprint arXiv:2310.06839</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib102\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[102]</span>\\n<span class=\"ltx_bibblock\">\\nV.\\xa0Karpukhin, B.\\xa0Oğuz, S.\\xa0Min, P.\\xa0Lewis, L.\\xa0Wu, S.\\xa0Edunov, D.\\xa0Chen, and W.-t. Yih, “Dense passage retrieval for open-domain question answering,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib102.1.1\">arXiv preprint arXiv:2004.04906</em>, 2020.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib103\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[103]</span>\\n<span class=\"ltx_bibblock\">\\nY.\\xa0Ma, Y.\\xa0Cao, Y.\\xa0Hong, and A.\\xa0Sun, “Large language model is not a good few-shot information extractor, but a good reranker for hard samples!” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib103.1.1\">ArXiv</em>, vol. abs/2303.08559, 2023. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://api.semanticscholar.org/CorpusID:257532405\" title=\"\">https://api.semanticscholar.org/CorpusID:257532405</a>\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib104\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[104]</span>\\n<span class=\"ltx_bibblock\">\\nJ.\\xa0Cui, Z.\\xa0Li, Y.\\xa0Yan, B.\\xa0Chen, and L.\\xa0Yuan, “Chatlaw: Open-source legal large language model with integrated external knowledge bases,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib104.1.1\">arXiv preprint arXiv:2306.16092</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib105\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[105]</span>\\n<span class=\"ltx_bibblock\">\\nO.\\xa0Yoran, T.\\xa0Wolfson, O.\\xa0Ram, and J.\\xa0Berant, “Making retrieval-augmented language models robust to irrelevant context,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib105.1.1\">arXiv preprint arXiv:2310.01558</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib106\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[106]</span>\\n<span class=\"ltx_bibblock\">\\nX.\\xa0Li, R.\\xa0Zhao, Y.\\xa0K. Chia, B.\\xa0Ding, L.\\xa0Bing, S.\\xa0Joty, and S.\\xa0Poria, “Chain of knowledge: A framework for grounding large language models with structured knowledge bases,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib106.1.1\">arXiv preprint arXiv:2305.13269</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib107\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[107]</span>\\n<span class=\"ltx_bibblock\">\\nH.\\xa0Yang, S.\\xa0Yue, and Y.\\xa0He, “Auto-gpt for online decision making: Benchmarks and additional opinions,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib107.1.1\">arXiv preprint arXiv:2306.02224</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib108\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[108]</span>\\n<span class=\"ltx_bibblock\">\\nT.\\xa0Schick, J.\\xa0Dwivedi-Yu, R.\\xa0Dessì, R.\\xa0Raileanu, M.\\xa0Lomeli, L.\\xa0Zettlemoyer, N.\\xa0Cancedda, and T.\\xa0Scialom, “Toolformer: Language models can teach themselves to use tools,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib108.1.1\">arXiv preprint arXiv:2302.04761</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib109\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[109]</span>\\n<span class=\"ltx_bibblock\">\\nJ.\\xa0Zhang, “Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib109.1.1\">arXiv preprint arXiv:2304.11116</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib110\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[110]</span>\\n<span class=\"ltx_bibblock\">\\nR.\\xa0Nakano, J.\\xa0Hilton, S.\\xa0Balaji, J.\\xa0Wu, L.\\xa0Ouyang, C.\\xa0Kim, C.\\xa0Hesse, S.\\xa0Jain, V.\\xa0Kosaraju, W.\\xa0Saunders <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib110.1.1\">et\\xa0al.</em>, “Webgpt: Browser-assisted question-answering with human feedback,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib110.2.2\">arXiv preprint arXiv:2112.09332</em>, 2021.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib111\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[111]</span>\\n<span class=\"ltx_bibblock\">\\nT.\\xa0Kwiatkowski, J.\\xa0Palomaki, O.\\xa0Redfield, M.\\xa0Collins, A.\\xa0Parikh, C.\\xa0Alberti, D.\\xa0Epstein, I.\\xa0Polosukhin, J.\\xa0Devlin, K.\\xa0Lee <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib111.1.1\">et\\xa0al.</em>, “Natural questions: a benchmark for question answering research,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib111.2.2\">Transactions of the Association for Computational Linguistics</em>, vol.\\xa07, pp. 453–466, 2019.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib112\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[112]</span>\\n<span class=\"ltx_bibblock\">\\nY.\\xa0Liu, S.\\xa0Yavuz, R.\\xa0Meng, M.\\xa0Moorthy, S.\\xa0Joty, C.\\xa0Xiong, and Y.\\xa0Zhou, “Exploring the integration strategies of retriever and large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib112.1.1\">arXiv preprint arXiv:2308.12574</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib113\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[113]</span>\\n<span class=\"ltx_bibblock\">\\nM.\\xa0Joshi, E.\\xa0Choi, D.\\xa0S. Weld, and L.\\xa0Zettlemoyer, “Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib113.1.1\">arXiv preprint arXiv:1705.03551</em>, 2017.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib114\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[114]</span>\\n<span class=\"ltx_bibblock\">\\nP.\\xa0Rajpurkar, J.\\xa0Zhang, K.\\xa0Lopyrev, and P.\\xa0Liang, “Squad: 100,000+ questions for machine comprehension of text,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib114.1.1\">arXiv preprint arXiv:1606.05250</em>, 2016.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib115\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[115]</span>\\n<span class=\"ltx_bibblock\">\\nJ.\\xa0Berant, A.\\xa0Chou, R.\\xa0Frostig, and P.\\xa0Liang, “Semantic parsing on freebase from question-answer pairs,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib115.1.1\">Proceedings of the 2013 conference on empirical methods in natural language processing</em>, 2013, pp. 1533–1544.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib116\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[116]</span>\\n<span class=\"ltx_bibblock\">\\nA.\\xa0Mallen, A.\\xa0Asai, V.\\xa0Zhong, R.\\xa0Das, H.\\xa0Hajishirzi, and D.\\xa0Khashabi, “When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib116.1.1\">arXiv preprint arXiv:2212.10511</em>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib117\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[117]</span>\\n<span class=\"ltx_bibblock\">\\nT.\\xa0Nguyen, M.\\xa0Rosenberg, X.\\xa0Song, J.\\xa0Gao, S.\\xa0Tiwary, R.\\xa0Majumder, and L.\\xa0Deng, “Ms marco: A human-generated machine reading comprehension dataset,” 2016.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib118\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[118]</span>\\n<span class=\"ltx_bibblock\">\\nZ.\\xa0Yang, P.\\xa0Qi, S.\\xa0Zhang, Y.\\xa0Bengio, W.\\xa0W. Cohen, R.\\xa0Salakhutdinov, and C.\\xa0D. Manning, “Hotpotqa: A dataset for diverse, explainable multi-hop question answering,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib118.1.1\">arXiv preprint arXiv:1809.09600</em>, 2018.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib119\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[119]</span>\\n<span class=\"ltx_bibblock\">\\nX.\\xa0Ho, A.-K.\\xa0D. Nguyen, S.\\xa0Sugawara, and A.\\xa0Aizawa, “Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib119.1.1\">arXiv preprint arXiv:2011.01060</em>, 2020.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib120\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[120]</span>\\n<span class=\"ltx_bibblock\">\\nH.\\xa0Trivedi, N.\\xa0Balasubramanian, T.\\xa0Khot, and A.\\xa0Sabharwal, “Musique: Multihop questions via single-hop question composition,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib120.1.1\">Transactions of the Association for Computational Linguistics</em>, vol.\\xa010, pp. 539–554, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib121\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[121]</span>\\n<span class=\"ltx_bibblock\">\\nA.\\xa0Fan, Y.\\xa0Jernite, E.\\xa0Perez, D.\\xa0Grangier, J.\\xa0Weston, and M.\\xa0Auli, “Eli5: Long form question answering,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib121.1.1\">arXiv preprint arXiv:1907.09190</em>, 2019.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib122\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[122]</span>\\n<span class=\"ltx_bibblock\">\\nT.\\xa0Kočiskỳ, J.\\xa0Schwarz, P.\\xa0Blunsom, C.\\xa0Dyer, K.\\xa0M. Hermann, G.\\xa0Melis, and E.\\xa0Grefenstette, “The narrativeqa reading comprehension challenge,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib122.1.1\">Transactions of the Association for Computational Linguistics</em>, vol.\\xa06, pp. 317–328, 2018.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib123\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[123]</span>\\n<span class=\"ltx_bibblock\">\\nK.-H. Lee, X.\\xa0Chen, H.\\xa0Furuta, J.\\xa0Canny, and I.\\xa0Fischer, “A human-inspired reading agent with gist memory of very long contexts,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib123.1.1\">arXiv preprint arXiv:2402.09727</em>, 2024.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib124\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[124]</span>\\n<span class=\"ltx_bibblock\">\\nI.\\xa0Stelmakh, Y.\\xa0Luan, B.\\xa0Dhingra, and M.-W. Chang, “Asqa: Factoid questions meet long-form answers,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib124.1.1\">arXiv preprint arXiv:2204.06092</em>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib125\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[125]</span>\\n<span class=\"ltx_bibblock\">\\nM.\\xa0Zhong, D.\\xa0Yin, T.\\xa0Yu, A.\\xa0Zaidi, M.\\xa0Mutuma, R.\\xa0Jha, A.\\xa0H. Awadallah, A.\\xa0Celikyilmaz, Y.\\xa0Liu, X.\\xa0Qiu <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib125.1.1\">et\\xa0al.</em>, “Qmsum: A new benchmark for query-based multi-domain meeting summarization,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib125.2.2\">arXiv preprint arXiv:2104.05938</em>, 2021.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib126\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[126]</span>\\n<span class=\"ltx_bibblock\">\\nP.\\xa0Dasigi, K.\\xa0Lo, I.\\xa0Beltagy, A.\\xa0Cohan, N.\\xa0A. Smith, and M.\\xa0Gardner, “A dataset of information-seeking questions and answers anchored in research papers,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib126.1.1\">arXiv preprint arXiv:2105.03011</em>, 2021.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib127\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[127]</span>\\n<span class=\"ltx_bibblock\">\\nT.\\xa0Möller, A.\\xa0Reina, R.\\xa0Jayakumar, and M.\\xa0Pietsch, “Covid-qa: A question answering dataset for covid-19,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib127.1.1\">ACL 2020 Workshop on Natural Language Processing for COVID-19 (NLP-COVID)</em>, 2020.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib128\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[128]</span>\\n<span class=\"ltx_bibblock\">\\nX.\\xa0Wang, G.\\xa0H. Chen, D.\\xa0Song, Z.\\xa0Zhang, Z.\\xa0Chen, Q.\\xa0Xiao, F.\\xa0Jiang, J.\\xa0Li, X.\\xa0Wan, B.\\xa0Wang <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib128.1.1\">et\\xa0al.</em>, “Cmb: A comprehensive medical benchmark in chinese,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib128.2.2\">arXiv preprint arXiv:2308.08833</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib129\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[129]</span>\\n<span class=\"ltx_bibblock\">\\nH.\\xa0Zeng, “Measuring massive multitask chinese understanding,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib129.1.1\">arXiv preprint arXiv:2304.12986</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib130\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[130]</span>\\n<span class=\"ltx_bibblock\">\\nR.\\xa0Y. Pang, A.\\xa0Parrish, N.\\xa0Joshi, N.\\xa0Nangia, J.\\xa0Phang, A.\\xa0Chen, V.\\xa0Padmakumar, J.\\xa0Ma, J.\\xa0Thompson, H.\\xa0He <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib130.1.1\">et\\xa0al.</em>, “Quality: Question answering with long input texts, yes!” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib130.2.2\">arXiv preprint arXiv:2112.08608</em>, 2021.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib131\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[131]</span>\\n<span class=\"ltx_bibblock\">\\nP.\\xa0Clark, I.\\xa0Cowhey, O.\\xa0Etzioni, T.\\xa0Khot, A.\\xa0Sabharwal, C.\\xa0Schoenick, and O.\\xa0Tafjord, “Think you have solved question answering? try arc, the ai2 reasoning challenge,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib131.1.1\">arXiv preprint arXiv:1803.05457</em>, 2018.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib132\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[132]</span>\\n<span class=\"ltx_bibblock\">\\nA.\\xa0Talmor, J.\\xa0Herzig, N.\\xa0Lourie, and J.\\xa0Berant, “Commonsenseqa: A question answering challenge targeting commonsense knowledge,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib132.1.1\">arXiv preprint arXiv:1811.00937</em>, 2018.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib133\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[133]</span>\\n<span class=\"ltx_bibblock\">\\nE.\\xa0Dinan, S.\\xa0Roller, K.\\xa0Shuster, A.\\xa0Fan, M.\\xa0Auli, and J.\\xa0Weston, “Wizard of wikipedia: Knowledge-powered conversational agents,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib133.1.1\">arXiv preprint arXiv:1811.01241</em>, 2018.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib134\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[134]</span>\\n<span class=\"ltx_bibblock\">\\nH.\\xa0Wang, M.\\xa0Hu, Y.\\xa0Deng, R.\\xa0Wang, F.\\xa0Mi, W.\\xa0Wang, Y.\\xa0Wang, W.-C. Kwan, I.\\xa0King, and K.-F. Wong, “Large language models as source planner for personalized knowledge-grounded dialogue,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib134.1.1\">arXiv preprint arXiv:2310.08840</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib135\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[135]</span>\\n<span class=\"ltx_bibblock\">\\n——, “Large language models as source planner for personalized knowledge-grounded dialogue,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib135.1.1\">arXiv preprint arXiv:2310.08840</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib136\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[136]</span>\\n<span class=\"ltx_bibblock\">\\nX.\\xa0Xu, Z.\\xa0Gou, W.\\xa0Wu, Z.-Y. Niu, H.\\xa0Wu, H.\\xa0Wang, and S.\\xa0Wang, “Long time no see! open-domain conversation with long-term persona memory,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib136.1.1\">arXiv preprint arXiv:2203.05797</em>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib137\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[137]</span>\\n<span class=\"ltx_bibblock\">\\nT.-H. Wen, M.\\xa0Gasic, N.\\xa0Mrksic, L.\\xa0M. Rojas-Barahona, P.-H. Su, S.\\xa0Ultes, D.\\xa0Vandyke, and S.\\xa0Young, “Conditional generation and snapshot learning in neural dialogue systems,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib137.1.1\">arXiv preprint arXiv:1606.03352</em>, 2016.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib138\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[138]</span>\\n<span class=\"ltx_bibblock\">\\nR.\\xa0He and J.\\xa0McAuley, “Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib138.1.1\">proceedings of the 25th international conference on world wide web</em>, 2016, pp. 507–517.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib139\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[139]</span>\\n<span class=\"ltx_bibblock\">\\nS.\\xa0Li, H.\\xa0Ji, and J.\\xa0Han, “Document-level event argument extraction by conditional generation,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib139.1.1\">arXiv preprint arXiv:2104.05919</em>, 2021.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib140\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[140]</span>\\n<span class=\"ltx_bibblock\">\\nS.\\xa0Ebner, P.\\xa0Xia, R.\\xa0Culkin, K.\\xa0Rawlins, and B.\\xa0Van\\xa0Durme, “Multi-sentence argument linking,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib140.1.1\">arXiv preprint arXiv:1911.03766</em>, 2019.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib141\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[141]</span>\\n<span class=\"ltx_bibblock\">\\nH.\\xa0Elsahar, P.\\xa0Vougiouklis, A.\\xa0Remaci, C.\\xa0Gravier, J.\\xa0Hare, F.\\xa0Laforest, and E.\\xa0Simperl, “T-rex: A large scale alignment of natural language with knowledge base triples,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib141.1.1\">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</em>, 2018.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib142\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[142]</span>\\n<span class=\"ltx_bibblock\">\\nO.\\xa0Levy, M.\\xa0Seo, E.\\xa0Choi, and L.\\xa0Zettlemoyer, “Zero-shot relation extraction via reading comprehension,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib142.1.1\">arXiv preprint arXiv:1706.04115</em>, 2017.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib143\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[143]</span>\\n<span class=\"ltx_bibblock\">\\nR.\\xa0Zellers, A.\\xa0Holtzman, Y.\\xa0Bisk, A.\\xa0Farhadi, and Y.\\xa0Choi, “Hellaswag: Can a machine really finish your sentence?” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib143.1.1\">arXiv preprint arXiv:1905.07830</em>, 2019.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib144\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[144]</span>\\n<span class=\"ltx_bibblock\">\\nS.\\xa0Kim, S.\\xa0J. Joo, D.\\xa0Kim, J.\\xa0Jang, S.\\xa0Ye, J.\\xa0Shin, and M.\\xa0Seo, “The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib144.1.1\">arXiv preprint arXiv:2305.14045</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib145\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[145]</span>\\n<span class=\"ltx_bibblock\">\\nA.\\xa0Saha, V.\\xa0Pahuja, M.\\xa0Khapra, K.\\xa0Sankaranarayanan, and S.\\xa0Chandar, “Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib145.1.1\">Proceedings of the AAAI conference on artificial intelligence</em>, vol.\\xa032, no.\\xa01, 2018.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib146\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[146]</span>\\n<span class=\"ltx_bibblock\">\\nD.\\xa0Hendrycks, C.\\xa0Burns, S.\\xa0Basart, A.\\xa0Zou, M.\\xa0Mazeika, D.\\xa0Song, and J.\\xa0Steinhardt, “Measuring massive multitask language understanding,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib146.1.1\">arXiv preprint arXiv:2009.03300</em>, 2020.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib147\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[147]</span>\\n<span class=\"ltx_bibblock\">\\nS.\\xa0Merity, C.\\xa0Xiong, J.\\xa0Bradbury, and R.\\xa0Socher, “Pointer sentinel mixture models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib147.1.1\">arXiv preprint arXiv:1609.07843</em>, 2016.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib148\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[148]</span>\\n<span class=\"ltx_bibblock\">\\nM.\\xa0Geva, D.\\xa0Khashabi, E.\\xa0Segal, T.\\xa0Khot, D.\\xa0Roth, and J.\\xa0Berant, “Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib148.1.1\">Transactions of the Association for Computational Linguistics</em>, vol.\\xa09, pp. 346–361, 2021.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib149\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[149]</span>\\n<span class=\"ltx_bibblock\">\\nJ.\\xa0Thorne, A.\\xa0Vlachos, C.\\xa0Christodoulopoulos, and A.\\xa0Mittal, “Fever: a large-scale dataset for fact extraction and verification,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib149.1.1\">arXiv preprint arXiv:1803.05355</em>, 2018.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib150\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[150]</span>\\n<span class=\"ltx_bibblock\">\\nN.\\xa0Kotonya and F.\\xa0Toni, “Explainable automated fact-checking for public health claims,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib150.1.1\">arXiv preprint arXiv:2010.09926</em>, 2020.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib151\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[151]</span>\\n<span class=\"ltx_bibblock\">\\nR.\\xa0Lebret, D.\\xa0Grangier, and M.\\xa0Auli, “Neural text generation from structured data with application to the biography domain,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib151.1.1\">arXiv preprint arXiv:1603.07771</em>, 2016.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib152\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[152]</span>\\n<span class=\"ltx_bibblock\">\\nH.\\xa0Hayashi, P.\\xa0Budania, P.\\xa0Wang, C.\\xa0Ackerson, R.\\xa0Neervannan, and G.\\xa0Neubig, “Wikiasp: A dataset for multi-domain aspect-based summarization,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib152.1.1\">Transactions of the Association for Computational Linguistics</em>, vol.\\xa09, pp. 211–225, 2021.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib153\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[153]</span>\\n<span class=\"ltx_bibblock\">\\nS.\\xa0Narayan, S.\\xa0B. Cohen, and M.\\xa0Lapata, “Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib153.1.1\">arXiv preprint arXiv:1808.08745</em>, 2018.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib154\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[154]</span>\\n<span class=\"ltx_bibblock\">\\nS.\\xa0Saha, J.\\xa0A. Junaed, M.\\xa0Saleki, A.\\xa0S. Sharma, M.\\xa0R. Rifat, M.\\xa0Rahouti, S.\\xa0I. Ahmed, N.\\xa0Mohammed, and M.\\xa0R. Amin, “Vio-lens: A novel dataset of annotated social network posts leading to different forms of communal violence and its evaluation,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib154.1.1\">Proceedings of the First Workshop on Bangla Language Processing (BLP-2023)</em>, 2023, pp. 72–84.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib155\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[155]</span>\\n<span class=\"ltx_bibblock\">\\nX.\\xa0Li and D.\\xa0Roth, “Learning question classifiers,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib155.1.1\">COLING 2002: The 19th International Conference on Computational Linguistics</em>, 2002.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib156\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[156]</span>\\n<span class=\"ltx_bibblock\">\\nR.\\xa0Socher, A.\\xa0Perelygin, J.\\xa0Wu, J.\\xa0Chuang, C.\\xa0D. Manning, A.\\xa0Y. Ng, and C.\\xa0Potts, “Recursive deep models for semantic compositionality over a sentiment treebank,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib156.1.1\">Proceedings of the 2013 conference on empirical methods in natural language processing</em>, 2013, pp. 1631–1642.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib157\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[157]</span>\\n<span class=\"ltx_bibblock\">\\nH.\\xa0Husain, H.-H. Wu, T.\\xa0Gazit, M.\\xa0Allamanis, and M.\\xa0Brockschmidt, “Codesearchnet challenge: Evaluating the state of semantic code search,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib157.1.1\">arXiv preprint arXiv:1909.09436</em>, 2019.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib158\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[158]</span>\\n<span class=\"ltx_bibblock\">\\nK.\\xa0Cobbe, V.\\xa0Kosaraju, M.\\xa0Bavarian, M.\\xa0Chen, H.\\xa0Jun, L.\\xa0Kaiser, M.\\xa0Plappert, J.\\xa0Tworek, J.\\xa0Hilton, R.\\xa0Nakano <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib158.1.1\">et\\xa0al.</em>, “Training verifiers to solve math word problems,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib158.2.2\">arXiv preprint arXiv:2110.14168</em>, 2021.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib159\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[159]</span>\\n<span class=\"ltx_bibblock\">\\nR.\\xa0Steinberger, B.\\xa0Pouliquen, A.\\xa0Widiger, C.\\xa0Ignat, T.\\xa0Erjavec, D.\\xa0Tufis, and D.\\xa0Varga, “The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib159.1.1\">arXiv preprint cs/0609058</em>, 2006.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib160\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[160]</span>\\n<span class=\"ltx_bibblock\">\\nY.\\xa0Hoshi, D.\\xa0Miyashita, Y.\\xa0Ng, K.\\xa0Tatsuno, Y.\\xa0Morioka, O.\\xa0Torii, and J.\\xa0Deguchi, “Ralle: A framework for developing and evaluating retrieval-augmented large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib160.1.1\">arXiv preprint arXiv:2308.10633</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib161\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[161]</span>\\n<span class=\"ltx_bibblock\">\\nJ.\\xa0Liu, “Building production-ready rag applications,” <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.ai.engineer/summit/schedule/building-production-ready-rag-applications\" title=\"\">https://www.ai.engineer/summit/schedule/building-production-ready-rag-applications</a>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib162\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[162]</span>\\n<span class=\"ltx_bibblock\">\\nI.\\xa0Nguyen, “Evaluating rag part i: How to evaluate document retrieval,” <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.deepset.ai/blog/rag-evaluation-retrieval\" title=\"\">https://www.deepset.ai/blog/rag-evaluation-retrieval</a>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib163\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[163]</span>\\n<span class=\"ltx_bibblock\">\\nQ.\\xa0Leng, K.\\xa0Uhlenhuth, and A.\\xa0Polyzotis, “Best practices for llm evaluation of rag applications,” <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG\" title=\"\">https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG</a>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib164\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[164]</span>\\n<span class=\"ltx_bibblock\">\\nS.\\xa0Es, J.\\xa0James, L.\\xa0Espinosa-Anke, and S.\\xa0Schockaert, “Ragas: Automated evaluation of retrieval augmented generation,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib164.1.1\">arXiv preprint arXiv:2309.15217</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib165\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[165]</span>\\n<span class=\"ltx_bibblock\">\\nJ.\\xa0Saad-Falcon, O.\\xa0Khattab, C.\\xa0Potts, and M.\\xa0Zaharia, “Ares: An automated evaluation framework for retrieval-augmented generation systems,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib165.1.1\">arXiv preprint arXiv:2311.09476</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib166\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[166]</span>\\n<span class=\"ltx_bibblock\">\\nC.\\xa0Jarvis and J.\\xa0Allard, “A survey of techniques for maximizing llm performance,” <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://community.openai.com/t/openai-dev-day-2023-breakout-sessions/505213#a-survey-of-techniques-for-maximizing-llm-performance-2\" title=\"\">https://community.openai.com/t/openai-dev-day-2023-breakout-sessions/505213#a-survey-of-techniques-for-maximizing-llm-performance-2</a>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib167\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[167]</span>\\n<span class=\"ltx_bibblock\">\\nJ.\\xa0Chen, H.\\xa0Lin, X.\\xa0Han, and L.\\xa0Sun, “Benchmarking large language models in retrieval-augmented generation,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib167.1.1\">arXiv preprint arXiv:2309.01431</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib168\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[168]</span>\\n<span class=\"ltx_bibblock\">\\nY.\\xa0Liu, L.\\xa0Huang, S.\\xa0Li, S.\\xa0Chen, H.\\xa0Zhou, F.\\xa0Meng, J.\\xa0Zhou, and X.\\xa0Sun, “Recall: A benchmark for llms robustness against external counterfactual knowledge,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib168.1.1\">arXiv preprint arXiv:2311.08147</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib169\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[169]</span>\\n<span class=\"ltx_bibblock\">\\nY.\\xa0Lyu, Z.\\xa0Li, S.\\xa0Niu, F.\\xa0Xiong, B.\\xa0Tang, W.\\xa0Wang, H.\\xa0Wu, H.\\xa0Liu, T.\\xa0Xu, and E.\\xa0Chen, “Crud-rag: A comprehensive chinese benchmark for retrieval-augmented generation of large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib169.1.1\">arXiv preprint arXiv:2401.17043</em>, 2024.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib170\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[170]</span>\\n<span class=\"ltx_bibblock\">\\nP.\\xa0Xu, W.\\xa0Ping, X.\\xa0Wu, L.\\xa0McAfee, C.\\xa0Zhu, Z.\\xa0Liu, S.\\xa0Subramanian, E.\\xa0Bakhturina, M.\\xa0Shoeybi, and B.\\xa0Catanzaro, “Retrieval meets long context large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib170.1.1\">arXiv preprint arXiv:2310.03025</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib171\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[171]</span>\\n<span class=\"ltx_bibblock\">\\nC.\\xa0Packer, V.\\xa0Fang, S.\\xa0G. Patil, K.\\xa0Lin, S.\\xa0Wooders, and J.\\xa0E. Gonzalez, “Memgpt: Towards llms as operating systems,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib171.1.1\">arXiv preprint arXiv:2310.08560</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib172\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[172]</span>\\n<span class=\"ltx_bibblock\">\\nG.\\xa0Xiao, Y.\\xa0Tian, B.\\xa0Chen, S.\\xa0Han, and M.\\xa0Lewis, “Efficient streaming language models with attention sinks,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib172.1.1\">arXiv preprint arXiv:2309.17453</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib173\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[173]</span>\\n<span class=\"ltx_bibblock\">\\nT.\\xa0Zhang, S.\\xa0G. Patil, N.\\xa0Jain, S.\\xa0Shen, M.\\xa0Zaharia, I.\\xa0Stoica, and J.\\xa0E. Gonzalez, “Raft: Adapting language model to domain specific rag,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib173.1.1\">arXiv preprint arXiv:2403.10131</em>, 2024.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib174\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[174]</span>\\n<span class=\"ltx_bibblock\">\\nJ.\\xa0Kaplan, S.\\xa0McCandlish, T.\\xa0Henighan, T.\\xa0B. Brown, B.\\xa0Chess, R.\\xa0Child, S.\\xa0Gray, A.\\xa0Radford, J.\\xa0Wu, and D.\\xa0Amodei, “Scaling laws for neural language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib174.1.1\">arXiv preprint arXiv:2001.08361</em>, 2020.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib175\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[175]</span>\\n<span class=\"ltx_bibblock\">\\nU.\\xa0Alon, F.\\xa0Xu, J.\\xa0He, S.\\xa0Sengupta, D.\\xa0Roth, and G.\\xa0Neubig, “Neuro-symbolic language modeling with automaton-augmented retrieval,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib175.1.1\">International Conference on Machine Learning</em>.\\xa0\\xa0\\xa0PMLR, 2022, pp. 468–485.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib176\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[176]</span>\\n<span class=\"ltx_bibblock\">\\nM.\\xa0Yasunaga, A.\\xa0Aghajanyan, W.\\xa0Shi, R.\\xa0James, J.\\xa0Leskovec, P.\\xa0Liang, M.\\xa0Lewis, L.\\xa0Zettlemoyer, and W.-t. Yih, “Retrieval-augmented multimodal language modeling,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib176.1.1\">arXiv preprint arXiv:2211.12561</em>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib177\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[177]</span>\\n<span class=\"ltx_bibblock\">\\nJ.\\xa0Li, D.\\xa0Li, S.\\xa0Savarese, and S.\\xa0Hoi, “Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib177.1.1\">arXiv preprint arXiv:2301.12597</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib178\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[178]</span>\\n<span class=\"ltx_bibblock\">\\nW.\\xa0Zhu, A.\\xa0Yan, Y.\\xa0Lu, W.\\xa0Xu, X.\\xa0E. Wang, M.\\xa0Eckstein, and W.\\xa0Y. Wang, “Visualize before you write: Imagination-guided open-ended text generation,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib178.1.1\">arXiv preprint arXiv:2210.03765</em>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib179\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[179]</span>\\n<span class=\"ltx_bibblock\">\\nJ.\\xa0Zhao, G.\\xa0Haffar, and E.\\xa0Shareghi, “Generating synthetic speech from spokenvocab for speech translation,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib179.1.1\">arXiv preprint arXiv:2210.08174</em>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib180\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[180]</span>\\n<span class=\"ltx_bibblock\">\\nD.\\xa0M. Chan, S.\\xa0Ghosh, A.\\xa0Rastrow, and B.\\xa0Hoffmeister, “Using external off-policy speech-to-text mappings in contextual end-to-end automated speech recognition,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib180.1.1\">arXiv preprint arXiv:2301.02736</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib181\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[181]</span>\\n<span class=\"ltx_bibblock\">\\nA.\\xa0Yang, A.\\xa0Nagrani, P.\\xa0H. Seo, A.\\xa0Miech, J.\\xa0Pont-Tuset, I.\\xa0Laptev, J.\\xa0Sivic, and C.\\xa0Schmid, “Vid2seq: Large-scale pretraining of a visual language model for dense video captioning,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib181.1.1\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023, pp. 10\\u2009714–10\\u2009726.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib182\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[182]</span>\\n<span class=\"ltx_bibblock\">\\nN.\\xa0Nashid, M.\\xa0Sintaha, and A.\\xa0Mesbah, “Retrieval-based prompt selection for code-related few-shot learning,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib182.1.1\">2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)</em>, 2023, pp. 2450–2462.\\n\\n</span>\\n</li>\\n</ul>\\n</section>\\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\\n</article>\\n</div>\\n<footer class=\"ltx_page_footer\">\\n<div class=\"ltx_page_logo\">Generated  on Wed Mar 27 09:16:19 2024 by <a class=\"ltx_LaTeXML_logo\" href=\"http://dlmf.nist.gov/LaTeXML/\"><span style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span style=\"font-size:70%;position:relative; bottom:2.2pt;\">A</span>T<span style=\"position:relative; bottom:-0.4ex;\">E</span></span><span class=\"ltx_font_smallcaps\">xml</span><img alt=\"[LOGO]\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==\"/></a>\\n</div></footer>\\n</div>\\n</body>\\n</html>\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader = HtmlDocumentLoader(doc_uri, cache_path)\n",
    "doc = loader.load()\n",
    "display(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Retrieval-Augmented Generation for Large Language Models: A Survey\n",
       "\n",
       "Yunfan Gao: Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n",
       "\n",
       "\n",
       "Yun Xiong: Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n",
       "\n",
       "\n",
       "Xinyu Gao: Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n",
       "\n",
       "\n",
       "Kangxiang Jia: Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n",
       "\n",
       "\n",
       "Jinliu Pan: Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n",
       "\n",
       "\n",
       "Yuxi Bi: College of Design and Innovation, Tongji University\n",
       "\n",
       "\n",
       "Yi Dai: Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n",
       "\n",
       "\n",
       "Jiawei Sun: Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n",
       "\n",
       "\n",
       "Meng Wang: College of Design and Innovation, Tongji University\n",
       "\n",
       "\n",
       "Haofen Wang: Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University College of Design and Innovation, Tongji University\n",
       "\n",
       "\n",
       "\n",
       "Abstract\n",
       "\n",
       "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs’ intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\n",
       "\n",
       "IIntroduction\n",
       "\n",
       "Large language models (LLMs) have achieved remarkable success, though they still face significant limitations, especially in domain-specific or knowledge-intensive tasks[1], notably producing “hallucinations”[2]when handling queries beyond their training data or requiring current information. To overcome challenges, Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic similarity calculation. By referencing external knowledge, RAG effectively reduces the problem of generating factually incorrect content. Its integration into LLMs has resulted in widespread adoption, establishing RAG as a key technology in advancing chatbots and enhancing the suitability of LLMs for real-world applications.\n",
       "\n",
       "RAG technology has rapidly developed in recent years, and the technology tree summarizing related research is shown in Figure1. The development trajectory of RAG in the era of large models exhibits several distinct stage characteristics. Initially, RAG’s inception coincided with the rise of the Transformer architecture, focusing on enhancing language models by incorporating additional knowledge through Pre-Training Models (PTM). This early stage was characterized by foundational work aimed at refining pre-training techniques[3,4,5].The subsequent arrival of ChatGPT[6]marked a pivotal moment, with LLM demonstrating powerful in context learning (ICL) capabilities. RAG research shifted towards providing better information for LLMs to answer more complex and knowledge-intensive tasks during the inference stage, leading to rapid development in RAG studies. As research progressed, the enhancement of RAG was no longer limited to the inference stage but began to incorporate more with LLM fine-tuning techniques.\n",
       "\n",
       "The burgeoning field of RAG has experienced swift growth, yet it has not been accompanied by a systematic synthesis that could clarify its broader trajectory. This survey endeavors to fill this gap by mapping out the RAG process and charting its evolution and anticipated future paths, with a focus on the integration of RAG within LLMs. This paper considers both technical paradigms and research methods, summarizing three main research paradigms from over 100 RAG studies, and analyzing key technologies in the core stages of “Retrieval,” “Generation,” and “Augmentation.” On the other hand, current research tends to focus more on methods, lacking analysis and summarization of how to evaluate RAG. This paper comprehensively reviews the downstream tasks, datasets, benchmarks, and evaluation methods applicable to RAG. Overall, this paper sets out to meticulously compile and categorize the foundational technical concepts, historical progression, and the spectrum of RAG methodologies and applications that have emerged post-LLMs. It is designed to equip readers and professionals with a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and speculate on upcoming trends and innovations.\n",
       "\n",
       "Our contributions are as follows:\n",
       "\n",
       "In this survey, we present a thorough and systematic review of the state-of-the-art RAG methods, delineating its evolution through paradigms including naive RAG, advanced RAG, and modular RAG. This review contextualizes the broader scope of RAG research within the landscape of LLMs.\n",
       "\n",
       "We identify and discuss the central technologies integral to the RAG process, specifically focusing on the aspects of “Retrieval”, “Generation” and “Augmentation”, and delve into their synergies, elucidating how these components intricately collaborate to form a cohesive and effective RAG framework.\n",
       "\n",
       "We have summarized the current assessment methods of RAG, covering 26 tasks, nearly 50 datasets, outlining the evaluation objectives and metrics, as well as the current evaluation benchmarks and tools. Additionally, we anticipate future directions for RAG, emphasizing potential enhancements to tackle current challenges.\n",
       "\n",
       "The paper unfolds as follows: SectionIIintroduces the main concept and current paradigms of RAG. The following three sections explore core components—“Retrieval”, “Generation” and “Augmentation”, respectively.\n",
       "SectionIIIfocuses on optimization methods in retrieval,including indexing, query and embedding optimization.\n",
       "SectionIVconcentrates on post-retrieval process and LLM fine-tuning in generation.\n",
       "SectionVanalyzes the three augmentation processes.\n",
       "SectionVIfocuses on RAG’s downstream tasks and evaluation system. SectionVIImainly discusses the challenges that RAG currently faces and its future development directions. At last, the paper concludes in SectionVIII.\n",
       "\n",
       "IIOverview of RAG\n",
       "\n",
       "A typical application of RAG is illustrated in Figure2. Here, a user poses a question to ChatGPT about a recent, widely discussed news. Given ChatGPT’s reliance on pre-training data, it initially lacks the capacity to provide updates on recent developments. RAG bridges this information gap by sourcing and incorporating knowledge from external databases. In this case, it gathers relevant news articles related to the user’s query. These articles, combined with the original question, form a comprehensive prompt that empowers LLMs to generate a well-informed answer.\n",
       "\n",
       "The RAG research paradigm is continuously evolving, and we categorize it into three stages: Naive RAG, Advanced RAG, and Modular RAG, as showed in Figure3. Despite RAG method are cost-effective and surpass the performance of the native LLM, they also exhibit several limitations. The development of Advanced RAG and Modular RAG is a response to these specific shortcomings in Naive RAG.\n",
       "\n",
       "\n",
       "\n",
       "II-ANaive RAG\n",
       "\n",
       "The Naive RAG research paradigm represents the earliest methodology, which gained prominence shortly after the widespread adoption of ChatGPT. The Naive RAG follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a “Retrieve-Read” framework[7].\n",
       "\n",
       "Indexingstarts with the cleaning and extraction of raw data in diverse formats like PDF, HTML, Word, and Markdown, which is then converted into a uniform plain text format. To accommodate the context limitations of language models, text is segmented into smaller, digestible chunks. Chunks are then encoded into vector representations using an embedding model and stored in vector database. This step is crucial for enabling efficient similarity searches in the subsequent retrieval phase.\n",
       "\n",
       "Retrieval. Upon receipt of a user query, the RAG system employs the same encoding model utilized during the indexing phase to transform the query into a vector representation. It then computes the similarity scores between the query vector and the vector of chunks within the indexed corpus. The system prioritizes and retrieves the top K chunks that demonstrate the greatest similarity to the query. These chunks are subsequently used as the expanded context in prompt.\n",
       "\n",
       "Generation. The posed query and selected documents are synthesized into a coherent prompt to which a large language model is tasked with formulating a response. The model’s approach to answering may vary depending on task-specific criteria, allowing it to either draw upon its inherent parametric knowledge or restrict its responses to the information contained within the provided documents. In cases of ongoing dialogues, any existing conversational history can be integrated into the prompt, enabling the model to engage in multi-turn dialogue interactions effectively.\n",
       "\n",
       "However, Naive RAG encounters notable drawbacks:\n",
       "\n",
       "Retrieval Challenges. The retrieval phase often struggles with precision and recall, leading to the selection of misaligned or irrelevant chunks, and the missing of crucial information.\n",
       "\n",
       "Generation Difficulties. In generating responses, the model may face the issue of hallucination, where it produces content not supported by the retrieved context. This phase can also suffer from irrelevance, toxicity, or bias in the outputs, detracting from the quality and reliability of the responses.\n",
       "\n",
       "Augmentation Hurdles. Integrating retrieved information with the different task can be challenging, sometimes resulting in disjointed or incoherent outputs. The process may also encounter redundancy when similar information is retrieved from multiple sources, leading to repetitive responses. Determining the significance and relevance of various passages and ensuring stylistic and tonal consistency add further complexity. Facing complex issues, a single retrieval based on the original query may not suffice to acquire adequate context information.\n",
       "\n",
       "Moreover, there’s a concern that generation models might overly rely on augmented information, leading to outputs that simply echo retrieved content without adding insightful or synthesized information.\n",
       "\n",
       "\n",
       "\n",
       "II-BAdvanced RAG\n",
       "\n",
       "Advanced RAG introduces specific improvements to overcome the limitations of Naive RAG. Focusing on enhancing retrieval quality, it employs pre-retrieval and post-retrieval strategies. To tackle the indexing issues, Advanced RAG refines its indexing techniques through the use of a sliding window approach, fine-grained segmentation, and the incorporation of metadata. Additionally, it incorporates several optimization methods to streamline the retrieval process[8].\n",
       "\n",
       "Pre-retrieval process. In this stage, the primary focus is on optimizing the indexing structure and the original query. The goal of optimizing indexing is to enhance the quality of the content being indexed. This involves strategies: enhancing data granularity, optimizing index structures, adding metadata, alignment optimization, and mixed retrieval. While the goal of query optimization is to make the user’s original question clearer and more suitable for the retrieval task. Common methods include query rewriting query transformation, query expansion and other techniques[7,9,10,11].\n",
       "\n",
       "Post-Retrieval Process. Once relevant context is retrieved, it’s crucial to integrate it effectively with the query. The main methods in post-retrieval process include rerank chunks and context compressing. Re-ranking the retrieved information to relocate the most relevant content to the edges of the prompt is a key strategy. This concept has been implemented in frameworks such as LlamaIndex222https://www.llamaindex.ai, LangChain333https://www.langchain.com/, and HayStack[12]. Feeding all relevant documents directly into LLMs can lead to information overload, diluting the focus on key details with irrelevant content.To mitigate this, post-retrieval efforts concentrate on selecting the essential information, emphasizing critical sections, and shortening the context to be processed.\n",
       "\n",
       "\n",
       "\n",
       "II-CModular RAG\n",
       "\n",
       "The modular RAG architecture advances beyond the former two RAG paradigms, offering enhanced adaptability and versatility. It incorporates diverse strategies for improving its components, such as adding a search module for similarity searches and refining the retriever through fine-tuning. Innovations like restructured RAG modules[13]and rearranged RAG pipelines[14]have been introduced to tackle specific challenges. The shift towards a modular RAG approach is becoming prevalent, supporting both sequential processing and integrated end-to-end training across its components. Despite its distinctiveness, Modular RAG builds upon the foundational principles of Advanced and Naive RAG, illustrating a progression and refinement within the RAG family.\n",
       "\n",
       "The Modular RAG framework introduces additional specialized components to enhance retrieval and processing capabilities. The Search module adapts to specific scenarios, enabling direct searches across various data sources like search engines, databases, and knowledge graphs, using LLM-generated code and query languages[15]. RAG-Fusion addresses traditional search limitations by employing a multi-query strategy that expands user queries into diverse perspectives, utilizing parallel vector searches and intelligent re-ranking to uncover both explicit and transformative knowledge[16]. The Memory module leverages the LLM’s memory to guide retrieval, creating an unbounded memory pool that aligns the text more closely with data distribution through iterative self-enhancement[17,18]. Routing in the RAG system navigates through diverse data sources, selecting the optimal pathway for a query, whether it involves summarization, specific database searches, or merging different information streams[19]. The Predict module aims to reduce redundancy and noise by generating context directly through the LLM, ensuring relevance and accuracy[13]. Lastly, the Task Adapter module tailors RAG to various downstream tasks, automating prompt retrieval for zero-shot inputs and creating task-specific retrievers through few-shot query generation[20,21].This comprehensive approach not only streamlines the retrieval process but also significantly improves the quality and relevance of the information retrieved, catering to a wide array of tasks and queries with enhanced precision and flexibility.\n",
       "\n",
       "Modular RAG offers remarkable adaptability by allowing module substitution or reconfiguration to address specific challenges. This goes beyond the fixed structures of Naive and Advanced RAG, characterized by a simple “Retrieve” and “Read” mechanism. Moreover, Modular RAG expands this flexibility by integrating new modules or adjusting interaction flow among existing ones, enhancing its applicability across different tasks.\n",
       "\n",
       "Innovations such as the Rewrite-Retrieve-Read[7]model leverage the LLM’s capabilities to refine retrieval queries through a rewriting module and a LM-feedback mechanism to update rewriting model., improving task performance. Similarly, approaches like Generate-Read[13]replace traditional retrieval with LLM-generated content, while Recite-Read[22]emphasizes retrieval from model weights, enhancing the model’s ability to handle knowledge-intensive tasks. Hybrid retrieval strategies integrate keyword, semantic, and vector searches to cater to diverse queries. Additionally, employing sub-queries and hypothetical document embeddings (HyDE)[11]seeks to improve retrieval relevance by focusing on embedding similarities between generated answers and real documents.\n",
       "\n",
       "Adjustments in module arrangement and interaction, such as the Demonstrate-Search-Predict (DSP)[23]framework and the iterative Retrieve-Read-Retrieve-Read flow of ITER-RETGEN[14], showcase the dynamic use of module outputs to bolster another module’s functionality, illustrating a sophisticated understanding of enhancing module synergy. The flexible orchestration of Modular RAG Flow showcases the benefits of adaptive retrieval through techniques such as FLARE[24]and Self-RAG[25]. This approach transcends the fixed RAG retrieval process by evaluating the necessity of retrieval based on different scenarios. Another benefit of a flexible architecture is that the RAG system can more easily integrate with other technologies (such as fine-tuning or reinforcement learning)[26]. For example, this can involve fine-tuning the retriever for better retrieval results, fine-tuning the generator for more personalized outputs, or engaging in collaborative fine-tuning[27].\n",
       "\n",
       "\n",
       "\n",
       "II-DRAG vs Fine-tuning\n",
       "\n",
       "The augmentation of LLMs has attracted considerable attention due to their growing prevalence. Among the optimization methods for LLMs, RAG is often compared with Fine-tuning (FT) and prompt engineering. Each method has distinct characteristics as illustrated in Figure4. We used a quadrant chart to illustrate the differences among three methods in two dimensions: external knowledge requirements and model adaption requirements. Prompt engineering leverages a model’s inherent capabilities with minimum necessity for external knowledge and model adaption. RAG can be likened to providing a model with a tailored textbook for information retrieval, ideal for precise information retrieval tasks. In contrast, FT is comparable to a student internalizing knowledge over time, suitable for scenarios requiring replication of specific structures, styles, or formats.\n",
       "\n",
       "RAG excels in dynamic environments by offering real-time knowledge updates and effective utilization of external knowledge sources with high interpretability. However, it comes with higher latency and ethical considerations regarding data retrieval. On the other hand, FT is more static, requiring retraining for updates but enabling deep customization of the model’s behavior and style. It demands significant computational resources for dataset preparation and training, and while it can reduce hallucinations, it may face challenges with unfamiliar data.\n",
       "\n",
       "In multiple evaluations of their performance on various knowledge-intensive tasks across different topics,[28]revealed that while unsupervised fine-tuning shows some improvement, RAG consistently outperforms it, for both existing knowledge encountered during training and entirely new knowledge. Additionally, it was found that LLMs struggle to learn new factual information through unsupervised fine-tuning. The choice between RAG and FT depends on the specific needs for data dynamics, customization, and computational capabilities in the application context. RAG and FT are not mutually exclusive and can complement each other, enhancing a model’s capabilities at different levels. In some instances, their combined use may lead to optimal performance. The optimization process involving RAG and FT may require multiple iterations to achieve satisfactory results.\n",
       "\n",
       "IIIRetrieval\n",
       "\n",
       "In the context of RAG, it is crucial to efficiently retrieve relevant documents from the data source. There are several key issues involved, such as the retrieval source, retrieval granularity, pre-processing of the retrieval, and selection of the corresponding embedding model.\n",
       "\n",
       "\n",
       "\n",
       "III-ARetrieval Source\n",
       "\n",
       "RAG relies on external knowledge to enhance LLMs, while the type of retrieval source and the granularity of retrieval units both affect the final generation results.\n",
       "\n",
       "Initially, text is s the mainstream source of retrieval. Subsequently, the retrieval source expanded to include semi-structured data (PDF) and structured data (Knowledge Graph, KG) for enhancement. In addition to retrieving from original external sources, there is also a growing trend in recent researches towards utilizing content generated by LLMs themselves for retrieval and enhancement purposes.\n",
       "\n",
       "Unstructured Data, such as text, is the most widely used retrieval source, which are mainly gathered from corpus. For open-domain question-answering (ODQA) tasks, the primary retrieval sources are Wikipedia Dump with the current major versions including HotpotQA444https://hotpotqa.github.io/wiki-readme.html(1st October , 2017), DPR555https://github.com/facebookresearch/DPR(20 December, 2018). In addition to encyclopedic data, common unstructured data includes cross-lingual text[19]and domain-specific data (such as medical[67]and legal domains[29]).\n",
       "\n",
       "Semi-structured data. typically refers to data that contains a combination of text and table information, such as PDF. Handling semi-structured data poses challenges for conventional RAG systems due to two main reasons. Firstly, text splitting processes may inadvertently separate tables, leading to data corruption during retrieval. Secondly, incorporating tables into the data can complicate semantic similarity searches. When dealing with semi-structured data, one approach involves leveraging the code capabilities of LLMs to execute Text-2-SQL queries on tables within databases, such as TableGPT[85]. Alternatively, tables can be transformed into text format for further analysis using text-based methods[75]. However, both of these methods are not optimal solutions, indicating substantial research opportunities in this area.\n",
       "\n",
       "Structured data, such as knowledge graphs (KGs)[86], which are typically verified and can provide more precise information. KnowledGPT[15]generates KB search queries and stores knowledge in a personalized base, enhancing the RAG model’s knowledge richness. In response to the limitations of LLMs in understanding and answering questions about textual graphs, G-Retriever[84]integrates Graph Neural Networks (GNNs), LLMs and RAG, enhancing graph comprehension and question-answering capabilities through soft prompting of the LLM, and employs the Prize-Collecting Steiner Tree (PCST) optimization problem for targeted graph retrieval. On the contrary, it requires additional effort to build, validate, and maintain structured databases. On the contrary, it requires additional effort to build, validate, and maintain structured databases.\n",
       "\n",
       "LLMs-Generated Content.Addressing the limitations of external auxiliary information in RAG, some research has focused on exploiting LLMs’ internal knowledge. SKR[58]classifies questions as known or unknown, applying retrieval enhancement selectively. GenRead[13]replaces the retriever with an LLM generator, finding that LLM-generated contexts often contain more accurate answers due to better alignment with the pre-training objectives of causal language modeling. Selfmem[17]iteratively creates an unbounded memory pool with a retrieval-enhanced generator, using a memory selector to choose outputs that serve as dual problems to the original question, thus self-enhancing the generative model. These methodologies underscore the breadth of innovative data source utilization in RAG, striving to improve model performance and task effectiveness.\n",
       "\n",
       "Another important factor besides the data format of the retrieval source is the granularity of the retrieved data. Coarse-grained retrieval units theoretically can provide more relevant information for the problem, but they may also contain redundant content, which could distract the retriever and language models in downstream tasks[87,50]. On the other hand, fine-grained retrieval unit granularity increases the burden of retrieval and does not guarantee semantic integrity and meeting the required knowledge. Choosing the appropriate retrieval granularity during inference can be a simple and effective strategy to improve the retrieval and downstream task performance of dense retrievers.\n",
       "\n",
       "In text, retrieval granularity ranges from fine to coarse, including Token, Phrase, Sentence, Proposition, Chunks, Document. Among them, DenseX[30]proposed the concept of using propositions as retrieval units. Propositions are defined as atomic expressions in the text, each encapsulating a unique factual segment and presented in a concise, self-contained natural language format. This approach aims to enhance retrieval precision and relevance. On the Knowledge Graph (KG), retrieval granularity includes Entity, Triplet, and sub-Graph. The granularity of retrieval can also be adapted to downstream tasks, such as retrieving Item IDs[40]in recommendation tasks and Sentence pairs[38]. Detailed information is illustrated in TableI.\n",
       "\n",
       "\n",
       "\n",
       "III-BIndexing Optimization\n",
       "\n",
       "In the Indexing phase, documents will be processed, segmented, and transformed into Embeddings to be stored in a vector database. The quality of index construction determines whether the correct context can be obtained in the retrieval phase.\n",
       "\n",
       "The most common method is to split the document into chunks on a fixed number of tokens (e.g., 100, 256, 512)[88]. Larger chunks can capture more context, but they also generate more noise, requiring longer processing time and higher costs. While smaller chunks may not fully convey the necessary context, they do have less noise. However, chunks leads to truncation within sentences, prompting the optimization of a recursive splits and sliding window methods, enabling layered retrieval by merging globally related information across multiple retrieval processes[89]. Nevertheless, these approaches still cannot strike a balance between semantic completeness and context length. Therefore, methods like Small2Big have been proposed, where sentences (small) are used as the retrieval unit, and the preceding and following sentences are provided as (big) context to LLMs[90].\n",
       "\n",
       "Chunks can be enriched with metadata information such as page number, file name, author,category timestamp. Subsequently, retrieval can be filtered based on this metadata, limiting the scope of the retrieval. Assigning different weights to document timestamps during retrieval can achieve time-aware RAG, ensuring the freshness of knowledge and avoiding outdated information.\n",
       "\n",
       "In addition to extracting metadata from the original documents, metadata can also be artificially constructed. For example, adding summaries of paragraph, as well as introducing hypothetical questions. This method is also known as Reverse HyDE. Specifically, using LLM to generate questions that can be answered by the document, then calculating the similarity between the original question and the hypothetical question during retrieval to reduce the semantic gap between the question and the answer.\n",
       "\n",
       "One effective method for enhancing information retrieval is to establish a hierarchical structure for the documents. By constructing In structure, RAG system can expedite the retrieval and processing of pertinent data.\n",
       "\n",
       "Hierarchical index structure. File are arranged in parent-child relationships, with chunks linked to them. Data summaries are stored at each node, aiding in the swift traversal of data and assisting the RAG system in determining which chunks to extract. This approach can also mitigate the illusion caused by block extraction issues.\n",
       "\n",
       "Knowledge Graph index. Utilize KG in constructing the hierarchical structure of documents contributes to maintaining consistency. It delineates the connections between different concepts and entities, markedly reducing the potential for illusions. Another advantage is the transformation of the information retrieval process into instructions that LLM can comprehend, thereby enhancing the accuracy of knowledge retrieval and enabling LLM to generate contextually coherent responses, thus improving the overall efficiency of the RAG system. To capture the logical relationship between document content and structure, KGP[91]proposed a method of building an index between multiple documents using KG. This KG consists of nodes (representing paragraphs or structures in the documents, such as pages and tables) and edges (indicating semantic/lexical similarity between paragraphs or relationships within the document structure), effectively addressing knowledge retrieval and reasoning problems in a multi-document environment.\n",
       "\n",
       "\n",
       "\n",
       "III-CQuery Optimization\n",
       "\n",
       "One of the primary challenges with Naive RAG is its direct reliance on the user’s original query as the basis for retrieval. Formulating a precise and clear question is difficult, and imprudent queries result in subpar retrieval effectiveness. Sometimes, the question itself is complex, and the language is not well-organized. Another difficulty lies in language complexity ambiguity. Language models often struggle when dealing with specialized vocabulary or ambiguous abbreviations with multiple meanings. For instance, they may not discern whether “LLM” refers tolarge language modelor aMaster of Lawsin a legal context.\n",
       "\n",
       "Expanding a single query into multiple queries enriches the content of the query, providing further context to address any lack of specific nuances, thereby ensuring the optimal relevance of the generated answers.\n",
       "\n",
       "Multi-Query. By employing prompt engineering to expand queries via LLMs, these queries can then be executed in parallel. The expansion of queries is not random, but rather meticulously designed.\n",
       "\n",
       "Sub-Query. The process of sub-question planning represents the generation of the necessary sub-questions to contextualize and fully answer the original question when combined. This process of adding relevant context is, in principle, similar to query expansion. Specifically, a complex question can be decomposed into a series of simpler sub-questions using the least-to-most prompting method[92].\n",
       "\n",
       "Chain-of-Verification(CoVe). The expanded queries undergo validation by LLM to achieve the effect of reducing hallucinations. Validated expanded queries typically exhibit higher reliability[93].\n",
       "\n",
       "The core concept is to retrieve chunks based on a transformed query instead of the user’s original query.\n",
       "\n",
       "Query Rewrite.The original queries are not always optimal for LLM retrieval, especially in real-world scenarios. Therefore, we can prompt LLM to rewrite the queries. In addition to using LLM for query rewriting, specialized smaller language models, such as RRR (Rewrite-retrieve-read)[7]. The implementation of the query rewrite method in the Taobao, known as BEQUE[9]has notably enhanced recall effectiveness for long-tail queries, resulting in a rise in GMV.\n",
       "\n",
       "Another query transformation method is to use prompt engineering to let LLM generate a query based on the original query for subsequent retrieval. HyDE[11]construct hypothetical documents (assumed answers to the original query). It focuses on embedding similarity from answer to answer rather than seeking embedding similarity for the problem or query. Using the Step-back Prompting method[10], the original query is abstracted to generate a high-level concept question (step-back question). In the RAG system, both the step-back question and the original query are used for retrieval, and both the results are utilized as the basis for language model answer generation.\n",
       "\n",
       "Based on varying queries, routing to distinct RAG pipeline,which is suitable for a versatile RAG system designed to accommodate diverse scenarios.\n",
       "\n",
       "Metadata Router/ Filter. The first step involves extracting keywords (entity) from the query, followed by filtering based on the keywords and metadata within the chunks to narrow down the search scope.\n",
       "\n",
       "Semantic Routeris another method of routing involves leveraging the semantic information of the query. Specific apprach see Semantic Router666https://github.com/aurelio-labs/semantic-router. Certainly, a hybrid routing approach can also be employed, combining both semantic and metadata-based methods for enhanced query routing.\n",
       "\n",
       "\n",
       "\n",
       "III-DEmbedding\n",
       "\n",
       "In RAG, retrieval is achieved by calculating the similarity (e.g. cosine similarity) between the embeddings of the question and document chunks, where the semantic representation capability of embedding models plays a key role. This mainly includes a sparse encoder (BM25) and a dense retriever (BERT architecture Pre-training language models). Recent research has introduced prominent embedding models such as AngIE, Voyage, BGE,etc[94,95,96], which are benefit from multi-task instruct tuning. Hugging Face’s MTEB leaderboard777https://huggingface.co/spaces/mteb/leaderboardevaluates embedding models across 8 tasks, covering 58 datasests. Additionally, C-MTEB focuses on Chinese capability, covering 6 tasks and 35 datasets. There is no one-size-fits-all answer to “which embedding model to use.” However, some specific models are better suited for particular use cases.\n",
       "\n",
       "Sparse and dense embedding approaches capture different relevance features and can benefit from each other by leveraging complementary relevance information. For instance, sparse retrieval models can be used to provide initial search results for training dense retrieval models. Additionally, pre-training language models (PLMs) can be utilized to learn term weights to enhance sparse retrieval. Specifically, it also demonstrates that sparse retrieval models can enhance the zero-shot retrieval capability of dense retrieval models and assist dense retrievers in handling queries containing rare entities, thereby improving robustness.\n",
       "\n",
       "In instances where the context significantly deviates from pre-training corpus, particularly within highly specialized disciplines such as healthcare, legal practice, and other sectors replete with proprietary jargon, fine-tuning the embedding model on your own domain dataset becomes essential to mitigate such discrepancies.\n",
       "\n",
       "In addition to supplementing domain knowledge, another purpose of fine-tuning is to align the retriever and generator, for example, using the results of LLM as the supervision signal for fine-tuning, known as LSR (LM-supervised Retriever). PROMPTAGATOR[21]utilizes the LLM as a few-shot query generator to create task-specific retrievers, addressing challenges in supervised fine-tuning, particularly in data-scarce domains. Another approach, LLM-Embedder[97], exploits LLMs to generate reward signals across multiple downstream tasks. The retriever is fine-tuned with two types of supervised signals: hard labels for the dataset and soft rewards from the LLMs. This dual-signal approach fosters a more effective fine-tuning process, tailoring the embedding model to diverse downstream applications. REPLUG[72]utilizes a retriever and an LLM to calculate the probability distributions of the retrieved documents and then performs supervised training by computing the KL divergence. This straightforward and effective training method enhances the performance of the retrieval model by using an LM as the supervisory signal, eliminating the need for specific cross-attention mechanisms. Moreover, inspired by RLHF (Reinforcement Learning from Human Feedback), utilizing LM-based feedback to reinforce the retriever through reinforcement learning.\n",
       "\n",
       "\n",
       "\n",
       "III-EAdapter\n",
       "\n",
       "Fine-tuning models may present challenges, such as integrating functionality through an API or addressing constraints arising from limited local computational resources. Consequently, some approaches opt to incorporate an external adapter to aid in alignment.\n",
       "\n",
       "To optimize the multi-task capabilities of LLM, UPRISE[20]trained a lightweight prompt retriever that can automatically retrieve prompts from a pre-built prompt pool that are suitable for a given zero-shot task input. AAR (Augmentation-Adapted Retriver)[47]introduces a universal adapter designed to accommodate multiple downstream tasks. While PRCA[69]add a pluggable reward-driven contextual adapter to enhance performance on specific tasks. BGM[26]keeps the retriever and LLM fixed,and trains a bridge Seq2Seq model in between. The bridge model aims to transform the retrieved information into a format that LLMs can work with effectively, allowing it to not only rerank but also dynamically select passages for each query, and potentially employ more advanced strategies like repetition. Furthermore, PKG introduces an innovative method for integrating knowledge into white-box models via directive fine-tuning[75]. In this approach, the retriever module is directly substituted to generate relevant documents according to a query. This method assists in addressing the difficulties encountered during the fine-tuning process and enhances model performance.\n",
       "\n",
       "IVGeneration\n",
       "\n",
       "After retrieval, it is not a good practice to directly input all the retrieved information to the LLM for answering questions. Following will introduce adjustments from two perspectives: adjusting the retrieved content and adjusting the LLM.\n",
       "\n",
       "\n",
       "\n",
       "IV-AContext Curation\n",
       "\n",
       "Redundant information can interfere with the final generation of LLM, and overly long contexts can also lead LLM to the “Lost in the middle” problem[98]. Like humans, LLM tends to only focus on the beginning and end of long texts, while forgetting the middle portion. Therefore, in the RAG system, we typically need to further process the retrieved content.\n",
       "\n",
       "Reranking fundamentally reorders document chunks to highlight the most pertinent results first, effectively reducing the overall document pool, severing a dual purpose in information retrieval, acting as both an enhancer and a filter, delivering refined inputs for more precise language model processing[70]. Reranking can be performed using rule-based methods that depend on predefined metrics like Diversity, Relevance, and MRR, or model-based approaches like Encoder-Decoder models from the BERT series (e.g., SpanBERT), specialized reranking models such as Cohere rerank or bge-raranker-large, and general large language models like GPT[12,99].\n",
       "\n",
       "A common misconception in the RAG process is the belief that retrieving as many relevant documents as possible and concatenating them to form a lengthy retrieval prompt is beneficial. However, excessive context can introduce more noise, diminishing the LLM’s perception of key information .\n",
       "\n",
       "(Long) LLMLingua[100,101]utilize small language models (SLMs) such as GPT-2 Small or LLaMA-7B, to detect and remove unimportant tokens, transforming it into a form that is challenging for humans to comprehend but well understood by LLMs. This approach presents a direct and practical method for prompt compression, eliminating the need for additional training of LLMs while balancing language integrity and compression ratio. PRCA tackled this issue by training an information extractor[69]. Similarly, RECOMP adopts a comparable approach by training an information condenser using contrastive learning[71]. Each training data point consists of one positive sample and five negative samples, and the encoder undergoes training using contrastive loss throughout this process[102].\n",
       "\n",
       "In addition to compressing the context, reducing the number of documents aslo helps improve the accuracy of the model’s answers. Ma et al.[103]propose the “Filter-Reranker” paradigm, which combines the strengths of LLMs and SLMs. In this paradigm, SLMs serve as filters, while LLMs function as reordering agents. The research shows that instructing LLMs to rearrange challenging samples identified by SLMs leads to significant improvements in various Information Extraction (IE) tasks. Another straightforward and effective approach involves having the LLM evaluate the retrieved content before generating the final answer. This allows the LLM to filter out documents with poor relevance through LLM critique. For instance, in Chatlaw[104], the LLM is prompted to self-suggestion on the referenced legal provisions to assess their relevance.\n",
       "\n",
       "\n",
       "\n",
       "IV-BLLM Fine-tuning\n",
       "\n",
       "Targeted fine-tuning based on the scenario and data characteristics on LLMs can yield better results. This is also one of the greatest advantages of using on-premise LLMs. When LLMs lack data in a specific domain, additional knowledge can be provided to the LLM through fine-tuning. Huggingface’s fine-tuning data can also be used as an initial step.\n",
       "\n",
       "Another benefit of fine-tuning is the ability to adjust the model’s input and output. For example, it can enable LLM to adapt to specific data formats and generate responses in a particular style as instructed[37]. For retrieval tasks that engage with structured data, the SANTA framework[76]implements a tripartite training regimen to effectively encapsulate both structural and semantic nuances. The initial phase focuses on the retriever, where contrastive learning is harnessed to refine the query and document embeddings.\n",
       "\n",
       "Aligning LLM outputs with human or retriever preferences through reinforcement learning is a potential approach. For instance, manually annotating the final generated answers and then providing feedback through reinforcement learning. In addition to aligning with human preferences, it is also possible to align with the preferences of fine-tuned models and retrievers[79]. When circumstances prevent access to powerful proprietary models or larger parameter open-source models, a simple and effective method is to distill the more powerful models(e.g. GPT-4). Fine-tuning of LLM can also be coordinated with fine-tuning of the retriever to align preferences. A typical approach, such as RA-DIT[27], aligns the scoring functions between Retriever and Generator using KL divergence.\n",
       "\n",
       "VAugmentation process in RAG\n",
       "\n",
       "In the domain of RAG, the standard practice often involves a singular (once) retrieval step followed by generation, which can lead to inefficiencies and sometimes is typically insufficient for complex problems demanding multi-step reasoning, as it provides a limited scope of information[105]. Many studies have optimized the retrieval process in response to this issue, and we have summarised them in Figure5.\n",
       "\n",
       "\n",
       "\n",
       "V-AIterative Retrieval\n",
       "\n",
       "Iterative retrieval is a process where the knowledge base is repeatedly searched based on the initial query and the text generated so far, providing a more comprehensive knowledge base for LLMs. This approach has been shown to enhance the robustness of subsequent answer generation by offering additional contextual references through multiple retrieval iterations. However, it may be affected by semantic discontinuity and the accumulation of irrelevant information. ITER-RETGEN[14]employs a synergistic approach that leverages “retrieval-enhanced generation” alongside “generation-enhanced retrieval” for tasks that necessitate the reproduction of specific information. The model harnesses the content required to address the input task as a contextual basis for retrieving pertinent knowledge, which in turn facilitates the generation of improved responses in subsequent iterations.\n",
       "\n",
       "\n",
       "\n",
       "V-BRecursive Retrieval\n",
       "\n",
       "Recursive retrieval is often used in information retrieval and NLP to improve the depth and relevance of search results. The process involves iteratively refining search queries based on the results obtained from previous searches. Recursive Retrieval aims to enhance the search experience by gradually converging on the most pertinent information through a feedback loop. IRCoT[61]uses chain-of-thought to guide the retrieval process and refines the CoT with the obtained retrieval results. ToC[57]creates a clarification tree that systematically optimizes the ambiguous parts in the Query. It can be particularly useful in complex search scenarios where the user’s needs are not entirely clear from the outset or where the information sought is highly specialized or nuanced. The recursive nature of the process allows for continuous learning and adaptation to the user’s requirements, often resulting in improved satisfaction with the search outcomes.\n",
       "\n",
       "To address specific data scenarios, recursive retrieval and multi-hop retrieval techniques are utilized together. Recursive retrieval involves a structured index to process and retrieve data in a hierarchical manner, which may include summarizing sections of a document or lengthy PDF before performing a retrieval based on this summary. Subsequently, a secondary retrieval within the document refines the search, embodying the recursive nature of the process. In contrast, multi-hop retrieval is designed to delve deeper into graph-structured data sources, extracting interconnected information[106].\n",
       "\n",
       "\n",
       "\n",
       "V-CAdaptive Retrieval\n",
       "\n",
       "Adaptive retrieval methods, exemplified by Flare[24]and Self-RAG[25], refine the RAG framework by enabling LLMs to actively determine the optimal moments and content for retrieval, thus enhancing the efficiency and relevance of the information sourced.\n",
       "\n",
       "These methods are part of a broader trend wherein LLMs employ active judgment in their operations, as seen in model agents like AutoGPT, Toolformer, and Graph-Toolformer[107,108,109]. Graph-Toolformer, for instance, divides its retrieval process into distinct steps where LLMs proactively use retrievers, apply Self-Ask techniques, and employ few-shot prompts to initiate search queries. This proactive stance allows LLMs to decide when to search for necessary information, akin to how an agent utilizes tools.\n",
       "\n",
       "WebGPT[110]integrates a reinforcement learning framework to train the GPT-3 model in autonomously using a search engine during text generation. It navigates this process using special tokens that facilitate actions such as search engine queries, browsing results, and citing references, thereby expanding GPT-3’s capabilities through the use of external search engines. Flare automates timing retrieval by monitoring the confidence of the generation process, as indicated by the probability of generated terms[24]. When the probability falls below a certain threshold would activates the retrieval system to collect relevant information, thus optimizing the retrieval cycle. Self-RAG[25]introduces “reflection tokens” that allow the model to introspect its outputs. These tokens come in two varieties: “retrieve” and “critic”. The model autonomously decides when to activate retrieval, or alternatively, a predefined threshold may trigger the process. During retrieval, the generator conducts a fragment-level beam search across multiple paragraphs to derive the most coherent sequence. Critic scores are used to update the subdivision scores, with the flexibility to adjust these weights during inference, tailoring the model’s behavior. Self-RAG’s design obviates the need for additional classifiers or reliance on Natural Language Inference (NLI) models, thus streamlining the decision-making process for when to engage retrieval mechanisms and improving the model’s autonomous judgment capabilities in generating accurate responses.\n",
       "\n",
       "VITask and Evaluation\n",
       "\n",
       "The rapid advancement and growing adoption of RAG in the field of NLP have propelled the evaluation of RAG models to the forefront of research in the LLMs community. The primary objective of this evaluation is to comprehend and optimize the performance of RAG models across diverse application scenarios.This chapter will mainly introduce the main downstream tasks of RAG, datasets, and how to evaluate RAG systems.\n",
       "\n",
       "\n",
       "\n",
       "VI-ADownstream Task\n",
       "\n",
       "The core task of RAG remains Question Answering (QA), including traditional single-hop/multi-hop QA, multiple-choice, domain-specific QA as well as long-form scenarios suitable for RAG. In addition to QA, RAG is continuously being expanded into multiple downstream tasks, such as Information Extraction (IE), dialogue generation, code search, etc. The main downstream tasks of RAG and their corresponding datasets are summarized in TableII.\n",
       "\n",
       "\n",
       "\n",
       "VI-BEvaluation Target\n",
       "\n",
       "Historically, RAG models assessments have centered on their execution in specific downstream tasks. These evaluations employ established metrics suitable to the tasks at hand. For instance, question answering evaluations might rely on EM and F1 scores[45,72,59,7], whereas fact-checking tasks often hinge on Accuracy as the primary metric[4,42,14]. BLEU and ROUGE metrics are also commonly used to evaluate answer quality[26,78,52,32]. Tools like RALLE, designed for the automatic evaluation of RAG applications, similarly base their assessments on these task-specific metrics[160]. Despite this, there is a notable paucity of research dedicated to evaluating the distinct characteristics of RAG models.The main evaluation objectives include:\n",
       "\n",
       "Retrieval Quality. Evaluating the retrieval quality is crucial for determining the effectiveness of the context sourced by the retriever component. Standard metrics from the domains of search engines, recommendation systems, and information retrieval systems are employed to measure the performance of the RAG retrieval module. Metrics such as Hit Rate, MRR, and NDCG are commonly utilized for this purpose[161,162].\n",
       "\n",
       "Generation Quality. The assessment of generation quality centers on the generator’s capacity to synthesize coherent and relevant answers from the retrieved context. This evaluation can be categorized based on the content’s objectives: unlabeled and labeled content. For unlabeled content, the evaluation encompasses the faithfulness, relevance, and non-harmfulness of the generated answers. In contrast, for labeled content, the focus is on the accuracy of the information produced by the model[161]. Additionally, both retrieval and generation quality assessments can be conducted through manual or automatic evaluation methods[161,29,163].\n",
       "\n",
       "\n",
       "\n",
       "VI-CEvaluation Aspects\n",
       "\n",
       "Contemporary evaluation practices of RAG models emphasize three primary quality scores and four essential abilities, which collectively inform the evaluation of the two principal targets of the RAG model: retrieval and generation.\n",
       "\n",
       "Quality scores include context relevance, answer faithfulness, and answer relevance. These quality scores evaluate the efficiency of the RAG model from different perspectives in the process of information retrieval and generation[164,165,166].\n",
       "\n",
       "Context Relevanceevaluates the precision and specificity of the retrieved context, ensuring relevance and minimizing processing costs associated with extraneous content.\n",
       "\n",
       "Answer Faithfulnessensures that the generated answers remain true to the retrieved context, maintaining consistency and avoiding contradictions.\n",
       "\n",
       "Answer Relevancerequires that the generated answers are directly pertinent to the posed questions, effectively addressing the core inquiry.\n",
       "\n",
       "RAG evaluation also encompasses four abilities indicative of its adaptability and efficiency: noise robustness, negative rejection, information integration, and counterfactual robustness[167,168]. These abilities are critical for the model’s performance under various challenges and complex scenarios, impacting the quality scores.\n",
       "\n",
       "Noise Robustnessappraises the model’s capability to manage noise documents that are question-related but lack substantive information.\n",
       "\n",
       "Negative Rejectionassesses the model’s discernment in refraining from responding when the retrieved documents do not contain the necessary knowledge to answer a question.\n",
       "\n",
       "Information Integrationevaluates the model’s proficiency in synthesizing information from multiple documents to address complex questions.\n",
       "\n",
       "Counterfactual Robustnesstests the model’s ability to recognize and disregard known inaccuracies within documents, even when instructed about potential misinformation.\n",
       "\n",
       "Context relevance and noise robustness are important for evaluating the quality of retrieval, while answer faithfulness, answer relevance, negative rejection, information integration, and counterfactual robustness are important for evaluating the quality of generation.\n",
       "\n",
       "The specific metrics for each evaluation aspect are summarized in TableIII. It is essential to recognize that these metrics, derived from related work, are traditional measures and do not yet represent a mature or standardized approach for quantifying RAG evaluation aspects. Custom metrics tailored to the nuances of RAG models, though not included here, have also been developed in some evaluation studies.\n",
       "\n",
       "\n",
       "\n",
       "ContextRelevance\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Faithfulness\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "AnswerRelevance\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "NoiseRobustness\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "NegativeRejection\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "InformationIntegration\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "CounterfactualRobustness\n",
       "\n",
       "\n",
       "\n",
       "Accuracy\n",
       "\n",
       "✓\n",
       "\n",
       "✓\n",
       "\n",
       "✓\n",
       "\n",
       "✓\n",
       "\n",
       "✓\n",
       "\n",
       "✓\n",
       "\n",
       "✓\n",
       "\n",
       "EM\n",
       "\n",
       "✓\n",
       "\n",
       "Recall\n",
       "\n",
       "✓\n",
       "\n",
       "Precision\n",
       "\n",
       "✓\n",
       "\n",
       "✓\n",
       "\n",
       "R-Rate\n",
       "\n",
       "✓\n",
       "\n",
       "Cosine Similarity\n",
       "\n",
       "✓\n",
       "\n",
       "Hit Rate\n",
       "\n",
       "✓\n",
       "\n",
       "MRR\n",
       "\n",
       "✓\n",
       "\n",
       "NDCG\n",
       "\n",
       "✓\n",
       "\n",
       "BLEU\n",
       "\n",
       "✓\n",
       "\n",
       "✓\n",
       "\n",
       "✓\n",
       "\n",
       "ROUGE/ROUGE-L\n",
       "\n",
       "✓\n",
       "\n",
       "✓\n",
       "\n",
       "✓\n",
       "\n",
       "\n",
       "\n",
       "VI-DEvaluation Benchmarks and Tools\n",
       "\n",
       "A series of benchmark tests and tools have been proposed to facilitate the evaluation of RAG.These instruments furnish quantitative metrics that not only gauge RAG model performance but also enhance comprehension of the model’s capabilities across various evaluation aspects. Prominent benchmarks such as RGB, RECALL and CRUD[167,168,169]focus on appraising the essential abilities of RAG models. Concurrently, state-of-the-art automated tools like RAGAS[164], ARES[165], and TruLens888https://www.trulens.org/trulens_eval/core_concepts_rag_triad/employ LLMs to adjudicate the quality scores. These tools and benchmarks collectively form a robust framework for the systematic evaluation of RAG models, as summarized in TableIV.\n",
       "\n",
       "\n",
       "\n",
       "Evaluation Framework\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Evaluation Targets\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Evaluation Aspects\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Quantitative Metrics\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "RGB††{}^{\\dagger}start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Retrieval QualityGeneration Quality\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Noise RobustnessNegative RejectionInformation IntegrationCounterfactual Robustness\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "AccuracyEMAccuracyAccuracy\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "RECALL††{}^{\\dagger}start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT\n",
       "\n",
       "\n",
       "\n",
       "Generation Quality\n",
       "\n",
       "Counterfactual Robustness\n",
       "\n",
       "R-Rate (Reappearance Rate)\n",
       "\n",
       "\n",
       "\n",
       "RAGAS‡‡{}^{\\ddagger}start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Retrieval QualityGeneration Quality\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Context RelevanceFaithfulnessAnswer Relevance\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "*Cosine Similarity\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "ARES‡‡{}^{\\ddagger}start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Retrieval QualityGeneration Quality\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Context RelevanceFaithfulnessAnswer Relevance\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "AccuracyAccuracyAccuracy\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "TruLens‡‡{}^{\\ddagger}start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Retrieval QualityGeneration Quality\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Context RelevanceFaithfulnessAnswer Relevance\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "*\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "CRUD††{}^{\\dagger}start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Retrieval QualityGeneration Quality\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Creative GenerationKnowledge-intensive QAError CorrectionSummarization\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "BLEUROUGE-LBertScoreRAGQuestEval\n",
       "\n",
       "\n",
       "\n",
       "† represents a benchmark, and ‡ represents a tool. * denotes customized quantitative metrics, which deviate from traditional metrics. Readers are encouraged to consult pertinent literature for the specific quantification formulas associated with these metrics, as required.\n",
       "\n",
       "VIIDiscussion and Future Prospects\n",
       "\n",
       "Despite the considerable progress in RAG technology, several challenges persist that warrant in-depth research.This chapter will mainly introduce the current challenges and future research directions faced by RAG.\n",
       "\n",
       "\n",
       "\n",
       "VII-ARAG vs Long Context\n",
       "\n",
       "With the deepening of related research, the context of LLMs is continuously expanding[170,171,172]. Presently, LLMs can effortlessly manage contexts exceeding 200,000 tokens999https://kimi.moonshot.cn. This capability signifies that long-document question answering, previously reliant on RAG, can now incorporate the entire document directly into the prompt. This has also sparked discussions on whether RAG is still necessary when LLMs are not constrained by context. In fact, RAG still plays an irreplaceable role. On one hand, providing LLMs with a large amount of context at once will significantly impact its inference speed, while chunked retrieval and on-demand input can significantly improve operational efficiency. On the other hand, RAG-based generation can quickly locate the original references for LLMs to help users verify the generated answers. The entire retrieval and reasoning process is observable, while generation solely relying on long context remains a black box. Conversely, the expansion of context provides new opportunities for the development of RAG, enabling it to address more complex problems and integrative or summary questions that require reading a large amount of material to answer[49]. Developing new RAG methods in the context of super-long contexts is one of the future research trends.\n",
       "\n",
       "\n",
       "\n",
       "VII-BRAG Robustness\n",
       "\n",
       "The presence of noise or contradictory information during retrieval can detrimentally affect RAG’s output quality. This situation is figuratively referred to as “Misinformation can be worse than no information at all”. Improving RAG’s resistance to such adversarial or counterfactual inputs is gaining research momentum and has become a key performance metric[50,48,82]. Cuconasu et al.[54]analyze which type of documents should be retrieved, evaluate the relevance of the documents to the prompt, their position, and the number included in the context. The research findings reveal that including irrelevant documents can unexpectedly increase accuracy by over 30%, contradicting the initial assumption of reduced quality. These results underscore the importance of developing specialized strategies to integrate retrieval with language generation models, highlighting the need for further research and exploration into the robustness of RAG.\n",
       "\n",
       "\n",
       "\n",
       "VII-CHybrid Approaches\n",
       "\n",
       "Combining RAG with fine-tuning is emerging as a leading strategy. Determining the optimal integration of RAG and fine-tuning whether sequential, alternating, or through end-to-end joint training—and how to harness both parameterized and non-parameterized advantages are areas ripe for exploration[27]. Another trend is to introduce SLMs with specific functionalities into RAG and fine-tuned by the results of RAG system. For example, CRAG[67]trains a lightweight retrieval evaluator to assess the overall quality of the retrieved documents for a query and triggers different knowledge retrieval actions based on confidence levels.\n",
       "\n",
       "\n",
       "\n",
       "VII-DScaling laws of RAG\n",
       "\n",
       "End-to-end RAG models and pre-trained models based on RAG are still one of the focuses of current researchers[173].The parameters of these models are one of the key factors.While scaling laws[174]are established for LLMs, their applicability to RAG remains uncertain. Initial studies like RETRO++[44]have begun to address this, yet the parameter count in RAG models still lags behind that of LLMs. The possibility of an Inverse Scaling Law101010https://github.com/inverse-scaling/prize, where smaller models outperform larger ones, is particularly intriguing and merits further investigation.\n",
       "\n",
       "\n",
       "\n",
       "VII-EProduction-Ready RAG\n",
       "\n",
       "RAG’s practicality and alignment with engineering requirements have facilitated its adoption. However, enhancing retrieval efficiency, improving document recall in large knowledge bases, and ensuring data security—such as preventing inadvertent disclosure of document sources or metadata by LLMs—are critical engineering challenges that remain to be addressed[175].\n",
       "\n",
       "The development of the RAG ecosystem is greatly impacted by the progression of its technical stack. Key tools like LangChain and LLamaIndex have quickly gained popularity with the emergence of ChatGPT, providing extensive RAG-related APIs and becoming essential in the realm of LLMs.The emerging technology stack, while not as rich in features as LangChain and LLamaIndex, stands out through its specialized products. For example, Flowise AI prioritizes a low-code approach, allowing users to deploy AI applications, including RAG, through a user-friendly drag-and-drop interface. Other technologies like HayStack, Meltano, and Cohere Coral are also gaining attention for their unique contributions to the field.\n",
       "\n",
       "In addition to AI-focused vendors, traditional software and cloud service providers are expanding their offerings to include RAG-centric services. Weaviate’s Verba111111https://github.com/weaviate/Verbais designed for personal assistant applications, while Amazon’s Kendra121212https://aws.amazon.com/cn/kendra/offers intelligent enterprise search services, enabling users to browse various content repositories using built-in connectors. In the development of RAG technology, there is a clear trend towards different specialization directions, such as: 1) Customization - tailoring RAG to meet specific requirements. 2) Simplification - making RAG easier to use to reduce the initial learning curve. 3) Specialization - optimizing RAG to better serve production environments.\n",
       "\n",
       "The mutual growth of RAG models and their technology stacks is evident; technological advancements continuously establish new standards for existing infrastructure. In turn, enhancements to the technology stack drive the development of RAG capabilities. RAG toolkits are converging into a foundational technology stack, laying the groundwork for advanced enterprise applications. However, a fully integrated, comprehensive platform concept is still in the future, requiring further innovation and development.\n",
       "\n",
       "\n",
       "\n",
       "VII-FMulti-modal RAG\n",
       "\n",
       "RAG has transcended its initial text-based question-answering confines, embracing a diverse array of modal data. This expansion has spawned innovative multimodal models that integrate RAG concepts across various domains:\n",
       "\n",
       "Image. RA-CM3[176]stands as a pioneering multimodal model of both retrieving and generating text and images. BLIP-2[177]leverages frozen image encoders alongside LLMs for efficient visual language pre-training, enabling zero-shot image-to-text conversions. The “Visualize Before You Write” method[178]employs image generation to steer the LM’s text generation, showing promise in open-ended text generation tasks.\n",
       "\n",
       "Audio and Video. The GSS method retrieves and stitches together audio clips to convert machine-translated data into speech-translated data[179]. UEOP marks a significant advancement in end-to-end automatic speech recognition by incorporating external, offline strategies for voice-to-text conversion[180]. Additionally, KNN-based attention fusion leverages audio embeddings and semantically related text embeddings to refine ASR, thereby accelerating domain adaptation. Vid2Seq augments language models with specialized temporal markers, facilitating the prediction of event boundaries and textual descriptions within a unified output sequence[181].\n",
       "\n",
       "Code. RBPS[182]excels in small-scale learning tasks by retrieving code examples that align with developers’ objectives through encoding and frequency analysis. This approach has demonstrated efficacy in tasks such as test assertion generation and program repair. For structured knowledge, the CoK method[106]first extracts facts pertinent to the input query from a knowledge graph, then integrates these facts as hints within the input, enhancing performance in knowledge graph question-answering tasks.\n",
       "\n",
       "VIIIConclusion\n",
       "\n",
       "The summary of this paper, as depicted in Figure6, emphasizes RAG’s significant advancement in enhancing the capabilities of LLMs by integrating parameterized knowledge from language models with extensive non-parameterized data from external knowledge bases. The survey showcases the evolution of RAG technologies and their application on many different tasks. The analysis outlines three developmental paradigms within the RAG framework: Naive, Advanced, and Modular RAG, each representing a progressive enhancement over its predecessors. RAG’s technical integration with other AI methodologies, such as fine-tuning and reinforcement learning, has further expanded its capabilities. Despite the progress in RAG technology, there are research opportunities to improve its robustness and its ability to handle extended contexts. RAG’s application scope is expanding into multimodal domains, adapting its principles to interpret and process diverse data forms like images, videos, and code. This expansion highlights RAG’s significant practical implications for AI deployment, attracting interest from academic and industrial sectors. The growing ecosystem of RAG is evidenced by the rise in RAG-centric AI applications and the continuous development of supportive tools. As RAG’s application landscape broadens, there is a need to refine evaluation methodologies to keep pace with its evolution. Ensuring accurate and representative performance assessments is crucial for fully capturing RAG’s contributions to the AI research and development community."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessor = ArxivHtmlPaperPreprocessor()\n",
    "cleaned_text = preprocessor.get_text(doc)\n",
    "display_md(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'paper.html'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-Augmented Generation for Large Language Models: A Survey\n"
     ]
    }
   ],
   "source": [
    "def extract_title(html_content):\n",
    "  strainer = SoupStrainer('h1', class_=\"ltx_title ltx_title_document\")\n",
    "  soup = BeautifulSoup(html_content, 'html.parser', parse_only=strainer)\n",
    "  title_text = soup.get_text()\n",
    "  return title_text\n",
    "\n",
    "print(extract_title(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Authors and Affiliations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yunfan Gao: Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n",
      "Yun Xiong: Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n",
      "Xinyu Gao: Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n",
      "Kangxiang Jia: Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n",
      "Jinliu Pan: Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n",
      "Yuxi Bi: College of Design and Innovation, Tongji University\n",
      "Yi Dai: Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n",
      "Jiawei Sun: Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n",
      "Meng Wang: College of Design and Innovation, Tongji University\n",
      "Haofen Wang: Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University College of Design and Innovation, Tongji University\n"
     ]
    }
   ],
   "source": [
    "def extract_authors_and_affiliations(html_content):\n",
    "    strainer = SoupStrainer('div', class_=\"ltx_authors\")\n",
    "    soup = BeautifulSoup(html_content, 'html.parser', parse_only=strainer)\n",
    "\n",
    "    formatted_output = []\n",
    "    for author in soup.find_all('span', class_='ltx_creator ltx_role_author'):\n",
    "        name = author.find('span', class_='ltx_personname').get_text(strip=True)\n",
    "        affiliation = ' '.join(span.get_text(strip=True) for span in author.find_all('span', class_='ltx_contact ltx_role_affiliation'))\n",
    "        formatted_output.append(f\"{name}: {affiliation}\")\n",
    "    output_text = \"\\n\".join(formatted_output)\n",
    "    return output_text\n",
    "\n",
    "print(extract_authors_and_affiliations(html_content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Abstract\n",
       "\n",
       "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs’ intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_abstract(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    abstract_div = soup.find('div', class_='ltx_abstract')\n",
    "    \n",
    "    if abstract_div:\n",
    "        abstract_title = abstract_div.find('h6', class_='ltx_title ltx_title_abstract')\n",
    "        abstract_title_text = abstract_title.get_text(strip=True) if abstract_title else \"Abstract\"\n",
    "        \n",
    "        abstract_paragraph = abstract_div.find('p', class_='ltx_p')\n",
    "        if abstract_paragraph:\n",
    "            for footnote in abstract_paragraph.find_all('span', class_='ltx_note'):\n",
    "                footnote.decompose()\n",
    "            \n",
    "            return f\"{abstract_title_text}\\n\\n{abstract_paragraph.get_text(strip=True)}\"\n",
    "    return \"Abstract not found\"\n",
    "  \n",
    "display_md(extract_abstract(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Section with Subheadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "IIOverview of RAG\n",
       "\n",
       "A typical application of RAG is illustrated in Figure2. Here, a user poses a question to ChatGPT about a recent, widely discussed news. Given ChatGPT’s reliance on pre-training data, it initially lacks the capacity to provide updates on recent developments. RAG bridges this information gap by sourcing and incorporating knowledge from external databases. In this case, it gathers relevant news articles related to the user’s query. These articles, combined with the original question, form a comprehensive prompt that empowers LLMs to generate a well-informed answer.\n",
       "\n",
       "The RAG research paradigm is continuously evolving, and we categorize it into three stages: Naive RAG, Advanced RAG, and Modular RAG, as showed in Figure3. Despite RAG method are cost-effective and surpass the performance of the native LLM, they also exhibit several limitations. The development of Advanced RAG and Modular RAG is a response to these specific shortcomings in Naive RAG.\n",
       "\n",
       "\n",
       "\n",
       "II-ANaive RAG\n",
       "\n",
       "The Naive RAG research paradigm represents the earliest methodology, which gained prominence shortly after the widespread adoption of ChatGPT. The Naive RAG follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a “Retrieve-Read” framework[7].\n",
       "\n",
       "Indexingstarts with the cleaning and extraction of raw data in diverse formats like PDF, HTML, Word, and Markdown, which is then converted into a uniform plain text format. To accommodate the context limitations of language models, text is segmented into smaller, digestible chunks. Chunks are then encoded into vector representations using an embedding model and stored in vector database. This step is crucial for enabling efficient similarity searches in the subsequent retrieval phase.\n",
       "\n",
       "Retrieval. Upon receipt of a user query, the RAG system employs the same encoding model utilized during the indexing phase to transform the query into a vector representation. It then computes the similarity scores between the query vector and the vector of chunks within the indexed corpus. The system prioritizes and retrieves the top K chunks that demonstrate the greatest similarity to the query. These chunks are subsequently used as the expanded context in prompt.\n",
       "\n",
       "Generation. The posed query and selected documents are synthesized into a coherent prompt to which a large language model is tasked with formulating a response. The model’s approach to answering may vary depending on task-specific criteria, allowing it to either draw upon its inherent parametric knowledge or restrict its responses to the information contained within the provided documents. In cases of ongoing dialogues, any existing conversational history can be integrated into the prompt, enabling the model to engage in multi-turn dialogue interactions effectively.\n",
       "\n",
       "However, Naive RAG encounters notable drawbacks:\n",
       "\n",
       "Retrieval Challenges. The retrieval phase often struggles with precision and recall, leading to the selection of misaligned or irrelevant chunks, and the missing of crucial information.\n",
       "\n",
       "Generation Difficulties. In generating responses, the model may face the issue of hallucination, where it produces content not supported by the retrieved context. This phase can also suffer from irrelevance, toxicity, or bias in the outputs, detracting from the quality and reliability of the responses.\n",
       "\n",
       "Augmentation Hurdles. Integrating retrieved information with the different task can be challenging, sometimes resulting in disjointed or incoherent outputs. The process may also encounter redundancy when similar information is retrieved from multiple sources, leading to repetitive responses. Determining the significance and relevance of various passages and ensuring stylistic and tonal consistency add further complexity. Facing complex issues, a single retrieval based on the original query may not suffice to acquire adequate context information.\n",
       "\n",
       "Moreover, there’s a concern that generation models might overly rely on augmented information, leading to outputs that simply echo retrieved content without adding insightful or synthesized information.\n",
       "\n",
       "\n",
       "\n",
       "II-BAdvanced RAG\n",
       "\n",
       "Advanced RAG introduces specific improvements to overcome the limitations of Naive RAG. Focusing on enhancing retrieval quality, it employs pre-retrieval and post-retrieval strategies. To tackle the indexing issues, Advanced RAG refines its indexing techniques through the use of a sliding window approach, fine-grained segmentation, and the incorporation of metadata. Additionally, it incorporates several optimization methods to streamline the retrieval process[8].\n",
       "\n",
       "Pre-retrieval process. In this stage, the primary focus is on optimizing the indexing structure and the original query. The goal of optimizing indexing is to enhance the quality of the content being indexed. This involves strategies: enhancing data granularity, optimizing index structures, adding metadata, alignment optimization, and mixed retrieval. While the goal of query optimization is to make the user’s original question clearer and more suitable for the retrieval task. Common methods include query rewriting query transformation, query expansion and other techniques[7,9,10,11].\n",
       "\n",
       "Post-Retrieval Process. Once relevant context is retrieved, it’s crucial to integrate it effectively with the query. The main methods in post-retrieval process include rerank chunks and context compressing. Re-ranking the retrieved information to relocate the most relevant content to the edges of the prompt is a key strategy. This concept has been implemented in frameworks such as LlamaIndex222https://www.llamaindex.ai, LangChain333https://www.langchain.com/, and HayStack[12]. Feeding all relevant documents directly into LLMs can lead to information overload, diluting the focus on key details with irrelevant content.To mitigate this, post-retrieval efforts concentrate on selecting the essential information, emphasizing critical sections, and shortening the context to be processed.\n",
       "\n",
       "\n",
       "\n",
       "II-CModular RAG\n",
       "\n",
       "The modular RAG architecture advances beyond the former two RAG paradigms, offering enhanced adaptability and versatility. It incorporates diverse strategies for improving its components, such as adding a search module for similarity searches and refining the retriever through fine-tuning. Innovations like restructured RAG modules[13]and rearranged RAG pipelines[14]have been introduced to tackle specific challenges. The shift towards a modular RAG approach is becoming prevalent, supporting both sequential processing and integrated end-to-end training across its components. Despite its distinctiveness, Modular RAG builds upon the foundational principles of Advanced and Naive RAG, illustrating a progression and refinement within the RAG family.\n",
       "\n",
       "The Modular RAG framework introduces additional specialized components to enhance retrieval and processing capabilities. The Search module adapts to specific scenarios, enabling direct searches across various data sources like search engines, databases, and knowledge graphs, using LLM-generated code and query languages[15]. RAG-Fusion addresses traditional search limitations by employing a multi-query strategy that expands user queries into diverse perspectives, utilizing parallel vector searches and intelligent re-ranking to uncover both explicit and transformative knowledge[16]. The Memory module leverages the LLM’s memory to guide retrieval, creating an unbounded memory pool that aligns the text more closely with data distribution through iterative self-enhancement[17,18]. Routing in the RAG system navigates through diverse data sources, selecting the optimal pathway for a query, whether it involves summarization, specific database searches, or merging different information streams[19]. The Predict module aims to reduce redundancy and noise by generating context directly through the LLM, ensuring relevance and accuracy[13]. Lastly, the Task Adapter module tailors RAG to various downstream tasks, automating prompt retrieval for zero-shot inputs and creating task-specific retrievers through few-shot query generation[20,21].This comprehensive approach not only streamlines the retrieval process but also significantly improves the quality and relevance of the information retrieved, catering to a wide array of tasks and queries with enhanced precision and flexibility.\n",
       "\n",
       "Modular RAG offers remarkable adaptability by allowing module substitution or reconfiguration to address specific challenges. This goes beyond the fixed structures of Naive and Advanced RAG, characterized by a simple “Retrieve” and “Read” mechanism. Moreover, Modular RAG expands this flexibility by integrating new modules or adjusting interaction flow among existing ones, enhancing its applicability across different tasks.\n",
       "\n",
       "Innovations such as the Rewrite-Retrieve-Read[7]model leverage the LLM’s capabilities to refine retrieval queries through a rewriting module and a LM-feedback mechanism to update rewriting model., improving task performance. Similarly, approaches like Generate-Read[13]replace traditional retrieval with LLM-generated content, while Recite-Read[22]emphasizes retrieval from model weights, enhancing the model’s ability to handle knowledge-intensive tasks. Hybrid retrieval strategies integrate keyword, semantic, and vector searches to cater to diverse queries. Additionally, employing sub-queries and hypothetical document embeddings (HyDE)[11]seeks to improve retrieval relevance by focusing on embedding similarities between generated answers and real documents.\n",
       "\n",
       "Adjustments in module arrangement and interaction, such as the Demonstrate-Search-Predict (DSP)[23]framework and the iterative Retrieve-Read-Retrieve-Read flow of ITER-RETGEN[14], showcase the dynamic use of module outputs to bolster another module’s functionality, illustrating a sophisticated understanding of enhancing module synergy. The flexible orchestration of Modular RAG Flow showcases the benefits of adaptive retrieval through techniques such as FLARE[24]and Self-RAG[25]. This approach transcends the fixed RAG retrieval process by evaluating the necessity of retrieval based on different scenarios. Another benefit of a flexible architecture is that the RAG system can more easily integrate with other technologies (such as fine-tuning or reinforcement learning)[26]. For example, this can involve fine-tuning the retriever for better retrieval results, fine-tuning the generator for more personalized outputs, or engaging in collaborative fine-tuning[27].\n",
       "\n",
       "\n",
       "\n",
       "II-DRAG vs Fine-tuning\n",
       "\n",
       "The augmentation of LLMs has attracted considerable attention due to their growing prevalence. Among the optimization methods for LLMs, RAG is often compared with Fine-tuning (FT) and prompt engineering. Each method has distinct characteristics as illustrated in Figure4. We used a quadrant chart to illustrate the differences among three methods in two dimensions: external knowledge requirements and model adaption requirements. Prompt engineering leverages a model’s inherent capabilities with minimum necessity for external knowledge and model adaption. RAG can be likened to providing a model with a tailored textbook for information retrieval, ideal for precise information retrieval tasks. In contrast, FT is comparable to a student internalizing knowledge over time, suitable for scenarios requiring replication of specific structures, styles, or formats.\n",
       "\n",
       "RAG excels in dynamic environments by offering real-time knowledge updates and effective utilization of external knowledge sources with high interpretability. However, it comes with higher latency and ethical considerations regarding data retrieval. On the other hand, FT is more static, requiring retraining for updates but enabling deep customization of the model’s behavior and style. It demands significant computational resources for dataset preparation and training, and while it can reduce hallucinations, it may face challenges with unfamiliar data.\n",
       "\n",
       "In multiple evaluations of their performance on various knowledge-intensive tasks across different topics,[28]revealed that while unsupervised fine-tuning shows some improvement, RAG consistently outperforms it, for both existing knowledge encountered during training and entirely new knowledge. Additionally, it was found that LLMs struggle to learn new factual information through unsupervised fine-tuning. The choice between RAG and FT depends on the specific needs for data dynamics, customization, and computational capabilities in the application context. RAG and FT are not mutually exclusive and can complement each other, enhancing a model’s capabilities at different levels. In some instances, their combined use may lead to optimal performance. The optimization process involving RAG and FT may require multiple iterations to achieve satisfactory results."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_section_with_subheadings(html_content, section_id):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    section = soup.find('section', id=section_id)\n",
    "    \n",
    "    if section:\n",
    "        output_text = []\n",
    "        main_heading = section.find(['h2', 'h3'], class_='ltx_title')\n",
    "        if main_heading:\n",
    "            output_text.append(main_heading.get_text(strip=True))\n",
    "        elements = section.find_all(['p', 'h3'], class_=lambda x: x in ['ltx_p', 'ltx_title ltx_title_subsection'])\n",
    "        for element in elements:\n",
    "            if element.name == 'h3':\n",
    "                output_text.append(\"\\n\\n\" + element.get_text(strip=True))\n",
    "            else:\n",
    "                output_text.append(element.get_text(strip=True))\n",
    "        return '\\n\\n'.join(output_text)\n",
    "    return \"Section not found\"\n",
    "\n",
    "display_md(extract_section_with_subheadings(html_content, \"S2\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
