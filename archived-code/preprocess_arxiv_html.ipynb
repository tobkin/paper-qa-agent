{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from IPython.display import Markdown\n",
    "from loaders import HtmlDocumentLoader\n",
    "from preprocessors import ArxivHtmlPaperPreprocessor \n",
    "\n",
    "def display_md(content):\n",
    "  display(Markdown(content))\n",
    "\n",
    "doc_uri = \"https://arxiv.org/html/2312.10997v5\"\n",
    "cache_path = \"./loader_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = HtmlDocumentLoader(doc_uri, cache_path)\n",
    "doc = loader.load()\n",
    "display(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ArxivHtmlPaperPreprocessor()\n",
    "cleaned_text = preprocessor.get_text(doc)\n",
    "display_md(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'paper.html'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(html_content):\n",
    "  strainer = SoupStrainer('h1', class_=\"ltx_title ltx_title_document\")\n",
    "  soup = BeautifulSoup(html_content, 'html.parser', parse_only=strainer)\n",
    "  title_text = soup.get_text()\n",
    "  return title_text\n",
    "\n",
    "print(extract_title(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Authors and Affiliations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_authors_and_affiliations(html_content):\n",
    "    strainer = SoupStrainer('div', class_=\"ltx_authors\")\n",
    "    soup = BeautifulSoup(html_content, 'html.parser', parse_only=strainer)\n",
    "\n",
    "    formatted_output = []\n",
    "    for author in soup.find_all('span', class_='ltx_creator ltx_role_author'):\n",
    "        name = author.find('span', class_='ltx_personname').get_text(strip=True)\n",
    "        affiliation = ' '.join(span.get_text(strip=True) for span in author.find_all('span', class_='ltx_contact ltx_role_affiliation'))\n",
    "        formatted_output.append(f\"{name}: {affiliation}\")\n",
    "    output_text = \"\\n\".join(formatted_output)\n",
    "    return output_text\n",
    "\n",
    "print(extract_authors_and_affiliations(html_content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_abstract(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    abstract_div = soup.find('div', class_='ltx_abstract')\n",
    "    \n",
    "    if abstract_div:\n",
    "        abstract_title = abstract_div.find('h6', class_='ltx_title ltx_title_abstract')\n",
    "        abstract_title_text = abstract_title.get_text(strip=True) if abstract_title else \"Abstract\"\n",
    "        \n",
    "        abstract_paragraph = abstract_div.find('p', class_='ltx_p')\n",
    "        if abstract_paragraph:\n",
    "            for footnote in abstract_paragraph.find_all('span', class_='ltx_note'):\n",
    "                footnote.decompose()\n",
    "            \n",
    "            return f\"{abstract_title_text}\\n\\n{abstract_paragraph.get_text(strip=True)}\"\n",
    "    return \"Abstract not found\"\n",
    "  \n",
    "display_md(extract_abstract(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Section with Subheadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_section_with_subheadings(html_content, section_id):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    section = soup.find('section', id=section_id)\n",
    "    \n",
    "    if section:\n",
    "        output_text = []\n",
    "        main_heading = section.find(['h2', 'h3'], class_='ltx_title')\n",
    "        if main_heading:\n",
    "            output_text.append(main_heading.get_text(strip=True))\n",
    "        elements = section.find_all(['p', 'h3'], class_=lambda x: x in ['ltx_p', 'ltx_title ltx_title_subsection'])\n",
    "        for element in elements:\n",
    "            if element.name == 'h3':\n",
    "                output_text.append(\"\\n\\n\" + element.get_text(strip=True))\n",
    "            else:\n",
    "                output_text.append(element.get_text(strip=True))\n",
    "        return '\\n\\n'.join(output_text)\n",
    "    return \"Section not found\"\n",
    "\n",
    "display_md(extract_section_with_subheadings(html_content, \"S2\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
