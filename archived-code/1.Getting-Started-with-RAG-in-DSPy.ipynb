{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4274da",
   "metadata": {},
   "source": [
    "# Getting Started with RAG in DSPy\n",
    "\n",
    "This notebook will show you how to use DSPy to compile a RAG program! DSPy compilation is a fairly new tool for LLM developers, so let's start with an overview of the concept. By `compiling`, we mean finding the prompts that elicit the behavior we want from LLMs when connected in some kind of pipeline.\n",
    "\n",
    "For example, RAG is a very common LLM pipeline. In it's simplest form, RAG consists of 2 steps, (1) Retrieve and (2) Answer a Question. Part (2), Answering a Question, has an associated prompt, for example, people generally use:\n",
    "\n",
    "```\n",
    "--\n",
    "\n",
    "Please answer the question based on the following context.\n",
    "\n",
    "context  {context}\n",
    "\n",
    "question {question}\n",
    "\n",
    "--\n",
    "```\n",
    "\n",
    "This prompt may be a good initial point for an LLM to understand the task. However, it is not the *optimal* prompt. DSPy optimizes the prompt for you by jointly (1) tweaking the instructions, such as rewriting an initial prompt like: \n",
    "\n",
    "```\n",
    "Please answer the question based on the following context.\n",
    "```\n",
    "\n",
    "to \n",
    "\n",
    "```\n",
    "Assess the context and answer the given questions that are predominantly about software usage, process optimization, and troubleshooting. Focus on providing accurate information related to tech or software-related queries.\n",
    "```\n",
    "\n",
    "Further, DSPy (2) finds examples of desired input-outputs in the prompt to further improve performance, also known as `In-Context Learning`. In this example, we will begin with the simple prompt: `Please answer the question based on the following context.` and end up with:\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "In order to leverage black-box optimization techniques like random search, bayesian optimization, or evolutionary algorithms, we need a metric. Coming up with metrics to describe desired system behavior has been a longstanding challenge in Machine Learning research. Excitingly, LLMs have made amazing progress. For example, we can evaluate a RAG answer by prompting an LLM with, `Is the assessed text grounded in the context? Say no if it includes significant facts not in the context`. We then optimize the RAG program to increase the metric LLM's assessment of answer quality.\n",
    "\n",
    "This example contains 4 parts:\n",
    "\n",
    "- 0: DSPy Settings and Installation\n",
    "- 1: DSPy Datasets with `dspy.Example`\n",
    "- 2: LLM Metrics in DSPy\n",
    "- 3: LLM Programming with `dspy.Module`\n",
    "- 4: Optimization with `BootstrapFewShot`, `BootstrapFewShotRandomSearch`, and `BayesianSignatureOptimizer`.\n",
    "\n",
    "\n",
    "We are using 2 datasets for this example. Firstly, we have an index of the Weaviate Blog Posts. We will use the Weaviate Blog Posts as the retrieved context to help with our second dataset, the Weaviate FAQs. The Weaviate FAQs consists of 44 question-answer pairs of frequently asked Weaviate questions such as: `Do I need to know about Docker (Compose) to use Weaviate?`\n",
    "\n",
    "We isolate 10 examples to use as our test set and optimize our program with the remaining 34.\n",
    "\n",
    "Our uncompiled RAG program achieves a score of 270 on the held-out test set.\n",
    "\n",
    "Our RAG program compiled with the `BayesianSignatureOptimizer` achieves a score of 340! A ~30% improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb763b0b",
   "metadata": {},
   "source": [
    "# 0: DSPy Settings and Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa540f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall dspy-ai weaviate-client\n",
    "# %pip install dspy-ai==2.1.9 weaviate-client==3.26.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42260862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Weaviate Retriever and configure LLM\n",
    "import dspy\n",
    "from dspy.retrieve.weaviate_rm import WeaviateRM\n",
    "from wcs_client_adapter import WcsClientAdapter\n",
    "from wcs_client_adapter import COLLECTION_TEXT_KEY, WCS_COLLECTION_NAME\n",
    "\n",
    "def display_md(content):\n",
    "  display(Markdown(content))\n",
    "\n",
    "llm = dspy.OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "wcs_client = WcsClientAdapter.get_wcs_client()\n",
    "retriever_model = WeaviateRM(WCS_COLLECTION_NAME, weaviate_client=wcs_client, weaviate_collection_text_key=COLLECTION_TEXT_KEY)\n",
    "dspy.settings.configure(lm=llm, rm=retriever_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b146a20",
   "metadata": {},
   "source": [
    "# 1. DSPy Datasets with `dspy.Example`\n",
    "\n",
    "Our retrieval engine is filled with chunks from Weaviate Blog posts.\n",
    "\n",
    "Please see weaviate/recipes/integrations/dspy/Weaviate-Import.ipynb for a full tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6373a6e2",
   "metadata": {},
   "source": [
    "# Index Paper to WCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f9ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexers import NaiveWcsIndexer\n",
    "\n",
    "doc_uri = \"https://arxiv.org/html/2312.10997v5\"\n",
    "indexer = NaiveWcsIndexer(doc_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08417e60",
   "metadata": {},
   "source": [
    "## Import User Questions from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21cacaa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answerable Questions (Top 5): \n",
      "What are the core challenges that RAG aims to solve in large language models (LLMs)?\n",
      "How does the paper define and differentiate between Naive RAG Advanced RAG and Modular RAG?\n",
      "What specific improvements does Advanced RAG introduce over Naive RAG?\n",
      "Can you explain the roles of retrieval generation and augmentation processes in the RAG framework?\n",
      "How are external knowledge sources integrated during the retrieval phase to enhance the generation quality?\n",
      "\n",
      "Unanswerable Questions (Top 5): \n",
      "How do different RAG implementations impact the latency of response generation in real-time systems?\n",
      "What metrics are used to evaluate the trade-off between retrieval accuracy and generation quality in RAG systems?\n",
      "How can RAG be optimized for low-resource languages or dialects?\n",
      "What are the implications of data drift on RAG systems over time?\n",
      "How can developers ensure that RAG systems do not inadvertently propagate fake news or misinformation?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from typing import List\n",
    "\n",
    "answerable_questions_path = \"./data/answerable-questions.csv\"\n",
    "unanswerable_questions_path = \"./data/unanswerable-questions.csv\"\n",
    "\n",
    "def load_questions_from_csv(file_path: str) -> List[str]:\n",
    "    questions = []\n",
    "    with open(file_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                questions.append(row[0])\n",
    "    return questions\n",
    "\n",
    "answerable_questions = load_questions_from_csv(answerable_questions_path)\n",
    "unanswerable_questions = load_questions_from_csv(unanswerable_questions_path)\n",
    "all_questions = answerable_questions + unanswerable_questions\n",
    "print(f\"Answerable Questions (Top 5): \\n\" + \"\\n\".join(answerable_questions[:5]) + \"\\n\")\n",
    "print(f\"Unanswerable Questions (Top 5): \\n\" + \"\\n\".join(unanswerable_questions[:5]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89745ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c34f8d",
   "metadata": {},
   "source": [
    "# Wrap each FAQ into an `Example` object\n",
    "\n",
    "The dspy `Example` object optionally lets you attach metadata, or additional labels, to input/output pairs.\n",
    "\n",
    "For example, you may want to jointly supervise the answer as well as the context the retrieval system produced to feed into the answer generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1485fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from typing import List, NamedTuple\n",
    "import random\n",
    "\n",
    "class DataSplits(NamedTuple):\n",
    "    train: List\n",
    "    dev: List\n",
    "    test: List\n",
    "\n",
    "def split_data(data: List, train_size: float, dev_size: float, test_size: float) -> DataSplits:\n",
    "    if train_size + dev_size + test_size != 1:\n",
    "        raise ValueError(\"The sum of train_size, dev_size, and test_size must be 1.\")\n",
    "\n",
    "    random.shuffle(data)  \n",
    "    \n",
    "    train_end = int(train_size * len(data))\n",
    "    dev_end = train_end + int(dev_size * len(data))\n",
    "    \n",
    "    train_set = data[:train_end]\n",
    "    dev_set = data[train_end:dev_end]\n",
    "    test_set = data[dev_end:]\n",
    "    \n",
    "    return DataSplits(train=train_set, dev=dev_set, test=test_set)\n",
    "\n",
    "splits = split_data(all_questions, 0.7, 0.15, 0.15)\n",
    "\n",
    "trainset = splits.train\n",
    "devset = splits.dev\n",
    "testset = splits.test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175ab89",
   "metadata": {},
   "source": [
    "# 2. LLM Metrics\n",
    "\n",
    "Define a Metric for Performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07a5411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference - https://github.com/stanfordnlp/dspy/blob/main/examples/tweets/tweet_metric.py\n",
    "\n",
    "metricLM = dspy.OpenAI(model='gpt-4-turbo', max_tokens=1000, model_type='chat')\n",
    "\n",
    "# Signature for LLM assessments.\n",
    "\n",
    "class Assess(dspy.Signature):\n",
    "    \"\"\"Assess the quality of an answer to a question.\"\"\"\n",
    "    \n",
    "    context = dspy.InputField(desc=\"The context for answering the question.\")\n",
    "    assessed_question = dspy.InputField(desc=\"The evaluation criterion.\")\n",
    "    assessed_answer = dspy.InputField(desc=\"The answer to the question.\")\n",
    "    assessment_answer = dspy.OutputField(desc=\"A rating between 1 and 5. Only output the rating and nothing else.\")\n",
    "\n",
    "def llm_metric(gold, pred, trace=None):\n",
    "    predicted_answer = pred.answer\n",
    "    question = gold.question\n",
    "    \n",
    "    print(f\"Test Question: {question}\")\n",
    "    print(f\"Predicted Answer: {predicted_answer}\")\n",
    "    \n",
    "    detail = \"Is the assessed answer detailed?\"\n",
    "    faithful = \"Is the assessed text grounded in the context? Say no if it includes significant facts not in the context.\"\n",
    "    overall = f\"Please rate how well this answer answers the question, `{question}` based on the context.\\n `{predicted_answer}`\"\n",
    "    \n",
    "    with dspy.context(lm=metricLM):\n",
    "        context = dspy.Retrieve(k=5)(question).passages\n",
    "        detail = dspy.ChainOfThought(Assess)(context=\"N/A\", assessed_question=detail, assessed_answer=predicted_answer)\n",
    "        faithful = dspy.ChainOfThought(Assess)(context=context, assessed_question=faithful, assessed_answer=predicted_answer)\n",
    "        overall = dspy.ChainOfThought(Assess)(context=context, assessed_question=overall, assessed_answer=predicted_answer)\n",
    "    \n",
    "    print(f\"Faithful: {faithful.assessment_answer}\")\n",
    "    print(f\"Detail: {detail.assessment_answer}\")\n",
    "    print(f\"Overall: {overall.assessment_answer}\")\n",
    "    \n",
    "    \n",
    "    total = float(detail.assessment_answer) + float(faithful.assessment_answer)*2 + float(overall.assessment_answer)\n",
    "    \n",
    "    return total / 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0cc41a",
   "metadata": {},
   "source": [
    "## Inspect the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16cf6048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: What are the core challenges that RAG aims to solve in large language models (LLMs)?\n",
      "Predicted Answer: Hallucinations, outdated knowledge, and opaque reasoning processes.\n",
      "Faithful: 5\n",
      "Detail: 1\n",
      "Overall: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example = dspy.Example(question=\"What are the core challenges that RAG aims to solve in large language models (LLMs)?\")\n",
    "test_pred = dspy.Example(answer=\"Hallucinations, outdated knowledge, and opaque reasoning processes.\")\n",
    "\n",
    "type(llm_metric(test_example, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0f763ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: How does the paper define and differentiate between Naive RAG Advanced RAG and Modular RAG?\n",
      "Predicted Answer: Naive, Advanced, and Modular RAG evolve in complexity and flexibility.\n",
      "Faithful: 5\n",
      "Detail: 2\n",
      "Overall: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example = dspy.Example(question=\"How does the paper define and differentiate between Naive RAG Advanced RAG and Modular RAG?\")\n",
    "test_pred = dspy.Example(answer=\"Naive, Advanced, and Modular RAG evolve in complexity and flexibility.\")\n",
    "\n",
    "type(llm_metric(test_example, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a4ccd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Assess the quality of an answer to a question.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: The context for answering the question.\n",
      "\n",
      "Assessed Question: The evaluation criterion.\n",
      "\n",
      "Assessed Answer: The answer to the question.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the assessment_answer}. We ...\n",
      "\n",
      "Assessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\n",
      "\n",
      "---\n",
      "\n",
      "Context: N/A\n",
      "\n",
      "Assessed Question: Is the assessed answer detailed?\n",
      "\n",
      "Assessed Answer: Naive, Advanced, and Modular RAG evolve in complexity and flexibility.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the assessment_answer. We need to determine if the assessed answer provides detailed information. The answer mentions three types of RAG (Naive, Advanced, and Modular) but does not elaborate on what these terms mean, how they differ from each other, or any specific details about their complexity and flexibility. The answer is very brief and lacks depth, which is necessary to consider it detailed.\n",
      "\n",
      "Assessment Answer: 2\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Assess the quality of an answer to a question.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: The context for answering the question.\n",
      "\n",
      "Assessed Question: The evaluation criterion.\n",
      "\n",
      "Assessed Answer: The answer to the question.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the assessment_answer}. We ...\n",
      "\n",
      "Assessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «approaches in their respective contexts, and speculate on upcoming trends and innovations. Our contributions are as follows: In this survey, we present a thorough and systematic review of the state-of-the-art RAG methods, delineating its evolution through paradigms including naive RAG, advanced RAG, and modular RAG. This review contextualizes the broader scope of RAG research within the landscape of LLMs. We identify and discuss the central technologies integral to the RAG process, specifically focusing on the aspects of “Retrieval”, “Generation” and “Augmentation”, and delve into their synergies, elucidating how these components intricately collaborate to form a cohesive and effective RAG framework. We have summarized the current assessment methods of RAG, covering 26 tasks, nearly 50 datasets, outlining the evaluation objectives and metrics, as well as the current evaluation benchmarks and tools. Additionally, we anticipate future directions for RAG, emphasizing potential enhancements to tackle current challenges. The paper unfolds as follows: SectionIIintroduces the main concept and current paradigms of RAG. The following three sections explore core components—“Retrieval”, “Generation” and “Augmentation”, respectively. SectionIIIfocuses on optimization methods in retrieval,including»\n",
      "[2] «comprehensive prompt that empowers LLMs to generate a well-informed answer. The RAG research paradigm is continuously evolving, and we categorize it into three stages: Naive RAG, Advanced RAG, and Modular RAG, as showed in Figure3. Despite RAG method are cost-effective and surpass the performance of the native LLM, they also exhibit several limitations. The development of Advanced RAG and Modular RAG is a response to these specific shortcomings in Naive RAG. II-ANaive RAG The Naive RAG research paradigm represents the earliest methodology, which gained prominence shortly after the widespread adoption of ChatGPT. The Naive RAG follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a “Retrieve-Read” framework[7]. Indexingstarts with the cleaning and extraction of raw data in diverse formats like PDF, HTML, Word, and Markdown, which is then converted into a uniform plain text format. To accommodate the context limitations of language models, text is segmented into smaller, digestible chunks. Chunks are then encoded into vector representations using an embedding model and stored in vector database. This step is»\n",
      "[3] «efforts concentrate on selecting the essential information, emphasizing critical sections, and shortening the context to be processed. II-CModular RAG The modular RAG architecture advances beyond the former two RAG paradigms, offering enhanced adaptability and versatility. It incorporates diverse strategies for improving its components, such as adding a search module for similarity searches and refining the retriever through fine-tuning. Innovations like restructured RAG modules[13]and rearranged RAG pipelines[14]have been introduced to tackle specific challenges. The shift towards a modular RAG approach is becoming prevalent, supporting both sequential processing and integrated end-to-end training across its components. Despite its distinctiveness, Modular RAG builds upon the foundational principles of Advanced and Naive RAG, illustrating a progression and refinement within the RAG family. The Modular RAG framework introduces additional specialized components to enhance retrieval and processing capabilities. The Search module adapts to specific scenarios, enabling direct searches across various data sources like search engines, databases, and knowledge graphs, using LLM-generated code and query languages[15]. RAG-Fusion addresses traditional search limitations by employing a multi-query strategy that expands user queries into diverse perspectives,»\n",
      "[4] «University College of Design and Innovation, Tongji University Abstract Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs’ intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and»\n",
      "[5] «significantly improves the quality and relevance of the information retrieved, catering to a wide array of tasks and queries with enhanced precision and flexibility. Modular RAG offers remarkable adaptability by allowing module substitution or reconfiguration to address specific challenges. This goes beyond the fixed structures of Naive and Advanced RAG, characterized by a simple “Retrieve” and “Read” mechanism. Moreover, Modular RAG expands this flexibility by integrating new modules or adjusting interaction flow among existing ones, enhancing its applicability across different tasks. Innovations such as the Rewrite-Retrieve-Read[7]model leverage the LLM’s capabilities to refine retrieval queries through a rewriting module and a LM-feedback mechanism to update rewriting model., improving task performance. Similarly, approaches like Generate-Read[13]replace traditional retrieval with LLM-generated content, while Recite-Read[22]emphasizes retrieval from model weights, enhancing the model’s ability to handle knowledge-intensive tasks. Hybrid retrieval strategies integrate keyword, semantic, and vector searches to cater to diverse queries. Additionally, employing sub-queries and hypothetical document embeddings (HyDE)[11]seeks to improve retrieval relevance by focusing on embedding similarities between generated answers and real documents. Adjustments in module arrangement and interaction,»\n",
      "\n",
      "Assessed Question: Is the assessed text grounded in the context? Say no if it includes significant facts not in the context.\n",
      "\n",
      "Assessed Answer: Naive, Advanced, and Modular RAG evolve in complexity and flexibility.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the assessment_answer. We start by examining the context provided, which includes detailed descriptions of Naive RAG, Advanced RAG, and Modular RAG across multiple excerpts. The context outlines the evolution from Naive RAG, characterized by basic indexing and retrieval processes, to Advanced RAG, which improves upon Naive RAG's limitations, and finally to Modular RAG, which introduces greater adaptability and specialized components for enhanced retrieval and processing capabilities.\n",
      "\n",
      "The assessed answer states that \"Naive, Advanced, and Modular RAG evolve in complexity and flexibility.\" This statement aligns well with the descriptions in the context, where each subsequent RAG paradigm builds upon the previous one, adding more sophisticated features and addressing earlier shortcomings. The context explicitly mentions the progression and refinement within the RAG family, highlighting the increasing complexity and flexibility from Naive to Modular RAG.\n",
      "\n",
      "Therefore, the assessed answer is grounded in the context as it accurately reflects the evolutionary trajectory of RAG paradigms described in the provided excerpts.\n",
      "\n",
      "Assessment Answer: 5\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Assess the quality of an answer to a question.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: The context for answering the question.\n",
      "\n",
      "Assessed Question: The evaluation criterion.\n",
      "\n",
      "Assessed Answer: The answer to the question.\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the assessment_answer}. We ...\n",
      "\n",
      "Assessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «approaches in their respective contexts, and speculate on upcoming trends and innovations. Our contributions are as follows: In this survey, we present a thorough and systematic review of the state-of-the-art RAG methods, delineating its evolution through paradigms including naive RAG, advanced RAG, and modular RAG. This review contextualizes the broader scope of RAG research within the landscape of LLMs. We identify and discuss the central technologies integral to the RAG process, specifically focusing on the aspects of “Retrieval”, “Generation” and “Augmentation”, and delve into their synergies, elucidating how these components intricately collaborate to form a cohesive and effective RAG framework. We have summarized the current assessment methods of RAG, covering 26 tasks, nearly 50 datasets, outlining the evaluation objectives and metrics, as well as the current evaluation benchmarks and tools. Additionally, we anticipate future directions for RAG, emphasizing potential enhancements to tackle current challenges. The paper unfolds as follows: SectionIIintroduces the main concept and current paradigms of RAG. The following three sections explore core components—“Retrieval”, “Generation” and “Augmentation”, respectively. SectionIIIfocuses on optimization methods in retrieval,including»\n",
      "[2] «comprehensive prompt that empowers LLMs to generate a well-informed answer. The RAG research paradigm is continuously evolving, and we categorize it into three stages: Naive RAG, Advanced RAG, and Modular RAG, as showed in Figure3. Despite RAG method are cost-effective and surpass the performance of the native LLM, they also exhibit several limitations. The development of Advanced RAG and Modular RAG is a response to these specific shortcomings in Naive RAG. II-ANaive RAG The Naive RAG research paradigm represents the earliest methodology, which gained prominence shortly after the widespread adoption of ChatGPT. The Naive RAG follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a “Retrieve-Read” framework[7]. Indexingstarts with the cleaning and extraction of raw data in diverse formats like PDF, HTML, Word, and Markdown, which is then converted into a uniform plain text format. To accommodate the context limitations of language models, text is segmented into smaller, digestible chunks. Chunks are then encoded into vector representations using an embedding model and stored in vector database. This step is»\n",
      "[3] «efforts concentrate on selecting the essential information, emphasizing critical sections, and shortening the context to be processed. II-CModular RAG The modular RAG architecture advances beyond the former two RAG paradigms, offering enhanced adaptability and versatility. It incorporates diverse strategies for improving its components, such as adding a search module for similarity searches and refining the retriever through fine-tuning. Innovations like restructured RAG modules[13]and rearranged RAG pipelines[14]have been introduced to tackle specific challenges. The shift towards a modular RAG approach is becoming prevalent, supporting both sequential processing and integrated end-to-end training across its components. Despite its distinctiveness, Modular RAG builds upon the foundational principles of Advanced and Naive RAG, illustrating a progression and refinement within the RAG family. The Modular RAG framework introduces additional specialized components to enhance retrieval and processing capabilities. The Search module adapts to specific scenarios, enabling direct searches across various data sources like search engines, databases, and knowledge graphs, using LLM-generated code and query languages[15]. RAG-Fusion addresses traditional search limitations by employing a multi-query strategy that expands user queries into diverse perspectives,»\n",
      "[4] «University College of Design and Innovation, Tongji University Abstract Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs’ intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and»\n",
      "[5] «significantly improves the quality and relevance of the information retrieved, catering to a wide array of tasks and queries with enhanced precision and flexibility. Modular RAG offers remarkable adaptability by allowing module substitution or reconfiguration to address specific challenges. This goes beyond the fixed structures of Naive and Advanced RAG, characterized by a simple “Retrieve” and “Read” mechanism. Moreover, Modular RAG expands this flexibility by integrating new modules or adjusting interaction flow among existing ones, enhancing its applicability across different tasks. Innovations such as the Rewrite-Retrieve-Read[7]model leverage the LLM’s capabilities to refine retrieval queries through a rewriting module and a LM-feedback mechanism to update rewriting model., improving task performance. Similarly, approaches like Generate-Read[13]replace traditional retrieval with LLM-generated content, while Recite-Read[22]emphasizes retrieval from model weights, enhancing the model’s ability to handle knowledge-intensive tasks. Hybrid retrieval strategies integrate keyword, semantic, and vector searches to cater to diverse queries. Additionally, employing sub-queries and hypothetical document embeddings (HyDE)[11]seeks to improve retrieval relevance by focusing on embedding similarities between generated answers and real documents. Adjustments in module arrangement and interaction,»\n",
      "\n",
      "Assessed Question: Please rate how well this answer answers the question, `How does the paper define and differentiate between Naive RAG Advanced RAG and Modular RAG?` based on the context. `Naive, Advanced, and Modular RAG evolve in complexity and flexibility.`\n",
      "\n",
      "Assessed Answer: Naive, Advanced, and Modular RAG evolve in complexity and flexibility.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the assessment_answer. We start by examining the context provided, which includes detailed descriptions of the Naive RAG, Advanced RAG, and Modular RAG paradigms. Each description outlines specific methodologies, technologies, and innovations associated with each stage of RAG development. The question specifically asks for definitions and distinctions between these three RAG types.\n",
      "\n",
      "The assessed answer, \"Naive, Advanced, and Modular RAG evolve in complexity and flexibility,\" provides a very high-level summary of the progression from Naive to Modular RAG. However, it lacks depth and does not mention any specific methodologies, technologies, or innovations that differentiate each type. The answer does not capture the detailed distinctions such as the technological advancements or the specific challenges each RAG type addresses, which are clearly outlined in the context.\n",
      "\n",
      "Therefore, the answer is overly simplistic and does not adequately reflect the detailed information available in the context. It fails to provide the definitions or detailed differences required by the question, focusing only on a general progression in terms of complexity and flexibility without any supporting details.\n",
      "\n",
      "Assessment Answer: 2\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nAssess the quality of an answer to a question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: The context for answering the question.\\n\\nAssessed Question: The evaluation criterion.\\n\\nAssessed Answer: The answer to the question.\\n\\nReasoning: Let\\'s think step by step in order to ${produce the assessment_answer}. We ...\\n\\nAssessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\\n\\n---\\n\\nContext: N/A\\n\\nAssessed Question: Is the assessed answer detailed?\\n\\nAssessed Answer: Naive, Advanced, and Modular RAG evolve in complexity and flexibility.\\n\\nReasoning: Let\\'s think step by step in order to\\x1b[32m produce the assessment_answer. We need to determine if the assessed answer provides detailed information. The answer mentions three types of RAG (Naive, Advanced, and Modular) but does not elaborate on what these terms mean, how they differ from each other, or any specific details about their complexity and flexibility. The answer is very brief and lacks depth, which is necessary to consider it detailed.\\n\\nAssessment Answer: 2\\x1b[0m\\n\\n\\n\\n\\n\\nAssess the quality of an answer to a question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: The context for answering the question.\\n\\nAssessed Question: The evaluation criterion.\\n\\nAssessed Answer: The answer to the question.\\n\\nReasoning: Let\\'s think step by step in order to ${produce the assessment_answer}. We ...\\n\\nAssessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\\n\\n---\\n\\nContext:\\n[1] «approaches in their respective contexts, and speculate on upcoming trends and innovations. Our contributions are as follows: In this survey, we present a thorough and systematic review of the state-of-the-art RAG methods, delineating its evolution through paradigms including naive RAG, advanced RAG, and modular RAG. This review contextualizes the broader scope of RAG research within the landscape of LLMs. We identify and discuss the central technologies integral to the RAG process, specifically focusing on the aspects of “Retrieval”, “Generation” and “Augmentation”, and delve into their synergies, elucidating how these components intricately collaborate to form a cohesive and effective RAG framework. We have summarized the current assessment methods of RAG, covering 26 tasks, nearly 50 datasets, outlining the evaluation objectives and metrics, as well as the current evaluation benchmarks and tools. Additionally, we anticipate future directions for RAG, emphasizing potential enhancements to tackle current challenges. The paper unfolds as follows: SectionIIintroduces the main concept and current paradigms of RAG. The following three sections explore core components—“Retrieval”, “Generation” and “Augmentation”, respectively. SectionIIIfocuses on optimization methods in retrieval,including»\\n[2] «comprehensive prompt that empowers LLMs to generate a well-informed answer. The RAG research paradigm is continuously evolving, and we categorize it into three stages: Naive RAG, Advanced RAG, and Modular RAG, as showed in Figure3. Despite RAG method are cost-effective and surpass the performance of the native LLM, they also exhibit several limitations. The development of Advanced RAG and Modular RAG is a response to these specific shortcomings in Naive RAG. II-ANaive RAG The Naive RAG research paradigm represents the earliest methodology, which gained prominence shortly after the widespread adoption of ChatGPT. The Naive RAG follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a “Retrieve-Read” framework[7]. Indexingstarts with the cleaning and extraction of raw data in diverse formats like PDF, HTML, Word, and Markdown, which is then converted into a uniform plain text format. To accommodate the context limitations of language models, text is segmented into smaller, digestible chunks. Chunks are then encoded into vector representations using an embedding model and stored in vector database. This step is»\\n[3] «efforts concentrate on selecting the essential information, emphasizing critical sections, and shortening the context to be processed. II-CModular RAG The modular RAG architecture advances beyond the former two RAG paradigms, offering enhanced adaptability and versatility. It incorporates diverse strategies for improving its components, such as adding a search module for similarity searches and refining the retriever through fine-tuning. Innovations like restructured RAG modules[13]and rearranged RAG pipelines[14]have been introduced to tackle specific challenges. The shift towards a modular RAG approach is becoming prevalent, supporting both sequential processing and integrated end-to-end training across its components. Despite its distinctiveness, Modular RAG builds upon the foundational principles of Advanced and Naive RAG, illustrating a progression and refinement within the RAG family. The Modular RAG framework introduces additional specialized components to enhance retrieval and processing capabilities. The Search module adapts to specific scenarios, enabling direct searches across various data sources like search engines, databases, and knowledge graphs, using LLM-generated code and query languages[15]. RAG-Fusion addresses traditional search limitations by employing a multi-query strategy that expands user queries into diverse perspectives,»\\n[4] «University College of Design and Innovation, Tongji University Abstract Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs’ intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and»\\n[5] «significantly improves the quality and relevance of the information retrieved, catering to a wide array of tasks and queries with enhanced precision and flexibility. Modular RAG offers remarkable adaptability by allowing module substitution or reconfiguration to address specific challenges. This goes beyond the fixed structures of Naive and Advanced RAG, characterized by a simple “Retrieve” and “Read” mechanism. Moreover, Modular RAG expands this flexibility by integrating new modules or adjusting interaction flow among existing ones, enhancing its applicability across different tasks. Innovations such as the Rewrite-Retrieve-Read[7]model leverage the LLM’s capabilities to refine retrieval queries through a rewriting module and a LM-feedback mechanism to update rewriting model., improving task performance. Similarly, approaches like Generate-Read[13]replace traditional retrieval with LLM-generated content, while Recite-Read[22]emphasizes retrieval from model weights, enhancing the model’s ability to handle knowledge-intensive tasks. Hybrid retrieval strategies integrate keyword, semantic, and vector searches to cater to diverse queries. Additionally, employing sub-queries and hypothetical document embeddings (HyDE)[11]seeks to improve retrieval relevance by focusing on embedding similarities between generated answers and real documents. Adjustments in module arrangement and interaction,»\\n\\nAssessed Question: Is the assessed text grounded in the context? Say no if it includes significant facts not in the context.\\n\\nAssessed Answer: Naive, Advanced, and Modular RAG evolve in complexity and flexibility.\\n\\nReasoning: Let\\'s think step by step in order to\\x1b[32m produce the assessment_answer. We start by examining the context provided, which includes detailed descriptions of Naive RAG, Advanced RAG, and Modular RAG across multiple excerpts. The context outlines the evolution from Naive RAG, characterized by basic indexing and retrieval processes, to Advanced RAG, which improves upon Naive RAG\\'s limitations, and finally to Modular RAG, which introduces greater adaptability and specialized components for enhanced retrieval and processing capabilities.\\n\\nThe assessed answer states that \"Naive, Advanced, and Modular RAG evolve in complexity and flexibility.\" This statement aligns well with the descriptions in the context, where each subsequent RAG paradigm builds upon the previous one, adding more sophisticated features and addressing earlier shortcomings. The context explicitly mentions the progression and refinement within the RAG family, highlighting the increasing complexity and flexibility from Naive to Modular RAG.\\n\\nTherefore, the assessed answer is grounded in the context as it accurately reflects the evolutionary trajectory of RAG paradigms described in the provided excerpts.\\n\\nAssessment Answer: 5\\x1b[0m\\n\\n\\n\\n\\n\\nAssess the quality of an answer to a question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: The context for answering the question.\\n\\nAssessed Question: The evaluation criterion.\\n\\nAssessed Answer: The answer to the question.\\n\\nReasoning: Let\\'s think step by step in order to ${produce the assessment_answer}. We ...\\n\\nAssessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\\n\\n---\\n\\nContext:\\n[1] «approaches in their respective contexts, and speculate on upcoming trends and innovations. Our contributions are as follows: In this survey, we present a thorough and systematic review of the state-of-the-art RAG methods, delineating its evolution through paradigms including naive RAG, advanced RAG, and modular RAG. This review contextualizes the broader scope of RAG research within the landscape of LLMs. We identify and discuss the central technologies integral to the RAG process, specifically focusing on the aspects of “Retrieval”, “Generation” and “Augmentation”, and delve into their synergies, elucidating how these components intricately collaborate to form a cohesive and effective RAG framework. We have summarized the current assessment methods of RAG, covering 26 tasks, nearly 50 datasets, outlining the evaluation objectives and metrics, as well as the current evaluation benchmarks and tools. Additionally, we anticipate future directions for RAG, emphasizing potential enhancements to tackle current challenges. The paper unfolds as follows: SectionIIintroduces the main concept and current paradigms of RAG. The following three sections explore core components—“Retrieval”, “Generation” and “Augmentation”, respectively. SectionIIIfocuses on optimization methods in retrieval,including»\\n[2] «comprehensive prompt that empowers LLMs to generate a well-informed answer. The RAG research paradigm is continuously evolving, and we categorize it into three stages: Naive RAG, Advanced RAG, and Modular RAG, as showed in Figure3. Despite RAG method are cost-effective and surpass the performance of the native LLM, they also exhibit several limitations. The development of Advanced RAG and Modular RAG is a response to these specific shortcomings in Naive RAG. II-ANaive RAG The Naive RAG research paradigm represents the earliest methodology, which gained prominence shortly after the widespread adoption of ChatGPT. The Naive RAG follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a “Retrieve-Read” framework[7]. Indexingstarts with the cleaning and extraction of raw data in diverse formats like PDF, HTML, Word, and Markdown, which is then converted into a uniform plain text format. To accommodate the context limitations of language models, text is segmented into smaller, digestible chunks. Chunks are then encoded into vector representations using an embedding model and stored in vector database. This step is»\\n[3] «efforts concentrate on selecting the essential information, emphasizing critical sections, and shortening the context to be processed. II-CModular RAG The modular RAG architecture advances beyond the former two RAG paradigms, offering enhanced adaptability and versatility. It incorporates diverse strategies for improving its components, such as adding a search module for similarity searches and refining the retriever through fine-tuning. Innovations like restructured RAG modules[13]and rearranged RAG pipelines[14]have been introduced to tackle specific challenges. The shift towards a modular RAG approach is becoming prevalent, supporting both sequential processing and integrated end-to-end training across its components. Despite its distinctiveness, Modular RAG builds upon the foundational principles of Advanced and Naive RAG, illustrating a progression and refinement within the RAG family. The Modular RAG framework introduces additional specialized components to enhance retrieval and processing capabilities. The Search module adapts to specific scenarios, enabling direct searches across various data sources like search engines, databases, and knowledge graphs, using LLM-generated code and query languages[15]. RAG-Fusion addresses traditional search limitations by employing a multi-query strategy that expands user queries into diverse perspectives,»\\n[4] «University College of Design and Innovation, Tongji University Abstract Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs’ intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and»\\n[5] «significantly improves the quality and relevance of the information retrieved, catering to a wide array of tasks and queries with enhanced precision and flexibility. Modular RAG offers remarkable adaptability by allowing module substitution or reconfiguration to address specific challenges. This goes beyond the fixed structures of Naive and Advanced RAG, characterized by a simple “Retrieve” and “Read” mechanism. Moreover, Modular RAG expands this flexibility by integrating new modules or adjusting interaction flow among existing ones, enhancing its applicability across different tasks. Innovations such as the Rewrite-Retrieve-Read[7]model leverage the LLM’s capabilities to refine retrieval queries through a rewriting module and a LM-feedback mechanism to update rewriting model., improving task performance. Similarly, approaches like Generate-Read[13]replace traditional retrieval with LLM-generated content, while Recite-Read[22]emphasizes retrieval from model weights, enhancing the model’s ability to handle knowledge-intensive tasks. Hybrid retrieval strategies integrate keyword, semantic, and vector searches to cater to diverse queries. Additionally, employing sub-queries and hypothetical document embeddings (HyDE)[11]seeks to improve retrieval relevance by focusing on embedding similarities between generated answers and real documents. Adjustments in module arrangement and interaction,»\\n\\nAssessed Question: Please rate how well this answer answers the question, `How does the paper define and differentiate between Naive RAG Advanced RAG and Modular RAG?` based on the context. `Naive, Advanced, and Modular RAG evolve in complexity and flexibility.`\\n\\nAssessed Answer: Naive, Advanced, and Modular RAG evolve in complexity and flexibility.\\n\\nReasoning: Let\\'s think step by step in order to\\x1b[32m produce the assessment_answer. We start by examining the context provided, which includes detailed descriptions of the Naive RAG, Advanced RAG, and Modular RAG paradigms. Each description outlines specific methodologies, technologies, and innovations associated with each stage of RAG development. The question specifically asks for definitions and distinctions between these three RAG types.\\n\\nThe assessed answer, \"Naive, Advanced, and Modular RAG evolve in complexity and flexibility,\" provides a very high-level summary of the progression from Naive to Modular RAG. However, it lacks depth and does not mention any specific methodologies, technologies, or innovations that differentiate each type. The answer does not capture the detailed distinctions such as the technological advancements or the specific challenges each RAG type addresses, which are clearly outlined in the context.\\n\\nTherefore, the answer is overly simplistic and does not adequately reflect the detailed information available in the context. It fails to provide the definitions or detailed differences required by the question, focusing only on a general progression in terms of complexity and flexibility without any supporting details.\\n\\nAssessment Answer: 2\\x1b[0m\\n\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricLM.inspect_history(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5202b5",
   "metadata": {},
   "source": [
    "# 3. The DSPy Programming Model\n",
    "\n",
    "This block of first code will initilaize the `GenerateAnswer` signature.\n",
    "\n",
    "Then we will compose a `dspy.Module` consisting of:\n",
    "- Retrieve\n",
    "- GenerateAnswer\n",
    "\n",
    "The DSPy programming model is one of the most powerful aspects of DSPy, we get:\n",
    "- An intuitive interface to compose prompts into programs.\n",
    "- A clean way to organize prompts into Signatures.\n",
    "- Structured output parsing with `dspy.OutputField`\n",
    "- Built-in prompt extensions such as `ChainOfThought`, `ReAct`, and more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c5ce19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions based on the context.\"\"\"\n",
    "    \n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad90d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743dda11",
   "metadata": {},
   "source": [
    "# A little more info on built-in dspy modules\n",
    "\n",
    "The DSPy programming model gives you a lot of cool features out of the box. Observe how different modules implement signatures with additional prompting techniques like `ChainOfThought` and `ReAct`. `Predict` is the base class to observe what a standrd prompt looks like without the module extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789b0cd",
   "metadata": {},
   "source": [
    "### dspy.Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80d4cdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Not all input fields were provided to module. Present: ['question']. Missing: ['context'].\n",
      "\n",
      "\n",
      "\n",
      "Answer questions based on the context.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "Question: ${question}\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: What are the core challenges that RAG aims to solve in large language models (LLMs)?\n",
      "Answer:\u001b[32m Context: RAG (Retrieval-Augmented Generation) aims to improve large language models by incorporating a retrieval mechanism to enhance their performance in generating text.\n",
      "Question: What are the core challenges that RAG aims to solve in large language models (LLMs)?\n",
      "Answer: RAG aims to address issues such as factual accuracy, coherence, and relevance in generated text by leveraging a retrieval mechanism to incorporate external knowledge sources.\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nAnswer questions based on the context.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: may contain relevant facts\\nQuestion: ${question}\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: What are the core challenges that RAG aims to solve in large language models (LLMs)?\\nAnswer:\\x1b[32m Context: RAG (Retrieval-Augmented Generation) aims to improve large language models by incorporating a retrieval mechanism to enhance their performance in generating text.\\nQuestion: What are the core challenges that RAG aims to solve in large language models (LLMs)?\\nAnswer: RAG aims to address issues such as factual accuracy, coherence, and relevance in generated text by leveraging a retrieval mechanism to incorporate external knowledge sources.\\x1b[0m\\n\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dspy.Predict(GenerateAnswer)(question=\"What are the core challenges that RAG aims to solve in large language models (LLMs)?\")\n",
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d2cfb6",
   "metadata": {},
   "source": [
    "### dspy.ChainOfThought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9e194e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Not all input fields were provided to module. Present: ['question']. Missing: ['context'].\n",
      "\n",
      "\n",
      "\n",
      "Answer questions based on the context.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: What are the core challenges that RAG aims to solve in large language models (LLMs)?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m Context: RAG (Retrieval-Augmented Generation) is a model that combines retrieval-based and generation-based approaches in large language models (LLMs) to improve performance in natural language processing tasks.\n",
      "\n",
      "Question: What are the core challenges that RAG aims to solve in large language models (LLMs)?\n",
      "\n",
      "Reasoning: Let's think step by step in order to understand the purpose of RAG in LLMs. RAG aims to address challenges such as information retrieval, context understanding, and response generation in LLMs by incorporating a retrieval mechanism to enhance the generation process.\n",
      "\n",
      "Answer: RAG aims to solve challenges related to information retrieval, context understanding, and response generation in large language models (LLMs).\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nAnswer questions based on the context.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: may contain relevant facts\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: What are the core challenges that RAG aims to solve in large language models (LLMs)?\\n\\nReasoning: Let's think step by step in order to\\x1b[32m Context: RAG (Retrieval-Augmented Generation) is a model that combines retrieval-based and generation-based approaches in large language models (LLMs) to improve performance in natural language processing tasks.\\n\\nQuestion: What are the core challenges that RAG aims to solve in large language models (LLMs)?\\n\\nReasoning: Let's think step by step in order to understand the purpose of RAG in LLMs. RAG aims to address challenges such as information retrieval, context understanding, and response generation in LLMs by incorporating a retrieval mechanism to enhance the generation process.\\n\\nAnswer: RAG aims to solve challenges related to information retrieval, context understanding, and response generation in large language models (LLMs).\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dspy.ChainOfThought(GenerateAnswer)(question=\"What are the core challenges that RAG aims to solve in large language models (LLMs)?\")\n",
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5666dd3",
   "metadata": {},
   "source": [
    "### dspy.ReAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd8c99b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Not all input fields were provided to module. Present: ['question']. Missing: ['context'].\n",
      "\n",
      "\n",
      "\n",
      "You will be given `context`, `question` and you will respond with `answer`.\n",
      "\n",
      "To do this, you will interleave Thought, Action, and Observation steps.\n",
      "\n",
      "Thought can reason about the current situation, and Action can be the following types:\n",
      "\n",
      "(1) Search[query], which takes a search query and returns one or more potentially relevant passages from a corpus\n",
      "(2) Finish[answer], which returns the final `answer` and finishes the task\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Thought 1: next steps to take based on last observation\n",
      "\n",
      "Action 1: always either Search[query] or, when done, Finish[answer]\n",
      "\n",
      "Observation 1: observations based on action\n",
      "\n",
      "Thought 2: next steps to take based on last observation\n",
      "\n",
      "Action 2: always either Search[query] or, when done, Finish[answer]\n",
      "\n",
      "Observation 2: observations based on action\n",
      "\n",
      "Thought 3: next steps to take based on last observation\n",
      "\n",
      "Action 3: always either Search[query] or, when done, Finish[answer]\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "RAG aims to solve challenges related to information retrieval in large language models.\n",
      "\n",
      "Action 1: Search[core challenges RAG aims to solve in large language models]\n",
      "\n",
      "Question: What are the core challenges that RAG aims to solve in large language models (LLMs)?\n",
      "\n",
      "Thought 1: \n",
      "\n",
      "Action 1: I need to find the core challenges that RAG aims to solve in large language models. Action 1: Search[core challenges RAG aims to solve in large language models]\n",
      "\n",
      "Observation 1: Failed to parse action. Bad formatting or incorrect action name.\n",
      "\n",
      "Thought 2: I need to correct the formatting and search for the core challenges that RAG aims to solve in large language models. Action 1: Search[core challenges RAG aims to solve in large language models]\n",
      "\n",
      "Action 2: Search[core challenges addressed by RAG in large language models]\n",
      "\n",
      "Observation 2:\n",
      "[1] «{'long_text': 'paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development. IIntroduction Large language models (LLMs) have achieved remarkable success, though they still face significant limitations, especially in domain-specific or knowledge-intensive tasks[1], notably producing “hallucinations”[2]when handling queries beyond their training data or requiring current information. To overcome challenges, Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic similarity calculation. By referencing external knowledge, RAG effectively reduces the problem of generating factually incorrect content. Its integration into LLMs has resulted in widespread adoption, establishing RAG as a key technology in advancing chatbots and enhancing the suitability of LLMs for real-world applications. RAG technology has rapidly developed in recent years, and the technology tree summarizing related research is shown in Figure1. The development trajectory of RAG in the era of large models exhibits several distinct stage characteristics. Initially, RAG’s inception coincided with the rise of the Transformer architecture, focusing on enhancing language models by incorporating'}»\n",
      "[2] «{'long_text': 'University College of Design and Innovation, Tongji University Abstract Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs’ intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and'}»\n",
      "[3] «{'long_text': 'RAG RAG’s practicality and alignment with engineering requirements have facilitated its adoption. However, enhancing retrieval efficiency, improving document recall in large knowledge bases, and ensuring data security—such as preventing inadvertent disclosure of document sources or metadata by LLMs—are critical engineering challenges that remain to be addressed[175]. The development of the RAG ecosystem is greatly impacted by the progression of its technical stack. Key tools like LangChain and LLamaIndex have quickly gained popularity with the emergence of ChatGPT, providing extensive RAG-related APIs and becoming essential in the realm of LLMs.The emerging technology stack, while not as rich in features as LangChain and LLamaIndex, stands out through its specialized products. For example, Flowise AI prioritizes a low-code approach, allowing users to deploy AI applications, including RAG, through a user-friendly drag-and-drop interface. Other technologies like HayStack, Meltano, and Cohere Coral are also gaining attention for their unique contributions to the field. In addition to AI-focused vendors, traditional software and cloud service providers are expanding their offerings to include RAG-centric services. Weaviate’s Verba111111https://github.com/weaviate/Verbais designed for personal assistant applications, while'}»\n",
      "\n",
      "Thought 3:\u001b[32m The core challenges addressed by RAG in large language models include hallucination, outdated knowledge, non-transparent reasoning processes, enhancing retrieval efficiency, improving document recall in large knowledge bases, and ensuring data security.\n",
      "\n",
      "Action 3: Finish[The core challenges addressed by RAG in large language models include hallucination, outdated knowledge, non-transparent reasoning processes, enhancing retrieval efficiency, improving document recall in large knowledge bases, and ensuring data security.]\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nYou will be given `context`, `question` and you will respond with `answer`.\\n\\nTo do this, you will interleave Thought, Action, and Observation steps.\\n\\nThought can reason about the current situation, and Action can be the following types:\\n\\n(1) Search[query], which takes a search query and returns one or more potentially relevant passages from a corpus\\n(2) Finish[answer], which returns the final `answer` and finishes the task\\n\\n---\\n\\nFollow the following format.\\n\\nContext: may contain relevant facts\\n\\nQuestion: ${question}\\n\\nThought 1: next steps to take based on last observation\\n\\nAction 1: always either Search[query] or, when done, Finish[answer]\\n\\nObservation 1: observations based on action\\n\\nThought 2: next steps to take based on last observation\\n\\nAction 2: always either Search[query] or, when done, Finish[answer]\\n\\nObservation 2: observations based on action\\n\\nThought 3: next steps to take based on last observation\\n\\nAction 3: always either Search[query] or, when done, Finish[answer]\\n\\n---\\n\\nContext:\\nRAG aims to solve challenges related to information retrieval in large language models.\\n\\nAction 1: Search[core challenges RAG aims to solve in large language models]\\n\\nQuestion: What are the core challenges that RAG aims to solve in large language models (LLMs)?\\n\\nThought 1: \\n\\nAction 1: I need to find the core challenges that RAG aims to solve in large language models. Action 1: Search[core challenges RAG aims to solve in large language models]\\n\\nObservation 1: Failed to parse action. Bad formatting or incorrect action name.\\n\\nThought 2: I need to correct the formatting and search for the core challenges that RAG aims to solve in large language models. Action 1: Search[core challenges RAG aims to solve in large language models]\\n\\nAction 2: Search[core challenges addressed by RAG in large language models]\\n\\nObservation 2:\\n[1] «{'long_text': 'paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development. IIntroduction Large language models (LLMs) have achieved remarkable success, though they still face significant limitations, especially in domain-specific or knowledge-intensive tasks[1], notably producing “hallucinations”[2]when handling queries beyond their training data or requiring current information. To overcome challenges, Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic similarity calculation. By referencing external knowledge, RAG effectively reduces the problem of generating factually incorrect content. Its integration into LLMs has resulted in widespread adoption, establishing RAG as a key technology in advancing chatbots and enhancing the suitability of LLMs for real-world applications. RAG technology has rapidly developed in recent years, and the technology tree summarizing related research is shown in Figure1. The development trajectory of RAG in the era of large models exhibits several distinct stage characteristics. Initially, RAG’s inception coincided with the rise of the Transformer architecture, focusing on enhancing language models by incorporating'}»\\n[2] «{'long_text': 'University College of Design and Innovation, Tongji University Abstract Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs’ intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and'}»\\n[3] «{'long_text': 'RAG RAG’s practicality and alignment with engineering requirements have facilitated its adoption. However, enhancing retrieval efficiency, improving document recall in large knowledge bases, and ensuring data security—such as preventing inadvertent disclosure of document sources or metadata by LLMs—are critical engineering challenges that remain to be addressed[175]. The development of the RAG ecosystem is greatly impacted by the progression of its technical stack. Key tools like LangChain and LLamaIndex have quickly gained popularity with the emergence of ChatGPT, providing extensive RAG-related APIs and becoming essential in the realm of LLMs.The emerging technology stack, while not as rich in features as LangChain and LLamaIndex, stands out through its specialized products. For example, Flowise AI prioritizes a low-code approach, allowing users to deploy AI applications, including RAG, through a user-friendly drag-and-drop interface. Other technologies like HayStack, Meltano, and Cohere Coral are also gaining attention for their unique contributions to the field. In addition to AI-focused vendors, traditional software and cloud service providers are expanding their offerings to include RAG-centric services. Weaviate’s Verba111111https://github.com/weaviate/Verbais designed for personal assistant applications, while'}»\\n\\nThought 3:\\x1b[32m The core challenges addressed by RAG in large language models include hallucination, outdated knowledge, non-transparent reasoning processes, enhancing retrieval efficiency, improving document recall in large knowledge bases, and ensuring data security.\\n\\nAction 3: Finish[The core challenges addressed by RAG in large language models include hallucination, outdated knowledge, non-transparent reasoning processes, enhancing retrieval efficiency, improving document recall in large knowledge bases, and ensuring data security.]\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dspy.ReAct(GenerateAnswer, tools=[dspy.settings.rm])(question=\"What are the core challenges that RAG aims to solve in large language models (LLMs)?\")\n",
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9987c317",
   "metadata": {},
   "source": [
    "# Initialize DSPy Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e8abafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncompiled_rag = RAG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5523912",
   "metadata": {},
   "source": [
    "# Test uncompiled inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37efc6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The core challenges that RAG aims to solve in large language models (LLMs) are handling queries beyond training data, ensuring current information accuracy, and reducing the risk\n"
     ]
    }
   ],
   "source": [
    "print(uncompiled_rag(\"What are the core challenges that RAG aims to solve in large language models (LLMs)?\").answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdca3b3c",
   "metadata": {},
   "source": [
    "# Check the last call to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7fa7a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Answer questions based on the context.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development. IIntroduction Large language models (LLMs) have achieved remarkable success, though they still face significant limitations, especially in domain-specific or knowledge-intensive tasks[1], notably producing “hallucinations”[2]when handling queries beyond their training data or requiring current information. To overcome challenges, Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic similarity calculation. By referencing external knowledge, RAG effectively reduces the problem of generating factually incorrect content. Its integration into LLMs has resulted in widespread adoption, establishing RAG as a key technology in advancing chatbots and enhancing the suitability of LLMs for real-world applications. RAG technology has rapidly developed in recent years, and the technology tree summarizing related research is shown in Figure1. The development trajectory of RAG in the era of large models exhibits several distinct stage characteristics. Initially, RAG’s inception coincided with the rise of the Transformer architecture, focusing on enhancing language models by incorporating»\n",
      "[2] «charting its evolution and anticipated future paths, with a focus on the integration of RAG within LLMs. This paper considers both technical paradigms and research methods, summarizing three main research paradigms from over 100 RAG studies, and analyzing key technologies in the core stages of “Retrieval,” “Generation,” and “Augmentation.” On the other hand, current research tends to focus more on methods, lacking analysis and summarization of how to evaluate RAG. This paper comprehensively reviews the downstream tasks, datasets, benchmarks, and evaluation methods applicable to RAG. Overall, this paper sets out to meticulously compile and categorize the foundational technical concepts, historical progression, and the spectrum of RAG methodologies and applications that have emerged post-LLMs. It is designed to equip readers and professionals with a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and speculate on upcoming trends and innovations. Our contributions are as follows: In this survey, we present a thorough and»\n",
      "[3] «University College of Design and Innovation, Tongji University Abstract Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs’ intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and»\n",
      "\n",
      "Question: What are the core challenges that RAG aims to solve in large language models (LLMs)?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m identify the core challenges that RAG aims to solve in large language models (LLMs). The context mentions that LLMs face limitations such as producing \"hallucinations\" when handling queries beyond their training data or requiring current information. RAG enhances LLMs by retrieving relevant document chunks from external knowledge bases through semantic similarity calculation, reducing the problem of generating factually incorrect content. Therefore, the core challenges that RAG aims to solve in LLMs are handling queries beyond training data, ensuring current information accuracy, and reducing the risk of generating factually incorrect content.\n",
      "\n",
      "Answer: The core challenges that RAG aims to solve in large language models (LLMs) are handling queries beyond training data, ensuring current information accuracy, and reducing the risk\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nAnswer questions based on the context.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: may contain relevant facts\\n\\nQuestion: ${question}\\n\\nReasoning: Let\\'s think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nContext:\\n[1] «paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development. IIntroduction Large language models (LLMs) have achieved remarkable success, though they still face significant limitations, especially in domain-specific or knowledge-intensive tasks[1], notably producing “hallucinations”[2]when handling queries beyond their training data or requiring current information. To overcome challenges, Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic similarity calculation. By referencing external knowledge, RAG effectively reduces the problem of generating factually incorrect content. Its integration into LLMs has resulted in widespread adoption, establishing RAG as a key technology in advancing chatbots and enhancing the suitability of LLMs for real-world applications. RAG technology has rapidly developed in recent years, and the technology tree summarizing related research is shown in Figure1. The development trajectory of RAG in the era of large models exhibits several distinct stage characteristics. Initially, RAG’s inception coincided with the rise of the Transformer architecture, focusing on enhancing language models by incorporating»\\n[2] «charting its evolution and anticipated future paths, with a focus on the integration of RAG within LLMs. This paper considers both technical paradigms and research methods, summarizing three main research paradigms from over 100 RAG studies, and analyzing key technologies in the core stages of “Retrieval,” “Generation,” and “Augmentation.” On the other hand, current research tends to focus more on methods, lacking analysis and summarization of how to evaluate RAG. This paper comprehensively reviews the downstream tasks, datasets, benchmarks, and evaluation methods applicable to RAG. Overall, this paper sets out to meticulously compile and categorize the foundational technical concepts, historical progression, and the spectrum of RAG methodologies and applications that have emerged post-LLMs. It is designed to equip readers and professionals with a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and speculate on upcoming trends and innovations. Our contributions are as follows: In this survey, we present a thorough and»\\n[3] «University College of Design and Innovation, Tongji University Abstract Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs’ intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and»\\n\\nQuestion: What are the core challenges that RAG aims to solve in large language models (LLMs)?\\n\\nReasoning: Let\\'s think step by step in order to\\x1b[32m identify the core challenges that RAG aims to solve in large language models (LLMs). The context mentions that LLMs face limitations such as producing \"hallucinations\" when handling queries beyond their training data or requiring current information. RAG enhances LLMs by retrieving relevant document chunks from external knowledge bases through semantic similarity calculation, reducing the problem of generating factually incorrect content. Therefore, the core challenges that RAG aims to solve in LLMs are handling queries beyond training data, ensuring current information accuracy, and reducing the risk of generating factually incorrect content.\\n\\nAnswer: The core challenges that RAG aims to solve in large language models (LLMs) are handling queries beyond training data, ensuring current information accuracy, and reducing the risk\\x1b[0m\\n\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3dda6",
   "metadata": {},
   "source": [
    "# 4. DSPy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ce3c72",
   "metadata": {},
   "source": [
    "# Evaluate our RAG Program before it is compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bfccd8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the latest evaluation frameworks and benchmarks used to measure the effectiveness of RAG systems?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reminder our dataset looks like this:\n",
    "\n",
    "devset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc24f324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-04-28T02:38:47.429068Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t 'str' object has no attribute 'inputs'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m147\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 1  (0.0):   7%|▋         | 1/15 [00:00<00:01, 10.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-04-28T02:38:47.431058Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t 'str' object has no attribute 'inputs'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m147\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 2  (0.0):  13%|█▎        | 2/15 [00:00<00:00, 20.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-04-28T02:38:47.432423Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t 'str' object has no attribute 'inputs'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m147\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 3  (0.0):  20%|██        | 3/15 [00:00<00:00, 29.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-04-28T02:38:47.433776Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t 'str' object has no attribute 'inputs'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m147\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 4  (0.0):  20%|██        | 3/15 [00:00<00:00, 29.92it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdspy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Evaluate\n\u001b[1;32m      3\u001b[0m evaluate \u001b[38;5;241m=\u001b[39m Evaluate(devset\u001b[38;5;241m=\u001b[39mdevset, num_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, display_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, display_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRAG\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_metric\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/paper-qa-agent/venv/lib/python3.12/site-packages/dspy/evaluate/evaluate.py:158\u001b[0m, in \u001b[0;36mEvaluate.__call__\u001b[0;34m(self, program, metric, devset, num_threads, display_progress, display_table, return_all_scores, return_outputs)\u001b[0m\n\u001b[1;32m    155\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mtqdm\u001b[38;5;241m.\u001b[39m_instances\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_threads \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 158\u001b[0m     reordered_devset, ncorrect, ntotal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_single_thread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped_program\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     reordered_devset, ncorrect, ntotal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_multi_thread(\n\u001b[1;32m    161\u001b[0m         wrapped_program,\n\u001b[1;32m    162\u001b[0m         devset,\n\u001b[1;32m    163\u001b[0m         num_threads,\n\u001b[1;32m    164\u001b[0m         display_progress,\n\u001b[1;32m    165\u001b[0m     )\n",
      "File \u001b[0;32m~/src/paper-qa-agent/venv/lib/python3.12/site-packages/dspy/evaluate/evaluate.py:67\u001b[0m, in \u001b[0;36mEvaluate._execute_single_thread\u001b[0;34m(self, wrapped_program, devset, display_progress)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, arg \u001b[38;5;129;01min\u001b[39;00m devset:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m logging_redirect_tqdm():\n\u001b[0;32m---> 67\u001b[0m         example_idx, example, prediction, score \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped_program\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m         reordered_devset\u001b[38;5;241m.\u001b[39mappend((example_idx, example, prediction, score))\n\u001b[1;32m     69\u001b[0m         ncorrect \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m score\n",
      "File \u001b[0;32m~/src/paper-qa-agent/venv/lib/python3.12/site-packages/dspy/evaluate/evaluate.py:145\u001b[0m, in \u001b[0;36mEvaluate.__call__.<locals>.wrapped_program\u001b[0;34m(example_idx, example)\u001b[0m\n\u001b[1;32m    143\u001b[0m     current_error_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_count\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_error_count \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_errors:\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    147\u001b[0m dspy\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError for example in dev set: \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m example_idx, example, {}, \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[0;32m~/src/paper-qa-agent/venv/lib/python3.12/site-packages/dspy/evaluate/evaluate.py:127\u001b[0m, in \u001b[0;36mEvaluate.__call__.<locals>.wrapped_program\u001b[0;34m(example_idx, example)\u001b[0m\n\u001b[1;32m    124\u001b[0m     thread_stacks[threading\u001b[38;5;241m.\u001b[39mget_ident()] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(dspy\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mmain_stack)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m program(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mexample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m())\n\u001b[1;32m    128\u001b[0m     score \u001b[38;5;241m=\u001b[39m metric(\n\u001b[1;32m    129\u001b[0m         example,\n\u001b[1;32m    130\u001b[0m         prediction,\n\u001b[1;32m    131\u001b[0m     )  \u001b[38;5;66;03m# FIXME: TODO: What's the right order? Maybe force name-based kwargs!\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# increment assert and suggest failures to program's attributes\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'inputs'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 4  (0.0):  27%|██▋       | 4/15 [00:20<00:00, 29.92it/s]"
     ]
    }
   ],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "evaluate = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=5)\n",
    "\n",
    "evaluate(RAG(), metric=llm_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976ee450",
   "metadata": {},
   "source": [
    "# Metric Analysis\n",
    "\n",
    "The maximum value per rating is (5 + 5*2 + 5) / 5 = 4\n",
    "\n",
    "4 * 10 test questions = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c998c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cccbf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metricLM.inspect_history(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9978562c",
   "metadata": {},
   "source": [
    "# BootstrapFewShot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18712073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "teleprompter = BootstrapFewShot(metric=llm_metric, max_labeled_demos=8, max_rounds=3)\n",
    "\n",
    "# also common to init here, e.g. Rag()\n",
    "compiled_rag = teleprompter.compile(uncompiled_rag, trainset=trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd62e57",
   "metadata": {},
   "source": [
    "### Inspect the compiled prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9507f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_rag(\"What do cross encoders do?\").answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e6fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ed105e",
   "metadata": {},
   "source": [
    "### Evaluate the Compiled RAG Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8926c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789072a8",
   "metadata": {},
   "source": [
    "# BootstrapFewShotWithRandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde3ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accidentally spent $12 on this with `num_candidate_programs=20`, caution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5998cbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(metric=llm_metric, \n",
    "                                                max_bootstrapped_demos=4,\n",
    "                                                max_labeled_demos=4, \n",
    "                                                max_rounds=1,\n",
    "                                                num_candidate_programs=2,\n",
    "                                                num_threads=2)\n",
    "\n",
    "# also common to init here, e.g. Rag()\n",
    "second_compiled_rag = teleprompter.compile(uncompiled_rag, trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6302e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_compiled_rag(\"What do cross encoders do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de6d6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(second_compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f1e2b1",
   "metadata": {},
   "source": [
    "# BayesianSignatureOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e272916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BayesianSignatureOptimizer\n",
    "\n",
    "llm_prompter = dspy.OpenAI(model='gpt-4', max_tokens=2000, model_type='chat')\n",
    "\n",
    "teleprompter = BayesianSignatureOptimizer(task_model=dspy.settings.lm,\n",
    "                                          metric=llm_metric,\n",
    "                                          prompt_model=llm_prompter,\n",
    "                                          n=5,\n",
    "                                          verbose=False)\n",
    "\n",
    "kwargs = dict(num_threads=1, display_progress=True, display_table=0)\n",
    "third_compiled_rag = teleprompter.compile(RAG(), devset=devset,\n",
    "                                         optuna_trials_num=3,\n",
    "                                         max_bootstrapped_demos=4,\n",
    "                                         max_labeled_demos=4,\n",
    "                                         eval_kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc8cb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "third_compiled_rag(\"What do cross encoders do?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1367bcc7",
   "metadata": {},
   "source": [
    "# Check this out!!\n",
    "\n",
    "Below you can see how the BayesianSignatureOptimizer jointly (1) optimizes the task instruction to:\n",
    "\n",
    "```\n",
    "Assess the context and answer the given questions that are predominantly about software usage, process optimization, and troubleshooting. Focus on providing accurate information related to tech or software-related queries.\n",
    "```\n",
    "\n",
    "As well as sourcing input-output examples for the prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a834857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98db525",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(third_compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecdc368",
   "metadata": {},
   "source": [
    "# Test Set Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f6e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Uncompiled\n",
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "# Set up the `evaluate_on_hotpotqa` function. We'll use this many times below.\n",
    "evaluate = Evaluate(devset=testset, num_threads=1, display_progress=True, display_table=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ec6960",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(uncompiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f919a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b00bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(second_compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0790a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(third_compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f76b0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
