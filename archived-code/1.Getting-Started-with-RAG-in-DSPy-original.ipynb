{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4274da",
   "metadata": {},
   "source": [
    "# Getting Started with RAG in DSPy\n",
    "\n",
    "This notebook will show you how to use DSPy to compile a RAG program! DSPy compilation is a fairly new tool for LLM developers, so let's start with an overview of the concept. By `compiling`, we mean finding the prompts that elicit the behavior we want from LLMs when connected in some kind of pipeline.\n",
    "\n",
    "For example, RAG is a very common LLM pipeline. In it's simplest form, RAG consists of 2 steps, (1) Retrieve and (2) Answer a Question. Part (2), Answering a Question, has an associated prompt, for example, people generally use:\n",
    "\n",
    "```\n",
    "--\n",
    "\n",
    "Please answer the question based on the following context.\n",
    "\n",
    "context  {context}\n",
    "\n",
    "question {question}\n",
    "\n",
    "--\n",
    "```\n",
    "\n",
    "This prompt may be a good initial point for an LLM to understand the task. However, it is not the *optimal* prompt. DSPy optimizes the prompt for you by jointly (1) tweaking the instructions, such as rewriting an initial prompt like: \n",
    "\n",
    "```\n",
    "Please answer the question based on the following context.\n",
    "```\n",
    "\n",
    "to \n",
    "\n",
    "```\n",
    "Assess the context and answer the given questions that are predominantly about software usage, process optimization, and troubleshooting. Focus on providing accurate information related to tech or software-related queries.\n",
    "```\n",
    "\n",
    "Further, DSPy (2) finds examples of desired input-outputs in the prompt to further improve performance, also known as `In-Context Learning`. In this example, we will begin with the simple prompt: `Please answer the question based on the following context.` and end up with:\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "In order to leverage black-box optimization techniques like random search, bayesian optimization, or evolutionary algorithms, we need a metric. Coming up with metrics to describe desired system behavior has been a longstanding challenge in Machine Learning research. Excitingly, LLMs have made amazing progress. For example, we can evaluate a RAG answer by prompting an LLM with, `Is the assessed text grounded in the context? Say no if it includes significant facts not in the context`. We then optimize the RAG program to increase the metric LLM's assessment of answer quality.\n",
    "\n",
    "This example contains 4 parts:\n",
    "\n",
    "- 0: DSPy Settings and Installation\n",
    "- 1: DSPy Datasets with `dspy.Example`\n",
    "- 2: LLM Metrics in DSPy\n",
    "- 3: LLM Programming with `dspy.Module`\n",
    "- 4: Optimization with `BootstrapFewShot`, `BootstrapFewShotRandomSearch`, and `BayesianSignatureOptimizer`.\n",
    "\n",
    "\n",
    "We are using 2 datasets for this example. Firstly, we have an index of the Weaviate Blog Posts. We will use the Weaviate Blog Posts as the retrieved context to help with our second dataset, the Weaviate FAQs. The Weaviate FAQs consists of 44 question-answer pairs of frequently asked Weaviate questions such as: `Do I need to know about Docker (Compose) to use Weaviate?`\n",
    "\n",
    "We isolate 10 examples to use as our test set and optimize our program with the remaining 34.\n",
    "\n",
    "Our uncompiled RAG program achieves a score of 270 on the held-out test set.\n",
    "\n",
    "Our RAG program compiled with the `BayesianSignatureOptimizer` achieves a score of 340! A ~30% improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb763b0b",
   "metadata": {},
   "source": [
    "# 0: DSPy Settings and Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a06dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dspy-ai==2.1.9 weaviate-client==3.26.2 > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e294d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42260862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Weaviate Retriever and configure LLM\n",
    "import dspy\n",
    "from dspy.retrieve.weaviate_rm import WeaviateRM\n",
    "import weaviate\n",
    "import openai\n",
    "\n",
    "\n",
    "llm = dspy.OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# ollamaLLM = dspy.OpenAI(api_base=\"http://localhost:11434/v1/\", api_key=\"ollama\", model=\"mistral-7b-instruct-v0.2-q6_K\", stop='\\n\\n', model_type='chat')\n",
    "# Thanks Knox! https://twitter.com/JR_Knox1977/status/1756018720818794916/photo/1\n",
    "\n",
    "weaviate_client = weaviate.Client(\"http://localhost:8080\")\n",
    "retriever_model = WeaviateRM(\"WeaviateBlogChunk\", weaviate_client=weaviate_client)\n",
    "# Assumes the Weaviate collection has a text key `content`\n",
    "dspy.settings.configure(lm=llm, rm=retriever_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba0a2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dspy.settings.lm(\"Write a 3 line poem about neural networks.\"))\n",
    "context_example = dspy.OpenAI(model=\"gpt-4\")\n",
    "\n",
    "with dspy.context(llm=context_example):\n",
    "    print(context_example(\"Write a 3 line poem about neural networks.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b146a20",
   "metadata": {},
   "source": [
    "# 1. DSPy Datasets with `dspy.Example`\n",
    "\n",
    "Our retrieval engine is filled with chunks from Weaviate Blog posts.\n",
    "\n",
    "Please see weaviate/recipes/integrations/dspy/Weaviate-Import.ipynb for a full tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08417e60",
   "metadata": {},
   "source": [
    "# Import FAQs from a markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cacaa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load FAQs\n",
    "import re\n",
    "\n",
    "f = open(\"faq.md\")\n",
    "markdown_content = f.read()\n",
    "\n",
    "def parse_questions(markdown_content):\n",
    "    # Regular expression pattern for finding questions\n",
    "    question_pattern = r'#### Q: (.+?)\\n'\n",
    "\n",
    "    # Finding all questions\n",
    "    questions = re.findall(question_pattern, markdown_content, re.DOTALL)\n",
    "\n",
    "    return questions\n",
    "\n",
    "# Parsing the markdown content to get only questions\n",
    "questions = parse_questions(markdown_content)\n",
    "\n",
    "# Displaying the first few extracted questions\n",
    "questions[:5]  # Displaying only the first few for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89745ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c34f8d",
   "metadata": {},
   "source": [
    "# Wrap each FAQ into an `Example` object\n",
    "\n",
    "The dspy `Example` object optionally lets you attach metadata, or additional labels, to input/output pairs.\n",
    "\n",
    "For example, you may want to jointly supervise the answer as well as the context the retrieval system produced to feed into the answer generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8449b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into dspy datasets\n",
    "import dspy\n",
    "\n",
    "# ToDo, add random splitting -- maybe wrap this entire thing in a cross-validation loop\n",
    "trainset = questions[:20] # 20 examples for training\n",
    "devset = questions[20:30] # 10 examples for development\n",
    "testset = questions[30:] # 14 examples for testing\n",
    "\n",
    "trainset = [dspy.Example(question=question).with_inputs(\"question\") for question in trainset]\n",
    "devset = [dspy.Example(question=question).with_inputs(\"question\") for question in devset]\n",
    "testset = [dspy.Example(question=question).with_inputs(\"question\") for question in testset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a884b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175ab89",
   "metadata": {},
   "source": [
    "# 2. LLM Metrics\n",
    "\n",
    "Define a Metric for Performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a5411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a WIP, the next step is to optimize this metric as itself a DSPy module (pretty meta)\n",
    "\n",
    "# Reference - https://github.com/stanfordnlp/dspy/blob/main/examples/tweets/tweet_metric.py\n",
    "\n",
    "metricLM = dspy.OpenAI(model='gpt-4', max_tokens=1000, model_type='chat')\n",
    "\n",
    "# Signature for LLM assessments.\n",
    "\n",
    "class Assess(dspy.Signature):\n",
    "    \"\"\"Assess the quality of an answer to a question.\"\"\"\n",
    "    \n",
    "    context = dspy.InputField(desc=\"The context for answering the question.\")\n",
    "    assessed_question = dspy.InputField(desc=\"The evaluation criterion.\")\n",
    "    assessed_answer = dspy.InputField(desc=\"The answer to the question.\")\n",
    "    assessment_answer = dspy.OutputField(desc=\"A rating between 1 and 5. Only output the rating and nothing else.\")\n",
    "\n",
    "def llm_metric(gold, pred, trace=None):\n",
    "    predicted_answer = pred.answer\n",
    "    question = gold.question\n",
    "    \n",
    "    print(f\"Test Question: {question}\")\n",
    "    print(f\"Predicted Answer: {predicted_answer}\")\n",
    "    \n",
    "    detail = \"Is the assessed answer detailed?\"\n",
    "    faithful = \"Is the assessed text grounded in the context? Say no if it includes significant facts not in the context.\"\n",
    "    overall = f\"Please rate how well this answer answers the question, `{question}` based on the context.\\n `{predicted_answer}`\"\n",
    "    \n",
    "    with dspy.context(lm=metricLM):\n",
    "        context = dspy.Retrieve(k=5)(question).passages\n",
    "        detail = dspy.ChainOfThought(Assess)(context=\"N/A\", assessed_question=detail, assessed_answer=predicted_answer)\n",
    "        faithful = dspy.ChainOfThought(Assess)(context=context, assessed_question=faithful, assessed_answer=predicted_answer)\n",
    "        overall = dspy.ChainOfThought(Assess)(context=context, assessed_question=overall, assessed_answer=predicted_answer)\n",
    "    \n",
    "    print(f\"Faithful: {faithful.assessment_answer}\")\n",
    "    print(f\"Detail: {detail.assessment_answer}\")\n",
    "    print(f\"Overall: {overall.assessment_answer}\")\n",
    "    \n",
    "    \n",
    "    total = float(detail.assessment_answer) + float(faithful.assessment_answer)*2 + float(overall.assessment_answer)\n",
    "    \n",
    "    return total / 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0cc41a",
   "metadata": {},
   "source": [
    "## Inspect the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf6048",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = dspy.Example(question=\"What do cross encoders do?\")\n",
    "test_pred = dspy.Example(answer=\"They re-rank documents.\")\n",
    "\n",
    "type(llm_metric(test_example, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f763ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = dspy.Example(question=\"What do cross encoders do?\")\n",
    "test_pred = dspy.Example(answer=\"They index data.\")\n",
    "\n",
    "type(llm_metric(test_example, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ccd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metricLM.inspect_history(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5202b5",
   "metadata": {},
   "source": [
    "# 3. The DSPy Programming Model\n",
    "\n",
    "This block of first code will initilaize the `GenerateAnswer` signature.\n",
    "\n",
    "Then we will compose a `dspy.Module` consisting of:\n",
    "- Retrieve\n",
    "- GenerateAnswer\n",
    "\n",
    "The DSPy programming model is one of the most powerful aspects of DSPy, we get:\n",
    "- An intuitive interface to compose prompts into programs.\n",
    "- A clean way to organize prompts into Signatures.\n",
    "- Structured output parsing with `dspy.OutputField`\n",
    "- Built-in prompt extensions such as `ChainOfThought`, `ReAct`, and more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5ce19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions based on the context.\"\"\"\n",
    "    \n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743dda11",
   "metadata": {},
   "source": [
    "# A little more info on built-in dspy modules\n",
    "\n",
    "The DSPy programming model gives you a lot of cool features out of the box. Observe how different modules implement signatures with additional prompting techniques like `ChainOfThought` and `ReAct`. `Predict` is the base class to observe what a standrd prompt looks like without the module extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789b0cd",
   "metadata": {},
   "source": [
    "### dspy.Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d4cdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.Predict(GenerateAnswer)(question=\"What are Cross Encoders?\")\n",
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d2cfb6",
   "metadata": {},
   "source": [
    "### dspy.ChainOfThought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e194e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.ChainOfThought(GenerateAnswer)(question=\"What are Cross Encoders?\")\n",
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5666dd3",
   "metadata": {},
   "source": [
    "### dspy.ReAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8c99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.ReAct(GenerateAnswer, tools=[dspy.settings.rm])(question=\"What are cross encoders?\")\n",
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9987c317",
   "metadata": {},
   "source": [
    "# Initialize DSPy Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8abafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncompiled_rag = RAG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5523912",
   "metadata": {},
   "source": [
    "# Test uncompiled inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37efc6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uncompiled_rag(\"What are re-rankers in search engines?\").answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdca3b3c",
   "metadata": {},
   "source": [
    "# Check the last call to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa7a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3dda6",
   "metadata": {},
   "source": [
    "# 4. DSPy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ce3c72",
   "metadata": {},
   "source": [
    "# Evaluate our RAG Program before it is compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfccd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder our dataset looks like this:\n",
    "\n",
    "devset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc24f324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "evaluate = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=5)\n",
    "\n",
    "evaluate(RAG(), metric=llm_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976ee450",
   "metadata": {},
   "source": [
    "# Metric Analysis\n",
    "\n",
    "The maximum value per rating is (5 + 5*2 + 5) / 5 = 4\n",
    "\n",
    "4 * 10 test questions = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c998c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cccbf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metricLM.inspect_history(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9978562c",
   "metadata": {},
   "source": [
    "# BootstrapFewShot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18712073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "teleprompter = BootstrapFewShot(metric=llm_metric, max_labeled_demos=8, max_rounds=3)\n",
    "\n",
    "# also common to init here, e.g. Rag()\n",
    "compiled_rag = teleprompter.compile(uncompiled_rag, trainset=trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd62e57",
   "metadata": {},
   "source": [
    "### Inspect the compiled prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9507f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_rag(\"What do cross encoders do?\").answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e6fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ed105e",
   "metadata": {},
   "source": [
    "### Evaluate the Compiled RAG Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8926c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789072a8",
   "metadata": {},
   "source": [
    "# BootstrapFewShotWithRandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde3ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accidentally spent $12 on this with `num_candidate_programs=20`, caution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5998cbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(metric=llm_metric, \n",
    "                                                max_bootstrapped_demos=4,\n",
    "                                                max_labeled_demos=4, \n",
    "                                                max_rounds=1,\n",
    "                                                num_candidate_programs=2,\n",
    "                                                num_threads=2)\n",
    "\n",
    "# also common to init here, e.g. Rag()\n",
    "second_compiled_rag = teleprompter.compile(uncompiled_rag, trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6302e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_compiled_rag(\"What do cross encoders do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de6d6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(second_compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f1e2b1",
   "metadata": {},
   "source": [
    "# BayesianSignatureOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e272916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BayesianSignatureOptimizer\n",
    "\n",
    "llm_prompter = dspy.OpenAI(model='gpt-4', max_tokens=2000, model_type='chat')\n",
    "\n",
    "teleprompter = BayesianSignatureOptimizer(task_model=dspy.settings.lm,\n",
    "                                          metric=llm_metric,\n",
    "                                          prompt_model=llm_prompter,\n",
    "                                          n=5,\n",
    "                                          verbose=False)\n",
    "\n",
    "kwargs = dict(num_threads=1, display_progress=True, display_table=0)\n",
    "third_compiled_rag = teleprompter.compile(RAG(), devset=devset,\n",
    "                                         optuna_trials_num=3,\n",
    "                                         max_bootstrapped_demos=4,\n",
    "                                         max_labeled_demos=4,\n",
    "                                         eval_kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc8cb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "third_compiled_rag(\"What do cross encoders do?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1367bcc7",
   "metadata": {},
   "source": [
    "# Check this out!!\n",
    "\n",
    "Below you can see how the BayesianSignatureOptimizer jointly (1) optimizes the task instruction to:\n",
    "\n",
    "```\n",
    "Assess the context and answer the given questions that are predominantly about software usage, process optimization, and troubleshooting. Focus on providing accurate information related to tech or software-related queries.\n",
    "```\n",
    "\n",
    "As well as sourcing input-output examples for the prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a834857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98db525",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(third_compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecdc368",
   "metadata": {},
   "source": [
    "# Test Set Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f6e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Uncompiled\n",
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "# Set up the `evaluate_on_hotpotqa` function. We'll use this many times below.\n",
    "evaluate = Evaluate(devset=testset, num_threads=1, display_progress=True, display_table=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ec6960",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(uncompiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f919a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b00bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(second_compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0790a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(third_compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f76b0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
