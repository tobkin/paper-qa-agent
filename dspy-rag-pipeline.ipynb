{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Global Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.retrieve.weaviate_rm import WeaviateRM\n",
    "from wcs_client_adapter import WcsClientAdapter\n",
    "from wcs_client_adapter import COLLECTION_TEXT_KEY, WCS_COLLECTION_NAME\n",
    "from indexers import NaiveWcsIndexer\n",
    "import csv\n",
    "from typing import List\n",
    "from typing import List, NamedTuple\n",
    "import random\n",
    "\n",
    "\n",
    "def display_md(content):\n",
    "  display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Paper for Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_uri = \"https://arxiv.org/html/2312.10997v5\"\n",
    "indexer = NaiveWcsIndexer(doc_uri) # TODO: this calling syntax doesn't make it clear what side effects the constructor has"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Language Model and Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbo = dspy.OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "wcs_client = WcsClientAdapter.get_wcs_client()\n",
    "wcs_rm = WeaviateRM(WCS_COLLECTION_NAME, weaviate_client=wcs_client, weaviate_collection_text_key=COLLECTION_TEXT_KEY)\n",
    "dspy.settings.configure(lm=turbo, rm=wcs_rm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Questions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerable_questions_path = \"./data/answerable-questions.csv\"\n",
    "unanswerable_questions_path = \"./data/unanswerable-questions.csv\"\n",
    "\n",
    "def load_questions_from_csv(file_path: str) -> List[str]:\n",
    "    questions = []\n",
    "    with open(file_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                questions.append(row[0])\n",
    "    return questions\n",
    "\n",
    "answerable_questions = load_questions_from_csv(answerable_questions_path)\n",
    "unanswerable_questions = load_questions_from_csv(unanswerable_questions_path)\n",
    "all_questions = answerable_questions + unanswerable_questions\n",
    "all_qs_as_dspy_examples = trainset = [dspy.Example(question=question).with_inputs(\"question\") for question in all_questions]\n",
    "\n",
    "class DataSplits(NamedTuple):\n",
    "    train: List\n",
    "    dev: List\n",
    "    test: List\n",
    "\n",
    "def split_data(data: List, train_size: float, dev_size: float, test_size: float) -> DataSplits:\n",
    "    if train_size + dev_size + test_size != 1:\n",
    "        raise ValueError(\"The sum of train_size, dev_size, and test_size must be 1.\")\n",
    "\n",
    "    random.shuffle(data)  \n",
    "    \n",
    "    train_end = int(train_size * len(data))\n",
    "    dev_end = train_end + int(dev_size * len(data))\n",
    "    \n",
    "    train_set = data[:train_end]\n",
    "    dev_set = data[train_end:dev_end]\n",
    "    test_set = data[dev_end:]\n",
    "    \n",
    "    return DataSplits(train=train_set, dev=dev_set, test=test_set)\n",
    "\n",
    "splits = split_data(all_qs_as_dspy_examples, 0.7, 0.15, 0.15)\n",
    "\n",
    "trainset = splits.train\n",
    "devset = splits.dev\n",
    "testset = splits.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"between 1 and 4 sentences\")\n",
    "    \n",
    "# TODO: LLM assessor signature goes here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Rag Pipeline as DSPy Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Complete RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Retrieval Pipeline Stage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
