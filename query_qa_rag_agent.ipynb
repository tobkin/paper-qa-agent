{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Global Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import NaiveWcsQaRagAgent\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def display_md(content):\n",
    "  display(Markdown(content))\n",
    "\n",
    "doc_uri = \"https://arxiv.org/html/2312.10997v5\"\n",
    "qa_agent = NaiveWcsQaRagAgent(doc_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Naive RAG is the earliest methodology within the RAG research paradigm, emerging after the widespread adoption of ChatGPT. It follows a traditional process of indexing, retrieval, and generation, known as the \"Retrieve-Read\" framework. Naive RAG involves cleaning and extracting raw data in various formats, segmenting text into smaller parts, encoding them into vector representations, and storing them in a vector database."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What is naive RAG?\"\n",
    "completion = qa_agent.query(question)\n",
    "display_md(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The authors of the paper are Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Who are the authors of this paper?\"\n",
    "completion = qa_agent.query(question)\n",
    "display_md(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Some benchmarks available for RAG evaluation include RGB, RECALL, and CRUD, which focus on assessing essential abilities of RAG models. These benchmarks are complemented by automated tools like RAGAS, ARES, and TruLens, which utilize LLMs to provide quality scores for RAG models. The evaluation framework for RAG models encompasses metrics such as retrieval quality, generation quality, context relevance, faithfulness, answer relevance, and various other aspects to comprehensively assess model performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What are some benchmarks available for RAG?\"\n",
    "completion = qa_agent.query(question)\n",
    "display_md(completion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
